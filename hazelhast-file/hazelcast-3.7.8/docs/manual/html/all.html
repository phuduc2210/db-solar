<!DOCTYPE html>
<html>
<head>

<link rel="canonical" href="http://docs.hazelcast.org/docs/latest-dev/manual/html-single/"/>
<noscript>
	<meta http-equiv="refresh" content="0;URL=http://docs.hazelcast.org/docs/latest-dev/manual/html-single/">
</noscript>

<!--[if lt IE 9]><script type="text/javascript">var IE_fix=true;</script><![endif]-->
<script type="text/javascript">
	var url = "http://docs.hazelcast.org/docs/latest-dev/manual/html-single/";
	if(typeof IE_fix != "undefined") // IE8 and lower fix to pass the http referer
	{
		document.write("redirecting..."); // Don't remove this line or appendChild() will fail because it is called before document.onload to make the redirect as fast as possible. Nobody will see this text, it is only a tech fix.
		var referLink = document.createElement("a");
		referLink.href = url;
		document.body.appendChild(referLink);
		referLink.click();
	}
	else { window.location.replace(url); } // All other browsers
</script>

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Hazelcast Documentation Version: 3.7.8</title>

    <link type="text/css" rel="stylesheet" href="assets/css/reset.css">
    <link type="text/css" rel="stylesheet" href="http://yandex.st/highlightjs/7.3/styles/github.min.css">
    <link type="text/css" rel="stylesheet" href="assets/css/docs.css">
    <link type="text/css" rel="stylesheet" href="assets/css/print.css" media="print">
    

    <script type="text/javascript" src="assets/js/jquery-1.10.1.min.js"></script>
    <script type="text/javascript" src="assets/js/jquery.sticky-kit.js"></script>
    <script type="text/javascript" src="http://yandex.st/highlightjs/6.1/highlight.min.js"></script>
    <script type="text/javascript" src="assets/js/lunr.min.js"></script>

    

    <script type="text/javascript">var BASE_URL = "";</script>
    <script type="text/javascript" src="assets/js/viewer.js"></script>
    <script type="text/javascript" src="assets/js/toc.scroll.js"></script>

    <script type="text/javascript">
  	$(function(){
		$("#sidebar").stick_in_parent()
	});
    </script>
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-3921306-1']);
        _gaq.push(['_setDomainName', 'hazelcast.com']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();

    </script>
    <script type="text/javascript">
        setTimeout(function(){var a=document.createElement("script");
            var b=document.getElementsByTagName("script")[0];
            a.src=document.location.protocol+"//dnn506yrbagrg.cloudfront.net/pages/scripts/0020/7413.js?"+Math.floor(new Date().getTime()/3600000);
            a.async=true;a.type="text/javascript";b.parentNode.insertBefore(a,b)}, 1);
    </script>

</head>
<body>
    <div id="page">
        <a name="top" />
         <div id="sitebar">
    <div class="content">
      <p><a href="http://hazelcast.org/documentation">Go to <span style="font-weight:bold">hazelcast.org/documentation</span></a></p>
      </div>
  </div>

        <header id="header">
			<div class="content">
			<div style="overflow:hidden">
				<div style="float:left">
                    <a href="http://www.hazelcast.org"><img src="assets/img/logo.png" style="width: 199px;padding-top: 18px"/></a>
					
				</div>	
					<div style="float:left;padding-top: 30px;padding-left: 30px;">
					<a href=""></a> <span style="font-size:36px;color:white"> Reference Manual </span> <span style="font-size:12px;color:white"> Version: 3.7.8 - Publication Date : May 25, 2017 </span>
					</div>
			</div>
			</div>
        </header>
        <div class="content" style="width:1200px; margin: 0 auto">
	        

	<div id="sidebar" style="overflow-y:auto">
	    <form action="search.html"><input id="search" type="text" placeholder="Search" name="q" /></form>
	    <nav id="toc">
	        
    <ol>
    
        <li>
             <a href="preface.html#preface">Preface</a>
            
                
    <ol>
    
        <li>
             <a href="preface.html#hazelcast-editions">Hazelcast Editions</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#hazelcast-architecture">Hazelcast Architecture</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#hazelcast-plugins">Hazelcast Plugins</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#licensing">Licensing</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#trademarks">Trademarks</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#customer-support">Customer Support</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#release-notes">Release Notes</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#contributing-to-hazelcast">Contributing to Hazelcast</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#partners">Partners</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#phone-home">Phone Home</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="preface.html#typographical-conventions">Typographical Conventions</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="documentrevisionhistory.html#document-revision-history">Document Revision History</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="installinghazelcast.html#getting-started">Getting Started</a>
            
                
    <ol>
    
        <li>
             <a href="installinghazelcast.html#installation">Installation</a>
            
                
    <ol>
    
        <li>
             <a href="installinghazelcast.html#hazelcast">Hazelcast</a>
            
        </li>
    
        <li>
             <a href="installinghazelcastenterprise.html#hazelcast-enterprise">Hazelcast Enterprise</a>
            
        </li>
    
        <li>
             <a href="licensekeys.html#setting-the-license-key">Setting the License Key</a>
            
        </li>
    
        <li>
             <a href="upgradingfrom3x.html#upgrading-from-3x">Upgrading from 3.x</a>
            
        </li>
    
        <li>
             <a href="upgradingfrom2x.html#upgrading-from-2x">Upgrading from 2.x</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="startingmemberandclient.html#starting-the-member-and-client">Starting the Member and Client</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="usingthescripts.html#using-the-scripts-in-the-package">Using the Scripts In The Package</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="deployingonamazonec2.html#deploying-on-amazon-ec2">Deploying On Amazon EC2</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="deployingonazure.html#deploying-on-microsoft-azure">Deploying On Microsoft Azure</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="deployingoncf.html#deploying-on-pivotal-cloud-foundry">Deploying On Pivotal Cloud Foundry</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="deploymentviadocker.html#deploying-using-docker">Deploying using Docker</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="hazelcastoverview.html#hazelcast-overview">Hazelcast Overview</a>
            
                
    <ol>
    
        <li>
             <a href="shardinginhazelcast.html#sharding-in-hazelcast">Sharding in Hazelcast</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="hazelcasttopology.html#hazelcast-topology">Hazelcast Topology</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="whyhazelcast.html#why-hazelcast">Why Hazelcast?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="datapartitioning.html#data-partitioning">Data Partitioning</a>
            
                
    <ol>
    
        <li>
             <a href="datapartitioning.html#how-the-data-is-partitioned">How the Data is Partitioned</a>
            
        </li>
    
        <li>
             <a href="datapartitioning.html#partition-table">Partition Table</a>
            
        </li>
    
        <li>
             <a href="datapartitioning.html#repartitioning">Repartitioning</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="usecases.html#use-cases">Use Cases</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="resources.html#resources">Resources</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#understanding-configuration">Understanding Configuration</a>
            
                
    <ol>
    
        <li>
             <a href="understandingconfiguration.html#configuring-declaratively">Configuring Declaratively</a>
            
                
    <ol>
    
        <li>
             <a href="understandingconfiguration.html#composing-declarative-configuration">Composing Declarative Configuration</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#configuring-programmatically">Configuring Programmatically</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#configuring-with-system-properties">Configuring with System Properties</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#configuring-within-spring-context">Configuring within Spring Context</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#checking-configuration">Checking Configuration</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#using-wildcards">Using Wildcards</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="understandingconfiguration.html#using-variables">Using Variables</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="settingupclusters.html#setting-up-clusters">Setting Up Clusters</a>
            
                
    <ol>
    
        <li>
             <a href="settingupclusters.html#discovering-cluster-members">Discovering Cluster Members</a>
            
                
    <ol>
    
        <li>
             <a href="multicast.html#discovering-members-by-multicast">Discovering Members by Multicast</a>
            
        </li>
    
        <li>
             <a href="tcp.html#discovering-members-by-tcp">Discovering Members by TCP</a>
            
        </li>
    
        <li>
             <a href="ec2.html#discovering-members-within-ec2-cloud">Discovering Members within EC2 Cloud</a>
            
        </li>
    
        <li>
             <a href="azure.html#discovering-members-within-azure-cloud">Discovering Members within Azure Cloud</a>
            
        </li>
    
        <li>
             <a href="jclouds.html#discovering-members-with-jclouds">Discovering Members with jclouds</a>
            
        </li>
    
        <li>
             <a href="discoveringnativeclients.html#discovering-native-clients">Discovering Native Clients</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="createclustergroups.html#creating-cluster-groups">Creating Cluster Groups</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="partitiongroupconfig.html#partition-group-configuration">Partition Group Configuration</a>
            
                
    <ol>
    
        <li>
             <a href="partitiongroupconfig.html#grouping-types">Grouping Types</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="logging.html#logging-configuration">Logging Configuration</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#other-network-configurations">Other Network Configurations</a>
            
                
    <ol>
    
        <li>
             <a href="othernetworkconfigurations.html#public-address">Public Address</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#port">Port</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#outbound-ports">Outbound Ports</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#reuse-address">Reuse Address</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#join">Join</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#interfaces">Interfaces</a>
            
        </li>
    
        <li>
             <a href="othernetworkconfigurations.html#ipv6-support">IPv6 Support</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="dds.html#distributed-data-structures">Distributed Data Structures</a>
            
                
    <ol>
    
        <li>
             <a href="map.html#map">Map</a>
            
                
    <ol>
    
        <li>
             <a href="map.html#getting-a-map-and-putting-an-entry">Getting a Map and Putting an Entry</a>
            
        </li>
    
        <li>
             <a href="map-backups.html#backing-up-maps">Backing Up Maps</a>
            
        </li>
    
        <li>
             <a href="map-eviction.html#map-eviction">Map Eviction</a>
            
        </li>
    
        <li>
             <a href="map-eviction.html#evicting-map-entries">Evicting Map Entries</a>
            
        </li>
    
        <li>
             <a href="map-inmemoryformat.html#setting-in-memory-format">Setting In-Memory Format</a>
            
        </li>
    
        <li>
             <a href="map-usinghd.html#using-high-density-memory-store-with-map">Using High-Density Memory Store with Map</a>
            
        </li>
    
        <li>
             <a href="map-persistence.html#loading-and-storing-persistent-data">Loading and Storing Persistent Data</a>
            
        </li>
    
        <li>
             <a href="map-nearcache.html#creating-near-cache-for-map">Creating Near Cache for Map</a>
            
        </li>
    
        <li>
             <a href="map-nearcache-usinghd.html#using-high-density-memory-store-with-near-cache">Using High-Density Memory Store with Near Cache</a>
            
        </li>
    
        <li>
             <a href="map-locks.html#locking-maps">Locking Maps</a>
            
        </li>
    
        <li>
             <a href="map-entrystatistics.html#accessing-entry-statistics">Accessing Entry Statistics</a>
            
        </li>
    
        <li>
             <a href="map-maplistener.html#map-listener">Map Listener</a>
            
        </li>
    
        <li>
             <a href="map-listenerwithpredicates.html#listening-to-map-entries-with-predicates">Listening to Map Entries with Predicates</a>
            
        </li>
    
        <li>
             <a href="map-interceptors.html#adding-interceptors">Adding Interceptors</a>
            
        </li>
    
        <li>
             <a href="map-queryresultsizelimiter.html#preventing-out-of-memory-exceptions">Preventing Out of Memory Exceptions</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="queue.html#queue">Queue</a>
            
                
    <ol>
    
        <li>
             <a href="queue.html#getting-a-queue-and-putting-items">Getting a Queue and Putting Items</a>
            
        </li>
    
        <li>
             <a href="queue-samplequeuecode.html#creating-an-example-queue">Creating an Example Queue</a>
            
        </li>
    
        <li>
             <a href="queue-boundedqueue.html#setting-a-bounded-queue">Setting a Bounded Queue</a>
            
        </li>
    
        <li>
             <a href="queue-persistence.html#queueing-with-persistent-datastore">Queueing with Persistent Datastore</a>
            
        </li>
    
        <li>
             <a href="queue-configuration.html#configuring-queue">Configuring Queue</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="multimap.html#multimap">MultiMap</a>
            
                
    <ol>
    
        <li>
             <a href="multimap.html#getting-a-multimap-and-putting-an-entry">Getting a MultiMap and Putting an Entry</a>
            
        </li>
    
        <li>
             <a href="multimap.html#configuring-multimap">Configuring MultiMap</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="set.html#set">Set</a>
            
                
    <ol>
    
        <li>
             <a href="set.html#getting-a-set-and-putting-items">Getting a Set and Putting Items</a>
            
        </li>
    
        <li>
             <a href="set.html#configuring-set">Configuring Set</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="list.html#list">List</a>
            
                
    <ol>
    
        <li>
             <a href="list.html#getting-a-list-and-putting-items">Getting a List and Putting Items</a>
            
        </li>
    
        <li>
             <a href="list.html#configuring-list">Configuring List</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="ringbuffer.html#ringbuffer">Ringbuffer</a>
            
                
    <ol>
    
        <li>
             <a href="ringbuffer.html#getting-a-ringbuffer-and-reading-items">Getting a Ringbuffer and Reading Items</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#adding-items-to-a-ringbuffer">Adding Items to a Ringbuffer</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#iqueue-vs-ringbuffer">IQueue vs. Ringbuffer</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#configuring-ringbuffer-capacity">Configuring Ringbuffer Capacity</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#backing-up-ringbuffer">Backing Up Ringbuffer</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#configuring-ringbuffer-time-to-live">Configuring Ringbuffer Time To Live</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#setting-ringbuffer-overflow-policy">Setting Ringbuffer Overflow Policy</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#configuring-ringbuffer-in-memory-format">Configuring Ringbuffer In-Memory Format</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#adding-batched-items">Adding Batched Items</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#reading-batched-items">Reading Batched Items</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#using-async-methods">Using Async Methods</a>
            
        </li>
    
        <li>
             <a href="ringbuffer.html#ringbuffer-configuration-examples">Ringbuffer Configuration Examples</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="topic.html#topic">Topic</a>
            
                
    <ol>
    
        <li>
             <a href="topic.html#getting-a-topic-and-publishing-messages">Getting a Topic and Publishing Messages</a>
            
        </li>
    
        <li>
             <a href="topic-statistics.html#getting-topic-statistics">Getting Topic Statistics</a>
            
        </li>
    
        <li>
             <a href="topic-internals.html#understanding-topic-behavior">Understanding Topic Behavior</a>
            
        </li>
    
        <li>
             <a href="topic-configuration.html#configuring-topic">Configuring Topic</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="reliabletopic.html#reliable-topic">Reliable Topic</a>
            
                
    <ol>
    
        <li>
             <a href="reliabletopic.html#sample-reliable-itopic-code">Sample Reliable ITopic Code</a>
            
        </li>
    
        <li>
             <a href="reliabletopic.html#slow-consumers">Slow Consumers</a>
            
        </li>
    
        <li>
             <a href="reliabletopic.html#configuring-reliable-topic">Configuring Reliable Topic</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="lock.html#lock">Lock</a>
            
                
    <ol>
    
        <li>
             <a href="lock.html#using-try-catch-blocks-with-locks">Using Try-Catch Blocks with Locks</a>
            
        </li>
    
        <li>
             <a href="lock.html#releasing-locks-with-trylock-timeout">Releasing Locks with tryLock Timeout</a>
            
        </li>
    
        <li>
             <a href="lock.html#avoiding-waiting-threads-with-lease-time">Avoiding Waiting Threads with Lease Time</a>
            
        </li>
    
        <li>
             <a href="lock.html#understanding-lock-behavior">Understanding Lock Behavior</a>
            
        </li>
    
        <li>
             <a href="lock.html#synchronizing-threads-with-icondition">Synchronizing Threads with ICondition</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="iatomiclong.html#iatomiclong">IAtomicLong</a>
            
                
    <ol>
    
        <li>
             <a href="iatomiclong.html#sending-functions-to-iatomiclong">Sending Functions to IAtomicLong</a>
            
        </li>
    
        <li>
             <a href="iatomiclong.html#executing-functions-on-iatomiclong">Executing Functions on IAtomicLong</a>
            
        </li>
    
        <li>
             <a href="iatomiclong.html#reasons-to-use-functions-with-iatomic">Reasons to Use Functions with IAtomic</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="isemaphore.html#isemaphore">ISemaphore</a>
            
                
    <ol>
    
        <li>
             <a href="isemaphore.html#controlling-thread-counts-with-permits">Controlling Thread Counts with Permits</a>
            
        </li>
    
        <li>
             <a href="isemaphore.html#example-semaphore-code">Example Semaphore Code</a>
            
        </li>
    
        <li>
             <a href="isemaphore.html#configuring-semaphore">Configuring Semaphore</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="iatomicreference.html#iatomicreference">IAtomicReference</a>
            
                
    <ol>
    
        <li>
             <a href="iatomicreference.html#sending-functions-to-iatomicreference">Sending Functions to IAtomicReference</a>
            
        </li>
    
        <li>
             <a href="iatomicreference.html#using-iatomicreference">Using IAtomicReference</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="icountdownlatch.html#icountdownlatch">ICountDownLatch</a>
            
                
    <ol>
    
        <li>
             <a href="icountdownlatch.html#gate-keeping-concurrent-activities">Gate-Keeping Concurrent Activities</a>
            
        </li>
    
        <li>
             <a href="icountdownlatch.html#recovering-from-failure">Recovering From Failure</a>
            
        </li>
    
        <li>
             <a href="icountdownlatch.html#using-icountdownlatch">Using ICountDownLatch</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="idgenerator.html#idgenerator">IdGenerator</a>
            
                
    <ol>
    
        <li>
             <a href="idgenerator.html#generating-cluster-wide-ids">Generating Cluster-Wide IDs</a>
            
        </li>
    
        <li>
             <a href="idgenerator.html#unique-ids-and-duplicate-ids">Unique IDs and Duplicate IDs</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="replicatedmap.html#replicated-map">Replicated Map</a>
            
                
    <ol>
    
        <li>
             <a href="replicatedmap.html#replicating-instead-of-partitioning">Replicating Instead of Partitioning</a>
            
        </li>
    
        <li>
             <a href="replicatedmap.html#example-replicated-map-code">Example Replicated Map Code</a>
            
        </li>
    
        <li>
             <a href="replicatedmap.html#considerations-for-replicated-map">Considerations for Replicated Map</a>
            
        </li>
    
        <li>
             <a href="replicatedmap.html#configuration-design-for-replicated-map">Configuration Design for Replicated Map</a>
            
        </li>
    
        <li>
             <a href="replicatedmap.html#configuring-replicated-map">Configuring Replicated Map</a>
            
        </li>
    
        <li>
             <a href="replicatedmap.html#using-entrylistener-on-replicated-map">Using EntryListener on Replicated Map</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="distributedevents.html#distributed-events">Distributed Events</a>
            
                
    <ol>
    
        <li>
             <a href="distributedevents.html#event-listeners-for-hazelcast-members">Event Listeners for Hazelcast Members</a>
            
                
    <ol>
    
        <li>
             <a href="memberevents.html#listening-for-member-events">Listening for Member Events</a>
            
        </li>
    
        <li>
             <a href="distributedobjectevents.html#listening-for-distributed-object-events">Listening for Distributed Object Events</a>
            
        </li>
    
        <li>
             <a href="migrationevents.html#listening-for-migration-events">Listening for Migration Events</a>
            
        </li>
    
        <li>
             <a href="partitionlostevents.html#listening-for-partition-lost-events">Listening for Partition Lost Events</a>
            
        </li>
    
        <li>
             <a href="lifecycleevents.html#listening-for-lifecycle-events">Listening for Lifecycle Events</a>
            
        </li>
    
        <li>
             <a href="mapevents.html#listening-for-map-events">Listening for Map Events</a>
            
        </li>
    
        <li>
             <a href="multimapevents.html#listening-for-multimap-events">Listening for MultiMap Events</a>
            
        </li>
    
        <li>
             <a href="itemevents.html#listening-for-item-events">Listening for Item Events</a>
            
        </li>
    
        <li>
             <a href="topicmessages.html#listening-for-topic-messages">Listening for Topic Messages</a>
            
        </li>
    
        <li>
             <a href="listeningclients.html#listening-for-clients">Listening for Clients</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="eventlistenersforclients.html#event-listeners-for-hazelcast-clients">Event Listeners for Hazelcast Clients</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="globaleventconfiguration.html#global-event-configuration">Global Event Configuration</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="distributedcomputing.html#distributed-computing">Distributed Computing</a>
            
                
    <ol>
    
        <li>
             <a href="distributedcomputing.html#executor-service">Executor Service</a>
            
                
    <ol>
    
        <li>
             <a href="distributedcomputing.html#implementing-a-callable-task">Implementing a Callable Task</a>
            
        </li>
    
        <li>
             <a href="distributedcomputing.html#implementing-a-runnable-task">Implementing a Runnable Task</a>
            
        </li>
    
        <li>
             <a href="distributedcomputing.html#scaling-the-executor-service">Scaling The Executor Service</a>
            
        </li>
    
        <li>
             <a href="execution.html#executing-code-in-the-cluster">Executing Code in the Cluster</a>
            
        </li>
    
        <li>
             <a href="executioncancellation.html#canceling-an-executing-task">Canceling an Executing Task</a>
            
        </li>
    
        <li>
             <a href="executioncallback.html#callback-when-task-completes">Callback When Task Completes</a>
            
        </li>
    
        <li>
             <a href="executionmemberselector.html#selecting-members-for-task-execution">Selecting Members for Task Execution</a>
            
        </li>
    
        <li>
             <a href="executionmemberselector.html#configuring-executor-service">Configuring Executor Service</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="durableexecutorservice.html#durable-executor-service">Durable Executor Service</a>
            
                
    <ol>
    
        <li>
             <a href="durableexecutorservice.html#configuring-durable-executor-service">Configuring Durable Executor Service</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="entryprocessor.html#entry-processor">Entry Processor</a>
            
                
    <ol>
    
        <li>
             <a href="entryprocessor.html#performing-fast-in-memory-map-operations">Performing Fast In-Memory Map Operations</a>
            
        </li>
    
        <li>
             <a href="entryprocessor.html#using-indexes">Using Indexes</a>
            
        </li>
    
        <li>
             <a href="entryprocessor.html#using-object-in-memory-format">Using OBJECT In-Memory Format</a>
            
        </li>
    
        <li>
             <a href="entryprocessor.html#entryprocessor-interface">`EntryProcessor` Interface</a>
            
        </li>
    
        <li>
             <a href="entryprocessor.html#processing-backup-entries">Processing Backup Entries</a>
            
        </li>
    
        <li>
             <a href="entryprocessorsamplecode.html#creating-an-entry-processor">Creating an Entry Processor</a>
            
        </li>
    
        <li>
             <a href="entryprocessorabstract.html#abstract-entry-processor">Abstract Entry Processor</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="distributedquery.html#distributed-query">Distributed Query</a>
            
                
    <ol>
    
        <li>
             <a href="distributedquery.html#how-distributed-query-works">How Distributed Query Works</a>
            
                
    <ol>
    
        <li>
             <a href="distributedquery.html#employee-map-query-example">Employee Map Query Example</a>
            
        </li>
    
        <li>
             <a href="querycriteriaapi.html#querying-with-criteria-api">Querying with Criteria API</a>
            
        </li>
    
        <li>
             <a href="querysql.html#querying-with-sql">Querying with SQL</a>
            
        </li>
    
        <li>
             <a href="querypagingpredicate.html#filtering-with-paging-predicates">Filtering with Paging Predicates</a>
            
        </li>
    
        <li>
             <a href="queryindexing.html#indexing-queries">Indexing Queries</a>
            
        </li>
    
        <li>
             <a href="querythreadconfiguration.html#configuring-query-thread-pool">Configuring Query Thread Pool</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="querycollections.html#querying-in-collections-and-arrays">Querying in Collections and Arrays</a>
            
                
    <ol>
    
        <li>
             <a href="querycollections.html#indexing-in-collections-and-arrays">Indexing in Collections and Arrays</a>
            
        </li>
    
        <li>
             <a href="querycollections.html#corner-cases">Corner cases</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="customattributes.html#custom-attributes">Custom Attributes</a>
            
                
    <ol>
    
        <li>
             <a href="customattributes.html#implementing-a-valueextractor">Implementing a ValueExtractor</a>
            
        </li>
    
        <li>
             <a href="customattributes.html#extraction-arguments">Extraction Arguments</a>
            
        </li>
    
        <li>
             <a href="customattributes.html#configuring-a-custom-attribute-programmatically">Configuring a Custom Attribute Programmatically</a>
            
        </li>
    
        <li>
             <a href="customattributes.html#configuring-a-custom-attribute-declaratively">Configuring a Custom Attribute Declaratively</a>
            
        </li>
    
        <li>
             <a href="customattributes.html#indexing-custom-attributes">Indexing Custom Attributes</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="mapreduce.html#mapreduce">MapReduce</a>
            
                
    <ol>
    
        <li>
             <a href="mr-essentials.html#understanding-mapreduce">Understanding MapReduce</a>
            
        </li>
    
        <li>
             <a href="mr-introduction.html#using-the-mapreduce-api">Using the MapReduce API</a>
            
        </li>
    
        <li>
             <a href="mr-architecture.html#hazelcast-mapreduce-architecture">Hazelcast MapReduce Architecture</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="aggregators.html#aggregators">Aggregators</a>
            
                
    <ol>
    
        <li>
             <a href="aggregators.html#aggregations-basics">Aggregations Basics</a>
            
        </li>
    
        <li>
             <a href="aggregatorsintroduction.html#using-the-aggregations-api">Using the Aggregations API</a>
            
        </li>
    
        <li>
             <a href="aggregatorsexample.html#aggregations-examples">Aggregations Examples</a>
            
        </li>
    
        <li>
             <a href="aggregatorsimplementation.html#implementing-aggregations">Implementing Aggregations</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="continuousquerycache.html#continuous-query-cache">Continuous Query Cache</a>
            
                
    <ol>
    
        <li>
             <a href="continuousquerycache.html#keeping-query-results-local-and-ready">Keeping Query Results Local and Ready</a>
            
        </li>
    
        <li>
             <a href="continuousquerycache.html#accessing-continuous-query-cache-from-member">Accessing Continuous Query Cache from Member</a>
            
        </li>
    
        <li>
             <a href="continuousquerycache.html#accessing-continuous-query-cache-from-client-side">Accessing Continuous Query Cache from Client Side</a>
            
        </li>
    
        <li>
             <a href="continuousquerycache.html#features-of-continuous-query-cache">Features of Continuous Query Cache</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="transactions.html#transactions">Transactions</a>
            
                
    <ol>
    
        <li>
             <a href="transactions.html#creating-a-transaction-interface">Creating a Transaction Interface</a>
            
                
    <ol>
    
        <li>
             <a href="transactions.html#queuesetlist-vs-mapmultimap">Queue/Set/List vs. Map/Multimap</a>
            
        </li>
    
        <li>
             <a href="transactions.html#one-phase-vs-two-phase">ONE_PHASE vs. TWO_PHASE</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="xatransactions.html#providing-xa-transactions">Providing XA Transactions</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="j2eeintegration.html#integrating-into-j2ee">Integrating into J2EE</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache.html#hazelcast-jcache">Hazelcast JCache</a>
            
                
    <ol>
    
        <li>
             <a href="jcache.html#jcache-overview">JCache Overview</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-setup.html#jcache-setup-and-configuration">JCache Setup and Configuration</a>
            
                
    <ol>
    
        <li>
             <a href="jcache-setup.html#setting-up-your-application">Setting up Your Application</a>
            
        </li>
    
        <li>
             <a href="jcache-quickexample.html#example-jcache-application">Example JCache Application</a>
            
        </li>
    
        <li>
             <a href="jcache-jcacheconfiguration.html#configuring-for-jcache">Configuring for JCache</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-providers.html#jcache-providers">JCache Providers</a>
            
                
    <ol>
    
        <li>
             <a href="jcache-providers.html#configuring-jcache-provider">Configuring JCache Provider</a>
            
        </li>
    
        <li>
             <a href="jcache-clientprovider.html#configuring-jcache-with-client-provider">Configuring JCache with Client Provider</a>
            
        </li>
    
        <li>
             <a href="jcache-serverprovider.html#configuring-jcache-with-server-provider">Configuring JCache with Server Provider</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-api.html#jcache-api">JCache API</a>
            
                
    <ol>
    
        <li>
             <a href="jcache-api.html#jcache-api-application-example">JCache API Application Example</a>
            
        </li>
    
        <li>
             <a href="jcache-apibasics.html#jcache-base-classes">JCache Base Classes</a>
            
        </li>
    
        <li>
             <a href="jcache-apifactory.html#implementing-factory-and-factorybuilder">Implementing Factory and FactoryBuilder</a>
            
        </li>
    
        <li>
             <a href="jcache-apicacheloader.html#implementing-cacheloader">Implementing CacheLoader</a>
            
        </li>
    
        <li>
             <a href="jcache-apicachewriter.html#cachewriter">CacheWriter</a>
            
        </li>
    
        <li>
             <a href="jcache-ep.html#implementing-entryprocessor">Implementing EntryProcessor</a>
            
        </li>
    
        <li>
             <a href="jcache-cacheentrylistener.html#cacheentrylistener">CacheEntryListener</a>
            
        </li>
    
        <li>
             <a href="jcache-expirepolicy.html#expirepolicy">ExpirePolicy</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-hzinstance.html#jcache-hazelcast-instance-integration">JCache - Hazelcast Instance Integration</a>
            
                
    <ol>
    
        <li>
             <a href="jcache-hzinstance.html#jcache-and-hazelcast-instance-awareness">JCache and Hazelcast Instance Awareness</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-icache.html#hazelcast-jcache-extension-icache">Hazelcast JCache Extension - ICache</a>
            
                
    <ol>
    
        <li>
             <a href="jcache-icache.html#scoping-to-join-clusters">Scoping to Join Clusters</a>
            
        </li>
    
        <li>
             <a href="jcache-icache.html#namespacing">Namespacing</a>
            
        </li>
    
        <li>
             <a href="jcache-retrievingicacheinstance.html#retrieving-an-icache-instance">Retrieving an ICache Instance</a>
            
        </li>
    
        <li>
             <a href="jcache-icacheconf.html#icache-configuration">ICache Configuration</a>
            
        </li>
    
        <li>
             <a href="jcache-asyncoperations.html#icache-async-methods">ICache Async Methods</a>
            
        </li>
    
        <li>
             <a href="jcache-customexpirypolicy.html#defining-a-custom-expirypolicy">Defining a Custom ExpiryPolicy</a>
            
        </li>
    
        <li>
             <a href="jcache-eviction.html#jcache-eviction">JCache Eviction</a>
            
        </li>
    
        <li>
             <a href="jcache-nearcache.html#jcache-near-cache">JCache Near Cache</a>
            
        </li>
    
        <li>
             <a href="jcache-additionalmethods.html#icache-convenience-methods">ICache Convenience Methods</a>
            
        </li>
    
        <li>
             <a href="jcache-backupawareep.html#implementing-backupawareentryprocessor">Implementing BackupAwareEntryProcessor</a>
            
        </li>
    
        <li>
             <a href="jcache-partitionlostlistener.html#icache-partition-lost-listener">ICache Partition Lost Listener</a>
            
        </li>
    
        <li>
             <a href="jcache-splitbrain.html#jcache-split-brain">JCache Split-Brain</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jcache-speccompliance.html#testing-for-jcache-specification-compliance">Testing for JCache Specification Compliance</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="integratedclustering.html#integrated-clustering">Integrated Clustering</a>
            
                
    <ol>
    
        <li>
             <a href="hibernate.html#hibernate-second-level-cache">Hibernate Second Level Cache</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="websessionreplication.html#web-session-replication">Web Session Replication</a>
            
                
    <ol>
    
        <li>
             <a href="websessionreplication.html#filter-based-web-session-replication">Filter Based Web Session Replication</a>
            
        </li>
    
        <li>
             <a href="tomcatsessionreplication.html#tomcat-based-web-session-replication">Tomcat Based Web Session Replication</a>
            
        </li>
    
        <li>
             <a href="jettysessionreplication.html#jetty-based-web-session-replication">Jetty Based Web Session Replication</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="springintegration.html#spring-integration">Spring Integration</a>
            
                
    <ol>
    
        <li>
             <a href="springintegration.html#supported-versions">Supported Versions</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#configuring-spring">Configuring Spring</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#enabling-springaware-objects">Enabling SpringAware Objects</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#adding-caching-to-spring">Adding Caching to Spring</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#configuring-hibernate-second-level-cache">Configuring Hibernate Second Level Cache</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#configuring-hazelcast-transaction-manager">Configuring Hazelcast Transaction Manager</a>
            
        </li>
    
        <li>
             <a href="springintegration.html#best-practices">Best Practices</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="storage-hd.html#storage">Storage</a>
            
                
    <ol>
    
        <li>
             <a href="storage-hd.html#high-density-memory-store">High-Density Memory Store</a>
            
                
    <ol>
    
        <li>
             <a href="configuringhd.html#configuring-high-density-memory-store">Configuring High-Density Memory Store</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="sizingpractices.html#sizing-practices">Sizing Practices</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#hot-restart-persistence">Hot Restart Persistence</a>
            
                
    <ol>
    
        <li>
             <a href="hotrestartpersistence.html#hot-restart-persistence-overview">Hot Restart Persistence Overview</a>
            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#configuring-hot-restart">Configuring Hot Restart</a>
            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#hot-restart-and-ip-address-port">Hot Restart and IP Address-Port</a>
            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#hot-restart-persistence-design-details">Hot Restart Persistence Design Details</a>
            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#concurrent-incremental-generational-gc">Concurrent, Incremental, Generational GC</a>
            
        </li>
    
        <li>
             <a href="hotrestartpersistence.html#hot-restart-performance-considerations">Hot Restart Performance Considerations</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclient.html#hazelcast-java-client">Hazelcast Java Client</a>
            
                
    <ol>
    
        <li>
             <a href="javaclient.html#hazelcast-clients-feature-comparison">Hazelcast Clients Feature Comparison</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclientoverview.html#java-client-overview">Java Client Overview</a>
            
                
    <ol>
    
        <li>
             <a href="javaclientoverview.html#including-dependencies-for-java-clients">Including Dependencies for Java Clients</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#getting-started-with-client-api">Getting Started with Client API</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#java-client-operation-modes">Java Client Operation Modes</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#handling-failures">Handling Failures</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#using-supported-distributed-data-structures">Using Supported Distributed Data Structures</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#using-client-services">Using Client Services</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#client-listeners">Client Listeners</a>
            
        </li>
    
        <li>
             <a href="javaclientoverview.html#client-transactions">Client Transactions</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#configuring-java-client">Configuring Java Client</a>
            
                
    <ol>
    
        <li>
             <a href="javaclientconfiguration.html#configuring-client-network">Configuring Client Network</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#configuring-client-load-balancer">Configuring Client Load Balancer</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#configuring-client-near-cache">Configuring Client Near Cache</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#client-group-configuration">Client Group Configuration</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#client-security-configuration">Client Security Configuration</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#client-serialization-configuration">Client Serialization Configuration</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#configuring-client-listeners">Configuring Client Listeners</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#executorpoolsize">ExecutorPoolSize</a>
            
        </li>
    
        <li>
             <a href="javaclientconfiguration.html#classloader">ClassLoader</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclientsystemproperties.html#client-system-properties">Client System Properties</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclientsystemproperties.html#sample-codes-for-client">Sample Codes for Client</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="javaclienthd.html#using-high-density-memory-store-with-java-client">Using High-Density Memory Store with Java Client</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="otherclientsintro.html#other-client-and-language-implementations">Other Client and Language Implementations</a>
            
                
    <ol>
    
        <li>
             <a href="cplusclient.html#c-client">C++ Client</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="csharpclient.html#net-client">.NET Client</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="restclient.html#rest-client">REST Client</a>
            
                
    <ol>
    
        <li>
             <a href="restclient.html#rest-client-getpostdelete-examples">REST Client GET/POST/DELETE Examples</a>
            
        </li>
    
        <li>
             <a href="restclient.html#checking-the-status-of-the-cluster-for-rest-client">Checking the Status of the Cluster for REST Client</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="memcacheclient.html#memcache-client">Memcache Client</a>
            
                
    <ol>
    
        <li>
             <a href="memcacheclient.html#memcache-client-code-examples">Memcache Client Code Examples</a>
            
        </li>
    
        <li>
             <a href="memcacheclient.html#unsupported-operations-for-memcache">Unsupported Operations for Memcache</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="serialization.html#serialization">Serialization</a>
            
                
    <ol>
    
        <li>
             <a href="serializationinterfaces.html#serialization-interface-types">Serialization Interface Types</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="serializationcomparisontable.html#comparing-serialization-interfaces">Comparing Serialization Interfaces</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="serializable.html#implementing-java-serializable-and-externalizable">Implementing Java Serializable and Externalizable</a>
            
                
    <ol>
    
        <li>
             <a href="serializable.html#implementing-java-externalizable">Implementing Java Externalizable</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="dataserialization.html#implementing-dataserializable">Implementing DataSerializable</a>
            
                
    <ol>
    
        <li>
             <a href="dataserialization.html#identifieddataserializable">IdentifiedDataSerializable</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="portableserialization.html#implementing-portable-serialization">Implementing Portable Serialization</a>
            
                
    <ol>
    
        <li>
             <a href="portableserialization.html#portable-serialization-example-code">Portable Serialization Example Code</a>
            
        </li>
    
        <li>
             <a href="portableserialization.html#registering-the-portable-factory">Registering the Portable Factory</a>
            
        </li>
    
        <li>
             <a href="portableserialization.html#versioning-for-portable-serialization">Versioning for Portable Serialization</a>
            
        </li>
    
        <li>
             <a href="portableserialization.html#null-portable-serialization">Null Portable Serialization</a>
            
        </li>
    
        <li>
             <a href="portableserialization.html#distributedobject-serialization">DistributedObject Serialization</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="customserialization.html#custom-serialization">Custom Serialization</a>
            
                
    <ol>
    
        <li>
             <a href="customserialization.html#implementing-streamserializer">Implementing StreamSerializer</a>
            
        </li>
    
        <li>
             <a href="bytearrayserializer.html#implementing-bytearrayserializer">Implementing ByteArraySerializer</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="globalserializer.html#global-serializer">Global Serializer</a>
            
                
    <ol>
    
        <li>
             <a href="globalserializer.html#sample-global-serializer">Sample Global Serializer</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="hazelcastinstanceaware.html#implementing-hazelcastinstanceaware">Implementing HazelcastInstanceAware</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="serializationconfigurationwrapup.html#serialization-configuration-wrap-up">Serialization Configuration Wrap-Up</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="management.html#management">Management</a>
            
                
    <ol>
    
        <li>
             <a href="management.html#getting-member-statistics">Getting Member Statistics</a>
            
                
    <ol>
    
        <li>
             <a href="management.html#map-statistics">Map Statistics</a>
            
        </li>
    
        <li>
             <a href="management.html#multimap-statistics">Multimap Statistics</a>
            
        </li>
    
        <li>
             <a href="management.html#queue-statistics">Queue Statistics</a>
            
        </li>
    
        <li>
             <a href="management.html#topic-statistics">Topic Statistics</a>
            
        </li>
    
        <li>
             <a href="management.html#executor-statistics">Executor Statistics</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="jmxapipermember.html#jmx-api-per-member">JMX API per Member</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="monitoringwithjmx.html#monitoring-with-jmx">Monitoring with JMX</a>
            
                
    <ol>
    
        <li>
             <a href="monitoringwithjmx.html#mbean-naming-for-hazelcast-data-structures">MBean Naming for Hazelcast Data Structures</a>
            
        </li>
    
        <li>
             <a href="monitoringwithjmx.html#connecting-to-jmx-agent">Connecting to JMX Agent</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="clusterutilities.html#cluster-utilities">Cluster Utilities</a>
            
                
    <ol>
    
        <li>
             <a href="clusterutilities.html#getting-member-events-and-member-sets">Getting Member Events and Member Sets</a>
            
        </li>
    
        <li>
             <a href="managingclusterstate.html#managing-cluster-and-member-states">Managing Cluster and Member States</a>
            
        </li>
    
        <li>
             <a href="clustershscript.html#using-the-script-clustersh">Using the Script cluster.sh</a>
            
        </li>
    
        <li>
             <a href="restforclustermanagement.html#using-rest-api-for-cluster-management">Using REST API for Cluster Management</a>
            
        </li>
    
        <li>
             <a href="litemember.html#enabling-lite-members">Enabling Lite Members</a>
            
        </li>
    
        <li>
             <a href="memberattributes.html#defining-member-attributes">Defining Member Attributes</a>
            
        </li>
    
        <li>
             <a href="clustermembersafetycheck.html#safety-checking-cluster-members">Safety Checking Cluster Members</a>
            
        </li>
    
        <li>
             <a href="clusterquorum.html#defining-a-cluster-quorum">Defining a Cluster Quorum</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="hazelcast-cli.html#hazelcast-cli">Hazelcast CLI</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="managementcenter.html#management-center">Management Center</a>
            
                
    <ol>
    
        <li>
             <a href="managementcenter.html#installing-management-center">Installing Management Center</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#getting-started-to-management-center">Getting Started to Management Center</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#management-center-tools">Management Center Tools</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#management-center-home-page">Management Center Home Page</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-caches">Monitoring Caches</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#managing-maps">Managing Maps</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-replicated-maps">Monitoring Replicated Maps</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-queues">Monitoring Queues</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-topics">Monitoring Topics</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-multimaps">Monitoring MultiMaps</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-executors">Monitoring Executors</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-wan-replication">Monitoring WAN Replication</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#monitoring-members">Monitoring Members</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#scripting">Scripting</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#executing-console-commands">Executing Console Commands</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#creating-alerts">Creating Alerts</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#administering-management-center">Administering Management Center</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#hot-restart">Hot Restart</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#checking-past-status-with-time-travel">Checking Past Status with Time Travel</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#management-center-documentation">Management Center Documentation</a>
            
        </li>
    
        <li>
             <a href="managementcenter.html#suggested-heap-size">Suggested Heap Size</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="clusteredjmx.html#clustered-jmx-via-management-center">Clustered JMX via Management Center</a>
            
                
    <ol>
    
        <li>
             <a href="clusteredjmx.html#configuring-clustered-jmx">Configuring Clustered JMX</a>
            
        </li>
    
        <li>
             <a href="clusteredjmx.html#clustered-jmx-api">Clustered JMX API</a>
            
        </li>
    
        <li>
             <a href="clusteredjmx.html#integrating-with-new-relic">Integrating with New Relic</a>
            
        </li>
    
        <li>
             <a href="clusteredjmx.html#integrating-with-appdynamics">Integrating with AppDynamics</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="clusteredrest.html#clustered-rest-via-management-center">Clustered REST via Management Center</a>
            
                
    <ol>
    
        <li>
             <a href="clusteredrest.html#enabling-clustered-rest">Enabling Clustered REST</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#clustered-rest-api-root">Clustered REST API Root</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#clusters-resource">Clusters Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#cluster-resource">Cluster Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#members-resource">Members Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#member-resource">Member Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#clients-resource">Clients Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#maps-resource">Maps Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#multimaps-resource">MultiMaps Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#queues-resource">Queues Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#topics-resource">Topics Resource</a>
            
        </li>
    
        <li>
             <a href="clusteredrest.html#executors-resource">Executors Resource</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="security.html#security">Security</a>
            
                
    <ol>
    
        <li>
             <a href="security.html#enabling-security-for-hazelcast-enterprise">Enabling Security for Hazelcast Enterprise</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="socketinterceptor.html#socket-interceptor">Socket Interceptor</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="securityinterceptor.html#security-interceptor">Security Interceptor</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="encryption.html#encryption">Encryption</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="ssl.html#ssl">SSL</a>
            
                
    <ol>
    
        <li>
             <a href="ssl.html#ssl-for-hazelcast-members">SSL for Hazelcast Members</a>
            
        </li>
    
        <li>
             <a href="ssl.html#ssl-for-hazelcast-clients">SSL for Hazelcast Clients</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="credentials.html#credentials">Credentials</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="clusterloginmodule.html#clusterloginmodule">ClusterLoginModule</a>
            
                
    <ol>
    
        <li>
             <a href="clusterloginmodule.html#enterprise-integration">Enterprise Integration</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="clustermembersecurity.html#cluster-member-security">Cluster Member Security</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="nativeclientsecurity.html#native-client-security">Native Client Security</a>
            
                
    <ol>
    
        <li>
             <a href="nativeclientsecurity.html#authentication">Authentication</a>
            
        </li>
    
        <li>
             <a href="nativeclientsecurity.html#authorization">Authorization</a>
            
        </li>
    
        <li>
             <a href="nativeclientsecurity.html#permissions">Permissions</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="performance.html#performance">Performance</a>
            
                
    <ol>
    
        <li>
             <a href="performance.html#data-affinity">Data Affinity</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="backpressure.html#back-pressure">Back Pressure</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="threadingmodel.html#threading-model">Threading Model</a>
            
                
    <ol>
    
        <li>
             <a href="threadingmodel.html#io-threading">I/O Threading</a>
            
        </li>
    
        <li>
             <a href="threadingevent.html#event-threading">Event Threading</a>
            
        </li>
    
        <li>
             <a href="threadingexecutor.html#iexecutor-threading">IExecutor Threading</a>
            
        </li>
    
        <li>
             <a href="threadingoperation.html#operation-threading">Operation Threading</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="slowoperationdetector.html#slowoperationdetector">SlowOperationDetector</a>
            
                
    <ol>
    
        <li>
             <a href="slowoperationdetector.html#logging-of-slow-operations">Logging of Slow Operations</a>
            
        </li>
    
        <li>
             <a href="slowoperationdetector.html#purging-of-slow-operation-logs">Purging of Slow Operation Logs</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="simulator.html#hazelcast-simulator">Hazelcast Simulator</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="wan.html#wan">WAN</a>
            
                
    <ol>
    
        <li>
             <a href="wan.html#wan-replication">WAN Replication</a>
            
                
    <ol>
    
        <li>
             <a href="definingwanreplication.html#defining-wan-replication">Defining WAN Replication</a>
            
        </li>
    
        <li>
             <a href="definingwanreplication.html#wanbatchreplication-implementation">WanBatchReplication Implementation</a>
            
        </li>
    
        <li>
             <a href="configuringwanreplicationformapcache.html#configuring-wan-replication-for-imap-and-icache">Configuring WAN Replication for IMap and ICache</a>
            
        </li>
    
        <li>
             <a href="batchsize.html#batch-size">Batch Size</a>
            
        </li>
    
        <li>
             <a href="batchmaximumdelay.html#batch-maximum-delay">Batch Maximum Delay</a>
            
        </li>
    
        <li>
             <a href="responsetimeout.html#response-timeout">Response Timeout</a>
            
        </li>
    
        <li>
             <a href="queuecapacity.html#queue-capacity">Queue Capacity</a>
            
        </li>
    
        <li>
             <a href="queuefullbehavior.html#queue-full-behavior">Queue Full Behavior</a>
            
        </li>
    
        <li>
             <a href="eventfiltering.html#event-filtering-api">Event Filtering API</a>
            
        </li>
    
        <li>
             <a href="acktypes.html#acknowledgment-types">Acknowledgment Types</a>
            
        </li>
    
        <li>
             <a href="wansync.html#synchronizing-wan-target-cluster">Synchronizing WAN Target Cluster</a>
            
        </li>
    
        <li>
             <a href="wanreplicationadditionalinfo.html#wan-replication-additional-information">WAN Replication Additional Information</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="solaceintegration.html#solace-integration">Solace Integration</a>
            
                
    <ol>
    
        <li>
             <a href="solaceintegration.html#installing-solace-jars">Installing Solace JARs</a>
            
        </li>
    
        <li>
             <a href="solaceintegration.html#enabling-integration">Enabling Integration</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="osgi.html#osgi">OSGI</a>
            
                
    <ol>
    
        <li>
             <a href="osgi.html#osgi-support">OSGI Support</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="osgi.html#api">API</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="osgi.html#configuring-hazelcast-osgi-support">Configuring Hazelcast OSGI Support</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="osgi.html#design">Design</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="osgi.html#using-hazelcast-osgi-service">Using Hazelcast OSGI Service</a>
            
                
    <ol>
    
        <li>
             <a href="osgi.html#getting-hazelcast-osgi-service-instances">Getting Hazelcast OSGI Service Instances</a>
            
        </li>
    
        <li>
             <a href="osgi.html#managing-and-using-hazelcast-instances">Managing and Using Hazelcast instances</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="extendinghazelcast.html#extending-hazelcast">Extending Hazelcast</a>
            
                
    <ol>
    
        <li>
             <a href="userdefinedservices.html#user-defined-services">User Defined Services</a>
            
                
    <ol>
    
        <li>
             <a href="spicreateclass.html#creating-the-service-class">Creating the Service Class</a>
            
        </li>
    
        <li>
             <a href="spienableclass.html#enabling-the-service-class">Enabling the Service Class</a>
            
        </li>
    
        <li>
             <a href="spiaddproperties.html#adding-properties-to-the-service">Adding Properties to the Service</a>
            
        </li>
    
        <li>
             <a href="spistartservice.html#starting-the-service">Starting the Service</a>
            
        </li>
    
        <li>
             <a href="spiproxy.html#placing-a-remote-call-via-proxy">Placing a Remote Call via Proxy</a>
            
        </li>
    
        <li>
             <a href="spicreatecontainer.html#creating-containers">Creating Containers</a>
            
        </li>
    
        <li>
             <a href="spipartitionmigration.html#partition-migration">Partition Migration</a>
            
        </li>
    
        <li>
             <a href="spicreatebackups.html#creating-backups">Creating Backups</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="spiwaitnotifyservice.html#waitnotifyservice">WaitNotifyService</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="discoveryspi.html#discovery-spi">Discovery SPI</a>
            
                
    <ol>
    
        <li>
             <a href="discoveryoverview.html#discovery-spi-interfaces-and-classes">Discovery SPI Interfaces and Classes</a>
            
        </li>
    
        <li>
             <a href="discoverystrategy.html#discovery-strategy">Discovery Strategy</a>
            
        </li>
    
        <li>
             <a href="discoveryintegration.html#discoveryservice-framework-integration">DiscoveryService (Framework integration)</a>
            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="config-properties.html#config-properties-spi">Config Properties SPI</a>
            
                
    <ol>
    
        <li>
             <a href="config-properties.html#config-properties-spi-classes">Config Properties SPI Classes</a>
            
        </li>
    
        <li>
             <a href="config-properties.html#config-properties-spi-example">Config Properties SPI Example</a>
            
        </li>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#network-partitioning-split-brain-syndrome">Network Partitioning - Split Brain Syndrome</a>
            
                
    <ol>
    
        <li>
             <a href="networkpartitioning.html#understanding-partition-recreation">Understanding Partition Recreation</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#understanding-backup-partition-creation">Understanding Backup Partition Creation</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#understanding-the-update-overwrite-scenario">Understanding The Update Overwrite Scenario</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#what-happens-when-the-network-failure-is-fixed">What Happens When The Network Failure Is Fixed</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#how-hazelcast-split-brain-merge-happens">How Hazelcast Split Brain Merge Happens</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="networkpartitioning.html#specifying-merge-policies">Specifying Merge Policies</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="systemproperties-member.html#system-properties">System Properties</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="commonexceptiontypes.html#common-exception-types">Common Exception Types</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="licenses.html#license-questions">License Questions</a>
            
                
    <ol>
    
        <li>
             <a href="licenses.html#embedded-dependencies">Embedded Dependencies</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="licenses.html#runtime-dependencies">Runtime Dependencies</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#frequently-asked-questions">Frequently Asked Questions</a>
            
                
    <ol>
    
        <li>
             <a href="faq.html#why-271-as-the-default-partition-count">Why 271 as the default partition count?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#is-hazelcast-thread-safe">Is Hazelcast thread safe?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-members-discover-each-other">How do members discover each other?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#what-happens-when-a-member-goes-down">What happens when a member goes down?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-i-test-the-connectivity">How do I test the connectivity?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-i-choose-keys-properly">How do I choose keys properly?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-i-reflect-value-modifications">How do I reflect value modifications?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-i-test-my-hazelcast-cluster">How do I test my Hazelcast cluster?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#does-hazelcast-support-hundreds-of-members">Does Hazelcast support hundreds of members?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#does-hazelcast-support-thousands-of-clients">Does Hazelcast support thousands of clients?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#difference-between-lite-member-and-smart-client">Difference between Lite Member and Smart Client?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-you-give-support">How do you give support?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#does-hazelcast-persist">Does Hazelcast persist?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#can-i-use-hazelcast-in-a-single-server">Can I use Hazelcast in a single server?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-can-i-monitor-hazelcast">How can I monitor Hazelcast?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-can-i-see-debug-level-logs">How can I see debug level logs?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#client-server-vs-embedded-topologies">Client-server vs. embedded topologies?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-do-i-know-it-is-safe-to-kill-the-second-member">How do I know it is safe to kill the second member?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#when-do-i-need-native-memory-solutions">When do I need Native Memory solutions?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#is-there-any-disadvantage-of-using-near-cache">Is there any disadvantage of using near-cache?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#is-hazelcast-secure">Is Hazelcast secure?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#how-can-i-set-socket-options">How can I set socket options?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#client-disconnections-during-idle-time">Client disconnections during idle time?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#oome-unable-to-create-new-native-thread">OOME: Unable to create new native thread?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#does-repartitioning-wait-for-entry-processor">Does repartitioning wait for Entry Processor?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#instances-on-different-machines-cannot-see-each-other">Instances on different machines cannot see each other?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
        <li>
             <a href="faq.html#what-does-replica-1-has-no-owner-mean">What Does &quot;Replica: 1 has no owner&quot; Mean?</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

            
        </li>
    
        <li>
             <a href="glossary.html#glossary">Glossary</a>
            
                
    <ol>
    
</ol>

            
        </li>
    
</ol>

	    </nav>
	    
	    <nav id="links">
	        <ul>
	            
	        </ul>
	    </nav>
	    	
	</div>

	        <div id="content">
	            <a name="preface"></a><h1 id="preface">Preface</h1>
<p>Welcome to the Hazelcast Reference Manual. This manual includes concepts, instructions, and samples to guide you on how to use Hazelcast and build Hazelcast applications.</p>
<p>As the reader of this manual, you must be familiar with the Java programming language and you should have installed your preferred Integrated Development Environment (IDE).</p>
<a name="hazelcast-editions"></a><h2 id="hazelcast-editions">Hazelcast Editions</h2>
<p>This Reference Manual covers all editions of Hazelcast. Throughout this manual:</p>
<ul>
<li><strong>Hazelcast</strong> refers to the open source edition of Hazelcast in-memory data grid middleware. It is also the name of the company (Hazelcast, Inc.) providing the Hazelcast product.</li>
<li><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font> is a commercially licensed edition of Hazelcast which provides high-value enterprise features in addition to Hazelcast.</li>
<li><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font> is a commercially licensed edition of Hazelcast which provides High-Density (HD) Memory Store and Hot Restart Persistence features in addition to Hazelcast Enterprise.</li>
</ul>
<a name="hazelcast-architecture"></a><h2 id="hazelcast-architecture">Hazelcast Architecture</h2>
<p>You can see the features for all Hazelcast editions in the following architecture diagram.</p>
<p><img src="images/HazelcastArchNew.png" alt="Hazelcast Architecture"></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong> <em>You can see small &quot;HD&quot; boxes for some features in the above diagram. Those features can use High-Density (HD) Memory Store when it is available. It means if you have Hazelcast Enterprise HD, you can use those features with HD Memory Store.</em></p>
<p>For more information on Hazelcast&#39;s Architecture, please see the white paper <a href="https://hazelcast.com/resources/architects-view-hazelcast/" target="_blank">An Architects View of Hazelcast</a>.</p>
<a name="hazelcast-plugins"></a><h2 id="hazelcast-plugins">Hazelcast Plugins</h2>
<p>You can extend Hazelcast&#39;s functionality by using its plugins. These plugins have their own lifecycles. Please see <a href="http://hazelcast.org/plugins/" target="_blank">Plugins page</a> to learn about Hazelcast plugins you can use. Hazelcast plugins are marked with <img src="images/Plugin_New.png" alt="Plugin" height="12" width="54"> label throughout this manual.</p>
<a name="licensing"></a><h2 id="licensing">Licensing</h2>
<p>Hazelcast and Hazelcast Reference Manual are free and provided under the Apache License, Version 2.0. Hazelcast Enterprise is commercially licensed by Hazelcast, Inc.</p>
<p>For more detailed information on licensing, please see the <a href="#license-questions">License Questions appendix</a>.</p>
<a name="trademarks"></a><h2 id="trademarks">Trademarks</h2>
<p>Hazelcast is a registered trademark of Hazelcast, Inc. All other trademarks in this manual are held by their respective owners. </p>
<a name="customer-support"></a><h2 id="customer-support">Customer Support</h2>
<p>Support for Hazelcast is provided via <a href="https://github.com/hazelcast/hazelcast/issues" target="_blank">GitHub</a>, <a href="https://groups.google.com/forum/#!forum/hazelcast" target="_blank">Mail Group</a> and <a href="http://www.stackoverflow.com" target="_blank">StackOverflow</a></p>
<p>For information on the commercial support for Hazelcast and Hazelcast Enterprise, please see 
<a href="https://hazelcast.com/pricing/" target="_blank">hazelcast.com</a>.</p>
<a name="release-notes"></a><h2 id="release-notes">Release Notes</h2>
<p>Please refer to the <a href="http://docs.hazelcast.org/docs/release-notes/" target="_blank">Release Notes document</a> for the new features, enhancements and fixes performed for each Hazelcast release.</p>
<a name="contributing-to-hazelcast"></a><h2 id="contributing-to-hazelcast">Contributing to Hazelcast</h2>
<p>You can contribute to the Hazelcast code, report a bug, or request an enhancement. Please see the following resources.</p>
<ul>
<li><a href="https://hazelcast.atlassian.net/wiki/display/COM/Developing+with+Git" target="_blank">Developing with Git</a>: Document that explains the branch mechanism of Hazelcast and how to request changes.</li>
<li><a href="https://hazelcast.atlassian.net/wiki/display/COM/Hazelcast+Contributor+Agreement" target="_blank">Hazelcast Contributor Agreement form</a>: Form that each contributing developer needs to fill and send back to Hazelcast.</li>
<li><a href="https://github.com/hazelcast/hazelcast" target="_blank">Hazelcast on GitHub</a>: Hazelcast repository where the code is developed, issues and pull requests are managed.</li>
</ul>
<a name="partners"></a><h2 id="partners">Partners</h2>
<p>Hazelcast partners with leading hardware and software technologies, system integrators, resellers and OEMs including Amazon Web Services, Vert.x, Azul Systems, C2B2. Please see the <a href="https://hazelcast.com/partners/">Partners</a> page for the full list of and information on our partners.</p>
<a name="phone-home"></a><h2 id="phone-home">Phone Home</h2>
<p>Hazelcast uses phone home data to learn about usage of Hazelcast.</p>
<p>Hazelcast member instances call our phone home server initially when they are started and then every 24 hours. This applies to all the instances joined to the cluster.</p>
<p><strong>What is sent in?</strong></p>
<p>The following information is sent in a phone home:</p>
<ul>
<li>Hazelcast version</li>
<li>Local Hazelcast member UUID</li>
<li>Download ID </li>
<li>A hash value of the cluster ID</li>
<li>Cluster size bands for 5, 10, 20, 40, 60, 100, 150, 300, 600 and &gt; 600</li>
<li>Number of connected clients bands of 5, 10, 20, 40, 60, 100, 150, 300, 600 and &gt; 600</li>
<li>Cluster uptime</li>
<li>Member uptime</li>
<li>Environment Information:<ul>
<li>Name of operating system</li>
<li>Kernel architecture (32-bit or 64-bit)</li>
<li>Version of operating system</li>
<li>Version of installed Java</li>
<li>Name of Java Virtual Machine</li>
</ul>
</li>
<li>Hazelcast Enterprise specific: <ul>
<li>Number of clients by language (Java, C++, C#)</li>
<li>Flag for Hazelcast Enterprise </li>
<li>Hash value of license key</li>
<li>Native memory usage</li>
</ul>
</li>
</ul>
<p><strong>Phone Home Code</strong></p>
<p>The phone home code itself is open source. Please see <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/util/PhoneHome.java" target="_blank">here</a>.</p>
<p><strong>Disabling Phone Homes</strong></p>
<p>Set the <code>hazelcast.phone.home.enabled</code> system property to false either in the config or on the Java command line. Please see the <a href="#system-properties">System Properties section</a> for information on how to set a property. </p>
<p><strong>Phone Home URLs</strong></p>
<p>For versions 1.x and 2.x: <a href="http://www.hazelcast.com/version.jsp" target="_blank">http://www.hazelcast.com/version.jsp</a>.</p>
<p>For versions 3.x up to 3.6: <a href="http://versioncheck.hazelcast.com/version.jsp" target="_blank">http://versioncheck.hazelcast.com/version.jsp</a>.</p>
<p>For versions after 3.6: <a href="http://phonehome.hazelcast.com/ping" target="_blank">http://phonehome.hazelcast.com/ping</a>.</p>
<a name="typographical-conventions"></a><h2 id="typographical-conventions">Typographical Conventions</h2>
<p>Below table shows the conventions used in this manual.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Convention</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>bold font</strong></td>
<td style="text-align:left">- Indicates part of a sentence that requires the reader&#39;s specific attention. <br> - Also indicates property/parameter values.</td>
</tr>
<tr>
<td style="text-align:left"><em>italic font</em></td>
<td style="text-align:left">- When italicized words are enclosed with &quot;&lt;&quot; and &quot;&gt;&quot;, it indicates a variable in the command or code syntax that you must replace (for example, <code>hazelcast-&lt;</code><em>version</em><code>&gt;.jar</code>). <br> - Note and Related Information texts are in italics.</td>
</tr>
<tr>
<td style="text-align:left"><code>monospace</code></td>
<td style="text-align:left">Indicates files, folders, class and library names, code snippets, and inline code words in a sentence.</td>
</tr>
<tr>
<td style="text-align:left"><strong><em>RELATED INFORMATION</em></strong></td>
<td style="text-align:left">Indicates a resource that is relevant to the topic, usually with a link or cross-reference.</td>
</tr>
<tr>
<td style="text-align:left"><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong></td>
<td style="text-align:left">Indicates information that is of special interest or importance, for example an additional action required only in certain circumstances.</td>
</tr>
<tr>
<td style="text-align:left">element &amp; attribute</td>
<td style="text-align:left">Mostly used in the context of declarative configuration that you perform using Hazelcast XML file. Element refers to an XML tag used to configure a Hazelcast feature. Attribute is a parameter owned by an element, contributing into the declaration of that element&#39;s configuration. Please see the following example.<br></br><code>&lt;port port-count=&quot;100&quot;&gt;5701&lt;/port&gt;</code><br></br> In this example, <code>port-count</code> is an <strong>attribute</strong> of the <code>port</code> <strong>element</strong>.</td>
</tr>
</tbody>
</table>
<p><br></br></p>

<a name="document-revision-history"></a><h1 id="document-revision-history">Document Revision History</h1>
<p>This chapter lists the changes made to this document from the previous release.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em>Please refer to the <a href="http://docs.hazelcast.org/docs/release-notes/" target="_blank">Release Notes</a> for the new features, enhancements and fixes performed for each Hazelcast release.</em></p>
<p><br></br></p>
<table>
<thead>
<tr>
<th style="text-align:left">Chapter</th>
<th style="text-align:left">Section</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="#preface">Chapter 1 - Preface</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Updated the architecture diagram.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#getting-started">Chapter 3 - Getting Started</a></td>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#deploying-on-microsoft-azure">Deploying on Microsoft Azure</a> and <a href="#deploying-on-pivotal-cloud-foundry">Deploying On Pivotal Cloud Foundry</a> added as a new sections.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#understanding-configuration">Chapter 5 - Understanding Configuration</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Added as a new chapter to provide the fundamentals of Hazelcast configuration.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#setting-up-clusters">Chapter 6 - Setting Up Clusters</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Added as a new chapter to provide all Hazelcast clusters related information.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#discovering-native-clients">Discovering Native Clients</a></td>
<td style="text-align:left">Added as a new section to explain Hazelcast&#39;s multicast discovery plugin.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#discovering-members-with-jclouds">Discovering Members with jclouds</a></td>
<td style="text-align:left">Section&#39;s content moved to its own repo since this feature has become a Hazelcast plugin. You can find its repo&#39;s link in this section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#discovering-members-within-ec2-cloud">Discovering Members within EC2 Cloud</a></td>
<td style="text-align:left">Section&#39;s content moved to its own repo since this feature has become a Hazelcast plugin. You can find its repo&#39;s link in this section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#discovering-members-within-azure-cloud">Discovering Members within Azure Cloud</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#partition-group-configuration">Partition Group Configuration</a></td>
<td style="text-align:left">Added explanations of the new member group types ZONE_AWARE and SPI.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#distributed-data-structures">Chapter 7 - Distributed Data Structures</a></td>
<td style="text-align:left"><a href="#replicated-map">Replicated Map</a></td>
<td style="text-align:left"><a href="#replicating-instead-of-partitioning">Replicating instead of Partitioning</a> updated by adding a note related to replicated map usage in a lite member. The whole section enhanced.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#lock">Lock</a></td>
<td style="text-align:left">Added explanations related to the maximum lease time for locks.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#map">Map</a></td>
<td style="text-align:left">Added the new section <a href="#custom-eviction-policy">Custom Eviction Policy</a> to explain how a customized eviction policy can be plugged.<br></br><a href="#listening-to-map-entries-with-predicates">Listening to Map Entries with Predicates</a> section updated by adding the explanation of a new system property (<code>hazelcast.map.entry.filtering.natural.event.types</code>) that allows better continuous query implementations.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#distributed-computing">Chapter 9 - Distributed Computing</a></td>
<td style="text-align:left"><a href="#using-indexes">Using Indexes</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#durable-executor-service">Durable Executor Service</a></td>
<td style="text-align:left">Added as a new section to describe Hazelcast&#39;s newly introduced data structure, Durable Executor Service.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#distributed-query">Chapter 10 - Distributed Query</a></td>
<td style="text-align:left"><a href="#valueExtractor-with-portable-serialization">ValueExtractor with Portable Serialization</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left">Explanation for the <code>__key</code> attribute added under <a href="#querying-with-sql">Querying with SQL</a> section.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#transactions">Chapter 11 - Transactions</a></td>
<td style="text-align:left"><a href="#integrating-into-j2ee">Integrating into J2EE</a></td>
<td style="text-align:left">Added information related to class loaders.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#hazelcast-jcache">Chapter 12 - Hazelcast JCache</a></td>
<td style="text-align:left"><a href="#icache-configuration">ICache Configuration</a></td>
<td style="text-align:left">Added description of the new element <code>disable-per-entry-invalidation-events</code>.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#jcache-hazelcast-instance-integration">JCache - Hazelcast Instance Integration</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#jcache-eviction">JCache Eviction</a></td>
<td style="text-align:left"><a href="#custom-eviction-policies">Custom Eviction Policies</a> added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#integrated-clustering">Chapter 13 - Integrated Clustering</a></td>
<td style="text-align:left"><a href="#web-session-replication">Web Session Replication</a></td>
<td style="text-align:left">Updated Tomcat and Jetty based web session replication sections since they have become Hazelcast plugins. These sections&#39; content is moved to their own repos. You can find these repos&#39; links in the related sections.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#hibernate-second-level-cache">Hibernate Second Level Cache</a></td>
<td style="text-align:left">Section&#39;s content moved to its own repo since this feature has become a Hazelcast plugin. You can find its repo&#39;s link in this section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#spring-integration">Spring Integration</a></td>
<td style="text-align:left"><a href="#configuring-hazelcast-transaction-manager">Configuring Hazelcast Transaction Manager</a> added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#storage">Chapter 14 - Storage</a></td>
<td style="text-align:left"><a href="#hot-restart-persistence">Hot Restart Persistence</a></td>
<td style="text-align:left">Added the new section <a href="#hot-restart-performance-considerations">Hot Restart Performance Considerations</a> to summarize the results of performance tests of Hot Restart Persistence.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#configuring-high-density-memory-store">Configuring High-Density Memory Store</a></td>
<td style="text-align:left">Enhanced the content for <code>allocator-type</code> configuration element.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#hazelcast-java-client">Chapter 15 - Hazelcast Java Client</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Enhanced the definition for the property <code>hazelcast.client.invocation.timeout.seconds</code>.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#hazelcast-clients-feature-comparison">Feature Comparison</a></td>
<td style="text-align:left">Updated to reflect the latest feature developments.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#client-system-properties">Client System Properties</a></td>
<td style="text-align:left">Added description for the property <code>hazelcast.client.max.concurrent.invocations</code>.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#other-client-and-language-implementations">Chapter 16 - Other Client and Language Implementations</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Content of C++ and .NET clients updated so that the reader is directed to the GitHub repositories of these clients.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#serialization">Chapter 17 - Serialization</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Removed <code>java.lang.Enum</code> from the default types since it is not among the default serializers.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#management">Chapter 18 - Management</a></td>
<td style="text-align:left"><a href="#management-center">Management Center</a></td>
<td style="text-align:left">Added information explaining how to configure Hazelcast Management Center when it is deployed onto an SSL-enabled web container.<br></br>Added information explaining the &quot;Update License&quot; button.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#clustered-jmx-via-management-center">Clustered JMX via Management Center</a></td>
<td style="text-align:left">List of attributes updated by adding the Replicated Map attributes.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#hazelcast-cli">Hazelcast CLI</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#safety-checking-cluster-members">Safety Checking Cluster Members</a></td>
<td style="text-align:left">Updated the content to reflect the improvements in graceful shutdown feature.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#security">Chapter 19 - Security</a></td>
<td style="text-align:left"><a href="#ssl">SSL</a></td>
<td style="text-align:left">Added information explaining the performance overhead for the clients when they use SSL.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#encryption">Encryption</a></td>
<td style="text-align:left">Added a note about the encryption at the client side.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#hazelcast=simulator">Chapter 21 - Hazelcast Simulator</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Moved the content to Simulator&#39;s own GitHub repository at <a href="https://github.com/hazelcast/hazelcast-simulator/blob/master/README.md">Hazelcast Simulator</a>.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#wan">Chapter 22 - WAN</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Updated to reflect the improvement which is the ability of generic WAN replication endpoint configurations.<br></br>Cleared the content related to <code>WanNoDelayReplication</code>  since this implementation has been removed, and added a note under the <a href="#defining-wan-replication">Defining WAN Replication section</a>.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#synchronizing-wan-target-cluster">Synchronizing WAN Target Cluster</a></td>
<td style="text-align:left">Added as a new section.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><a href="#solace-integration">Solace Integration</a></td>
<td style="text-align:left">Added as a new section explaining how to integrate Hazelcast WAN replication with Solace messaging appliances.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#system-properties">Chapter 26 - System Properties</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Added definitions for the new properties: <br></br>- <code>hazelcast.partition.migration.stale.read.disabled</code><br></br>- <code>hazelcast.map.entry.filtering.natural.event.types</code><br></br>- <code>hazelcast.internal.map.expiration.cleanup.operation.count</code><br></br>- <code>hazelcast.internal.map.expiration.cleanup.percentage</code><br></br>- <code>hazelcast.internal.map.expiration.task.period.seconds</code></td>
</tr>
<tr>
<td style="text-align:left"><a href="#frequently-asked-questions">Chapter 29 - FAQ</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Added new questions/answers.</td>
</tr>
<tr>
<td style="text-align:left"><a href="#glossary">Chapter 30 - Glossary</a></td>
<td style="text-align:left"></td>
<td style="text-align:left">Added new glossary items.</td>
</tr>
</tbody>
</table>
<p><br> </br></p>

<a name="getting-started"></a><h1 id="getting-started">Getting Started</h1>
<p>This chapter explains how to install Hazelcast and start a Hazelcast member and client. It describes the executable files in the download package and also provides the fundamentals for configuring Hazelcast and its deployment options.</p>
<a name="installation"></a><h2 id="installation">Installation</h2>
<p>The following sections explain the installation of Hazelcast and Hazelcast Enterprise. It also includes notes and changes to consider when upgrading Hazelcast.</p>
<a name="hazelcast"></a><h3 id="hazelcast">Hazelcast</h3>
<p>You can find Hazelcast in standard Maven repositories. If your project uses Maven, you do not need to add 
additional repositories to your <code>pom.xml</code> or add <code>hazelcast-&lt;version&gt;.jar</code> file into your 
classpath (Maven does that for you). Just add the following lines to your <code>pom.xml</code>:</p>
<pre><code class="lang-xml">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
        &lt;artifactId&gt;hazelcast&lt;/artifactId&gt;
        &lt;version&gt;3.7&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
<p>As an alternative, you can download and install Hazelcast yourself. You only need to:</p>
<ul>
<li><p>Download the package <code>hazelcast-&lt;version&gt;.zip</code> or <code>hazelcast-&lt;version&gt;.tar.gz</code> from 
<a 
href="http://hazelcast.org/download/" target="_blank">hazelcast.org</a>.</p>
</li>
<li><p>Extract the downloaded <code>hazelcast-&lt;version&gt;.zip</code> or <code>hazelcast-&lt;version&gt;.tar.gz</code>.</p>
</li>
<li><p>Add the file <code>hazelcast-&lt;version&gt;.jar</code> to your classpath.</p>
</li>
</ul>

<a name="hazelcast-enterprise"></a><h3 id="hazelcast-enterprise">Hazelcast Enterprise</h3>
<p>There are two Maven repositories defined for Hazelcast Enterprise:</p>
<pre><code>&lt;repository&gt;
       &lt;id&gt;Hazelcast Private Snapshot Repository&lt;/id&gt;
       &lt;url&gt;https://repository-hazelcast-l337.forge.cloudbees.com/snapshot/&lt;/url&gt;
&lt;/repository&gt;
&lt;repository&gt;
        &lt;id&gt;Hazelcast Private Release Repository&lt;/id&gt;
        &lt;url&gt;https://repository-hazelcast-l337.forge.cloudbees.com/release/&lt;/url&gt;
&lt;/repository&gt;
</code></pre><p>Hazelcast Enterprise customers may also define dependencies, a sample of which is shown below.</p>
<pre><code>&lt;dependency&gt;
     &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
     &lt;artifactId&gt;hazelcast-enterprise-tomcat6&lt;/artifactId&gt;
     &lt;version&gt;${project.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
     &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
     &lt;artifactId&gt;hazelcast-enterprise-tomcat7&lt;/artifactId&gt;
     &lt;version&gt;${project.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
      &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
      &lt;artifactId&gt;hazelcast-enterprise&lt;/artifactId&gt;
      &lt;version&gt;${project.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
      &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
      &lt;artifactId&gt;hazelcast-enterprise-all&lt;/artifactId&gt;
      &lt;version&gt;${project.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<a name="setting-the-license-key"></a><h3 id="setting-the-license-key">Setting the License Key</h3>
<p>Hazelcast Enterprise offers you two types of licenses: <strong>Enterprise</strong> and <strong>Enterprise HD</strong>. The supported features differ in your Hazelcast setup according to the license type you own.</p>
<ul>
<li><strong>Enterprise license</strong>: In addition to the open source edition of Hazelcast, Enterprise features are the following:<ul>
<li>Security</li>
<li>WAN Replication</li>
<li>Continuous Query Cache</li>
<li>Clustered REST</li>
<li>Clustered JMX </li>
<li>Web Sessions
<br></br></li>
</ul>
</li>
<li><strong>Enterprise HD license</strong>: In addition to the Enterprise features, Enterprise HD features are the following:<ul>
<li>High-Density Memory Store</li>
<li>Hot Restart Persistence </li>
</ul>
</li>
</ul>
<p>To use Hazelcast Enterprise, you need to set the provided license key using one of the configuration methods shown below. </p>
<p><br></br>
<strong>Declarative Configuration:</strong></p>
<p>Add the below line to any place you like in the file <code>hazelcast.xml</code>. This XML file offers you a declarative way to configure your Hazelcast. It is included in the Hazelcast download package. When you extract the downloaded package, you will see the file <code>hazelcast.xml</code> under the <code>/bin</code> directory.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;license-key&gt;Your Enterprise License Key&lt;/license-key&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p><br></br>
<strong>Client Declarative Configuration:</strong></p>
<p>Native client distributions (Java, C++, .NET) of Hazelcast are open source. However, there are some Hazelcast Enterprise features which can be used with the Java Client such as SSL, Socket Interceptors, High-Density backed Near Cache, etc. In that case, you also need to have a Hazelcast Enterprise license and you should include this license in the file <code>hazelcast-client-full.xml</code> which is located under the directory <code>src/main/resources</code> of your <code>hazelcast-client</code> package. Set the license key in the <code>hazelcast-client-full.xml</code> as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  ...
  &lt;license-key&gt;Your Enterprise License Key&lt;/license-key&gt;
  ...
&lt;/hazelcast-client&gt;
</code></pre>
<p><br></br>
<strong>Programmatic Configuration:</strong></p>
<p>Alternatively, you can set your license key programmatically as shown below.</p>
<pre><code class="lang-java">Config config = new Config();
config.setLicenseKey( &quot;Your Enterprise License Key&quot; );
</code></pre>
<p><br></br>
<strong>Spring XML Configuration:</strong></p>
<p>If you are using Spring with Hazelcast, then you can set the license key using the Spring XML schema, as shown below.</p>
<pre><code class="lang-xml">&lt;hz:config&gt;
  ...
  &lt;hz:license-key&gt;Your Enterprise License Key&lt;/hz:license-key&gt;
  ...
&lt;/hz:config&gt;
</code></pre>
<p><br></br>
<strong>JVM System Property:</strong></p>
<p>As another option, you can set your license key using the below command (the &quot;-D&quot; command line option).</p>
<pre><code class="lang-plain">-Dhazelcast.enterprise.license.key=Your Enterprise License Key
</code></pre>
<p><br> </br></p>

<a name="upgrading-from-3x"></a><h3 id="upgrading-from-3-x">Upgrading from 3.x</h3>
<ul>
<li><p><strong>Upgrading from 3.6.x to 3.7.x when using <code>JCache</code>:</strong>
Hazelcast 3.7 introduced changes in <code>JCache</code> implementation which broke compatibility of 3.6.x clients to 3.7-3.7.2 cluster members and vice versa,
so 3.7-3.7.2 clients are also incompatible with 3.6.x cluster members. This issue only affects Java clients which use <code>JCache</code> functionality.</p>
<p>  Starting with Hazelcast version 3.7.3, a compatibility option is provided which can be used to ensure backwards compatibility with 3.6.x clients.
In order to upgrade a 3.6.x cluster and clients to 3.7.3 (or later), you will need to use this compatibility option on either the member or the client
side, depending on which one is upgraded first:</p>
<ul>
<li>first upgrade your cluster members to 3.7.3, adding property <code>hazelcast.compatibility.3.6.client=true</code> to your configuration; when started with this
property, cluster members are compatible with 3.6.x and 3.7.3+ clients but not with 3.7-3.7.2 clients. Once your cluster is upgraded, you may
upgrade your applications to use client version 3.7.3+.</li>
<li><p>upgrade your clients from 3.6.x to 3.7.3, adding property <code>hazelcast.compatibility.3.6.server=true</code> to your Hazelcast client configuration. A
3.7.3 client started with this compatibility option is compatible with 3.6.x and 3.7.3+ cluster members but incompatible with 3.7-3.7.2 cluster
members. Once your clients are upgraded, you may then proceed to upgrade your cluster members to version 3.7.3 or later.</p>
<p>You may use any of the supported ways <a href="#system-properties">as described in System Properties section</a> to configure the compatibility option. When done
upgrading your cluster and clients, you may remove the compatibility property from your Hazelcast member configuration. </p>
</li>
</ul>
</li>
<li><p><strong>Introducing the <code>spring-aware</code> element:</strong>
Before the release 3.5, Hazelcast uses <code>SpringManagedContext</code> to scan <code>SpringAware</code> annotations by default. This may cause some performance overhead for the users who do not use <code>SpringAware</code>.
This behavior has been changed with the release of Hazelcast 3.5. <code>SpringAware</code> annotations are disabled by default. By introducing the <code>spring-aware</code> element, now it is possible to enable it by adding the <code>&lt;hz:spring-aware /&gt;</code> tag to the configuration. Please see the <a href="#spring-integration">Spring Integration section</a>.</p>
</li>
<li><p><strong>Introducing new configuration options for WAN replication:</strong>
Starting with the release 3.6, WAN replication related system properties, which are configured on a per member basis, can now be configured per target cluster.
The 4 system properties below are no longer valid.</p>
<ul>
<li><p><code>hazelcast.enterprise.wanrep.batch.size</code>, please see the <a href="http://docs.hazelcast.org/docs/latest-dev/manual/html-single/index.html#batch-size">WAN Replication Batch Size</a>. </p>
</li>
<li><p><code>hazelcast.enterprise.wanrep.batchfrequency.seconds</code>, please see the <a href="http://docs.hazelcast.org/docs/latest-dev/manual/html-single/index.html#batch-maximum-delay">WAN Replication Batch Maximum Delay</a>.</p>
</li>
<li><p><code>hazelcast.enterprise.wanrep.optimeout.millis</code>, please see the <a href="http://docs.hazelcast.org/docs/latest-dev/manual/html-single/index.html#response-timeout">WAN Replication Response Timeout</a>.</p>
</li>
<li><p><code>hazelcast.enterprise.wanrep.queue.capacity</code>, please see the <a href="http://docs.hazelcast.org/docs/latest-dev/manual/html-single/index.html#queue-capacity">WAN Replication Queue Capacity</a>.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Removal of deprecated getId() method</strong>: 
The method <code>getId()</code> in the interface <code>DistributedObject</code> has been removed. Please use the method <code>getName()</code> instead.</p>
</li>
<li><p><strong>Change in the Custom Serialization in the C++ Client Distribution</strong>:</p>
</li>
</ul>
<p>Before, the method <code>getTypeId()</code> was used to retrieve the ID of the object to be serialized. Now, the method <code>getHazelcastTypeId()</code> is used and you give your object as a parameter to this new method. Also, <code>getTypeId()</code> was used in your custom serializer class, now it has been renamed to <code>getHazelcastTypeId()</code> too. Note that, these changes also apply when you want to switch from Hazelcast 3.6.1 to 3.6.2 too.</p>

<a name="upgrading-from-2x"></a><h3 id="upgrading-from-2-x">Upgrading from 2.x</h3>
<ul>
<li><strong>Removal of deprecated static methods:</strong>
The static methods of Hazelcast class reaching Hazelcast data components have been removed. The functionality of these methods can be reached from the HazelcastInstance interface. You should replace the following:</li>
</ul>
<pre><code class="lang-java">Map&lt;Integer, String&gt; customers = Hazelcast.getMap( &quot;customers&quot; );
</code></pre>
<p>with</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
// or if you already started an instance named &quot;instance1&quot;
// HazelcastInstance hazelcastInstance = Hazelcast.getHazelcastInstanceByName( &quot;instance1&quot; );
Map&lt;Integer, String&gt; customers = hazelcastInstance.getMap( &quot;customers&quot; );
</code></pre>
<ul>
<li><strong>Renaming &quot;instance&quot; to &quot;distributed object&quot;:</strong>
Before 3.0 there was confusion about the term &quot;instance&quot;: it was used for both the cluster members and the distributed objects (map, queue, topic, etc. instances). Starting with 3.0, the term instance will be only used for Hazelcast instances, namely cluster members. We will use the term &quot;distributed object&quot; for map, queue, etc. instances. You should replace the related methods with the new renamed ones. 3.0 clients are smart clients in that they know in which cluster member the data is located, so you can replace your lite members with native clients.</li>
</ul>
<pre><code class="lang-java">public static void main( String[] args ) throws InterruptedException {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IMap map = hazelcastInstance.getMap( &quot;test&quot; );
  Collection&lt;Instance&gt; instances = hazelcastInstance.getInstances();
  for ( Instance instance : instances ) {
    if ( instance.getInstanceType() == Instance.InstanceType.MAP ) {
      System.out.println( &quot;There is a map with name: &quot; + instance.getId() );
    }
  }
}
</code></pre>
<p>with</p>
<pre><code class="lang-java">public static void main( String[] args ) throws InterruptedException {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IMap map = hz.getMap( &quot;test&quot; );
  Collection&lt;DistributedObject&gt; objects = hazelcastInstance.getDistributedObjects();
  for ( DistributedObject distributedObject : objects ) {
    if ( distributedObject instanceof IMap ) {
      System.out.println( &quot;There is a map with name: &quot; + distributedObject.getName() );
    }
  }
}
</code></pre>
<ul>
<li><strong>Package structure change:</strong>
PartitionService has been moved to package <code>com.hazelcast.core</code> from <code>com.hazelcast.partition</code>.</li>
</ul>
<ul>
<li><strong>Listener API change:</strong>
Before 3.0, <code>removeListener</code> methods were taking the Listener object as a parameter. But this caused confusion because same listener object may be used as a parameter for different listener registrations. So we have changed the listener API. <code>addListener</code> methods returns a unique ID and you can remove a listener by using this ID. So you should do the following replacement if needed:</li>
</ul>
<pre><code class="lang-java">IMap map = hazelcastInstance.getMap( &quot;map&quot; );
map.addEntryListener( listener, true );
map.removeEntryListener( listener );
</code></pre>
<p>with</p>
<pre><code class="lang-java">IMap map = hazelcastInstance.getMap( &quot;map&quot; );
String listenerId = map.addEntryListener( listener, true );
map.removeEntryListener( listenerId );
</code></pre>
<ul>
<li><strong>IMap changes:</strong></li>
<li><code>tryRemove(K key, long timeout, TimeUnit timeunit)</code> returns boolean indicating whether operation is successful.</li>
<li><code>tryLockAndGet(K key, long time, TimeUnit timeunit)</code> is removed.</li>
<li><code>putAndUnlock(K key, V value)</code> is removed.</li>
<li><code>lockMap(long time, TimeUnit timeunit)</code> and <code>unlockMap()</code> are removed.</li>
<li><code>getMapEntry(K key)</code> is renamed as <code>getEntryView(K key)</code>. The returned object&#39;s type, MapEntry class is renamed as EntryView.</li>
<li>There is no predefined names for merge policies. You just give the full class name of the merge policy implementation.</li>
</ul>
<pre><code class="lang-xml">&lt;merge-policy&gt;com.hazelcast.map.merge.PassThroughMergePolicy&lt;/merge-policy&gt;
</code></pre>
<p>Also MergePolicy interface has been renamed to MapMergePolicy and also returning null from the implemented <code>merge()</code> method causes the existing entry to be removed.</p>
<ul>
<li><strong>IQueue changes:</strong>
There is no change on IQueue API but there are changes on how <code>IQueue</code> is configured. With Hazelcast 3.0 there will be no backing map configuration for queue. Settings like backup count will be directly configured on queue config. For queue configuration details, please see the <a href="#queue">Queue section</a>.</li>
<li><strong>Transaction API change:</strong>
In Hazelcast 3.0, transaction API is completely different. Please see the <a href="#transactions">Transactions chapter</a>.</li>
<li><strong>ExecutorService API change:</strong>
Classes MultiTask and DistributedTask have been removed. All the functionality is supported by the newly presented interface IExecutorService. Please see the <a href="#executor-service">Executor Service section</a>.</li>
<li><strong>LifeCycleService API:</strong>
The lifecycle has been simplified. <code>pause()</code>, <code>resume()</code>, <code>restart()</code> methods have been removed.</li>
<li><strong>AtomicNumber:</strong>
<code>AtomicNumber</code> class has been renamed to <code>IAtomicLong</code>.</li>
<li><strong>ICountDownLatch:</strong>
<code>await()</code> operation has been removed. We expect users to use <code>await()</code> method with timeout parameters.</li>
<li><strong>ISemaphore API:</strong>
The <code>ISemaphore</code> has been substantially changed. <code>attach()</code>, <code>detach()</code> methods have been removed.</li>
<li>In 2.x releases, the default value for <code>max-size</code> eviction policy was <strong>cluster_wide_map_size</strong>. In 3.x releases, default is <strong>PER_NODE</strong>. After upgrading, the <code>max-size</code> should be set according to this new default, if it is not changed. Otherwise, it is likely that OutOfMemory exception may be thrown.</li>
</ul>

<a name="starting-the-member-and-client"></a><h2 id="starting-the-member-and-client">Starting the Member and Client</h2>
<p>Having installed Hazelcast, you can get started. </p>
<p>In this short tutorial, you perform the following activities.</p>
<ol>
<li>Create a simple Java application using the Hazelcast distributed map and queue. </li>
<li>Run our application twice to have a cluster with two members (JVMs). </li>
<li>Connect to our cluster from another Java application by using the Hazelcast Native Java Client API.</li>
</ol>
<p>Let&#39;s begin.</p>
<ul>
<li>The following code starts the first Hazelcast member and creates and uses the <code>customers</code> map and queue.</li>
</ul>
<pre><code class="lang-java">import com.hazelcast.core.*;
import com.hazelcast.config.*;

import java.util.Map;
import java.util.Queue;

public class GettingStarted {
    public static void main(String[] args) {
        Config cfg = new Config();
        HazelcastInstance instance = Hazelcast.newHazelcastInstance(cfg);
        Map&lt;Integer, String&gt; mapCustomers = instance.getMap(&quot;customers&quot;);
        mapCustomers.put(1, &quot;Joe&quot;);
        mapCustomers.put(2, &quot;Ali&quot;);
        mapCustomers.put(3, &quot;Avi&quot;);

        System.out.println(&quot;Customer with key 1: &quot;+ mapCustomers.get(1));
        System.out.println(&quot;Map Size:&quot; + mapCustomers.size());

        Queue&lt;String&gt; queueCustomers = instance.getQueue(&quot;customers&quot;);
        queueCustomers.offer(&quot;Tom&quot;);
        queueCustomers.offer(&quot;Mary&quot;);
        queueCustomers.offer(&quot;Jane&quot;);
        System.out.println(&quot;First customer: &quot; + queueCustomers.poll());
        System.out.println(&quot;Second customer: &quot;+ queueCustomers.peek());
        System.out.println(&quot;Queue size: &quot; + queueCustomers.size());
    }
}
</code></pre>
<ul>
<li><p>Now, add the <code>hazelcast-client-</code><em><code>&lt;version&gt;</code></em><code>.jar</code> library to your classpath. 
This is required to use a Hazelcast client.</p>
</li>
<li><p>The following code starts a Hazelcast Client, connects to our cluster, 
and prints the size of the <code>customers</code> map.</p>
</li>
</ul>
<pre><code class="lang-java">package com.hazelcast.test;

import com.hazelcast.client.config.ClientConfig;
import com.hazelcast.client.HazelcastClient;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IMap;

public class GettingStartedClient {
    public static void main( String[] args ) {
        ClientConfig clientConfig = new ClientConfig();
        HazelcastInstance client = HazelcastClient.newHazelcastClient( clientConfig );
        IMap map = client.getMap( &quot;customers&quot; );
        System.out.println( &quot;Map Size:&quot; + map.size() );
    }
}
</code></pre>
<ul>
<li>When you run it, you see the client properly connecting to the cluster 
and printing the map size as <strong>3</strong>.</li>
</ul>
<p>Hazelcast also offers a tool, <strong>Management Center</strong>, that enables you to monitor your cluster. 
To use it, deploy the <code>mancenter-</code><em><code>&lt;version&gt;</code></em><code>.war</code> included in the ZIP file to your web server. 
You can use it to monitor your maps, queues, and other distributed data structures and members. Please 
see the <a href="#management-center">Management Center section</a> for usage explanations.</p>
<p>By default, Hazelcast uses Multicast to discover other members that can form a cluster.  If you are 
working with other Hazelcast developers on the same network, you may find yourself joining their 
clusters under the default settings.  Hazelcast provides a way to segregate clusters within the same 
network when using Multicast. Please see the <a href="#creating-cluster-groups">Creating Cluster Groups</a> 
for more information.  Alternatively, if you do not wish to use the default Multicast mechanism, 
you can provide a fixed list of IP addresses that are allowed to join. Please see 
the <a href="#join">Join Configuration section</a> for more information.
<br> </br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>You can also check the video tutorials <a href="http://hazelcast.org/getting-started/" target="_blank">here</a>.</em>
<br> </br></p>

<a name="using-the-scripts-in-the-package"></a><h2 id="using-the-scripts-in-the-package">Using the Scripts In The Package</h2>
<p>When you download and extract the Hazelcast ZIP or TAR.GZ package, you will see three scripts under the <code>/bin</code> folder that provide basic functionalities for member and cluster management.</p>
<p>The following are the names and descriptions of each script:</p>
<ul>
<li><code>start.sh</code> / <code>start.bat</code>: Starts a Hazelcast member with default configuration in the working directory*.</li>
<li><code>stop.sh</code> / <code>stop.bat</code>: Stops the Hazelcast member that was started in the current working directory.</li>
<li><code>cluster.sh</code>: Provides basic functionalities for cluster management, such as getting and changing the cluster state, shutting down the cluster or forcing the cluster to clean its persisted data and make a fresh start.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>start.sh</code> / <code>start.bat</code> scripts lets you start one Hazelcast instance per folder. To start a new instance, please unzip Hazelcast ZIP or TAR.GZ package in a new folder. </em></p>
<p>Please refer to the <a href="#using-the-script-cluster-sh">Using the Script cluster.sh section</a> to learn the usage of this script.</p>

<a name="deploying-on-amazon-ec2"></a><h2 id="deploying-on-amazon-ec2">Deploying On Amazon EC2</h2>
<p>You can deploy your Hazelcast project onto an Amazon EC2 environment using Third Party tools such as <a href="https://www.vagrantup.com" target="_blank">Vagrant</a> and <a href="https://www.chef.io/chef/" target="_blank">Chef</a>.</p>
<p>You can find a sample deployment project (<code>amazon-ec2-vagrant-chef</code>) with step-by-step instructions in the <code>hazelcast-integration</code> folder of the <strong>hazelcast-code-samples</strong> package, which you can download at <a href="http://hazelcast.org/download/" target="_blank">hazelcast.org</a>. Please refer to this sample project for more information.</p>

<a name="deploying-on-microsoft-azure"></a><h2 id="deploying-on-microsoft-azure">Deploying On Microsoft Azure</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>You can deploy your Hazelcast cluster onto a Microsoft Azure environment. For this, your cluster should make use of Hazelcast Discovery Plugin for Microsoft Azure. You can find information about this plugin on its GitHub repository at <a href="https://github.com/hazelcast/hazelcast-azure">Hazelcast Azure</a>.</p>
<p>For information on how to automatically deploy your cluster onto Azure, please see the <a href="https://github.com/hazelcast/hazelcast-azure/blob/master/README.md#automated-deployment">Deployment</a> section of <a href="https://github.com/hazelcast/hazelcast-azure">Hazelcast Azure</a> plugin repository.</p>

<a name="deploying-on-pivotal-cloud-foundry"></a><h2 id="deploying-on-pivotal-cloud-foundry">Deploying On Pivotal Cloud Foundry</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Starting with Hazelcast 3.7, you can deploy your Hazelcast cluster onto Cloud Foundry. This deployment enables Hazelcast to be used in multiple ways on Cloud Foundry platform. You can deploy Hazelcast in the following ways:</p>
<ul>
<li><strong>Unmanaged/User Managed Service</strong>: In this way of deployment, Hazelcast members run as a service. This service is hosted in an environment outside of Cloud Foundry platform and bound to the applications pushed to Cloud Foundry by the User Provided Services framework. </li>
<li><strong>Embedded</strong>: Applications with Hazelcast embedded topology are pushed onto Cloud Foundry as individual instances. These instances discover each other using the Discovery SPI implementation. This implementation is integrated with a registration service such as Consul and Eureka.</li>
<li><strong>Hazelcast Service Broker</strong>: In this way of deployment, Hazelcast runs as a native Cloud Foundry service. Using the Hazelcast Service Broker implementation, this service is provisioned and maintained by the Cloud Foundry platform itself. </li>
</ul>
<p>Integration between Hazelcast and Pivotal Cloud Foundry is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-cloudfoundry" target="_blank">Hazelcast Cloud Foundry</a> for details on configurations and usages.</p>

<a name="deploying-using-docker"></a><h2 id="deploying-using-docker">Deploying using Docker</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>You can deploy your Hazelcast projects using the Docker containers. Hazelcast has the following images on Docker:</p>
<ul>
<li>Hazelcast</li>
<li>Hazelcast Enterprise</li>
<li>Hazelcast Management Center</li>
<li>Hazelcast OpenShift</li>
</ul>
<p>After you pull an image from the Docker registry, you can run your image to start the management center or a Hazelcast instance with Hazelcast&#39;s default configuration. All repositories provide the latest stable releases but you can pull a specific release too. You can also specify environment variables when running the image.</p>
<p>If you want to start a customized Hazelcast instance, you can extend the Hazelcast image by providing your own configuration file.</p>
<p>This feature is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-docker" target="_blank">Hazelcast Docker</a> for details on configurations and usages.</p>

<a name="hazelcast-overview"></a><h1 id="hazelcast-overview">Hazelcast Overview</h1>
<p>Hazelcast is an open source In-Memory Data Grid (IMDG). 
It provides elastically scalable distributed In-Memory computing, widely recognized as the fastest and most scalable
approach to application performance. Hazelcast does this in open source.
More importantly, Hazelcast makes distributed computing simple by offering distributed implementations of many
developer-friendly interfaces from Java such as Map, Queue, ExecutorService, Lock, and JCache. For example, the Map
interface provides an In-Memory Key Value store which confers many of the advantages of NoSQL in terms of developer
friendliness and developer productivity.</p>
<p>In addition to distributing data In-Memory, Hazelcast provides a convenient set of APIs to access the CPUs in your
cluster for maximum processing speed.
Hazelcast is designed to be lightweight and easy to use. Since Hazelcast is delivered as a compact library (JAR) and
since it has no external dependencies other than Java, it easily plugs into your software solution and provides
distributed data structures and distributed computing utilities.</p>
<p>Hazelcast is highly scalable and available (100% operational, never failing). Distributed applications can use
Hazelcast for distributed caching, synchronization, clustering, processing, pub/sub messaging, etc. Hazelcast is
implemented in Java and has clients for Java, C/C++, .NET and REST. Hazelcast also speaks memcache protocol. It plugs into Hibernate and can easily be used with any existing database system.</p>
<p>If you are looking for In-Memory speed, elastic scalability, and the developer friendliness of NoSQL, Hazelcast is a
great choice.</p>
<p><strong>Hazelcast is Simple</strong></p>
<p>Hazelcast is written in Java with no other dependencies. It exposes the same API from the familiar Java util package,
exposing the same interfaces. Just add <code>hazelcast.jar</code> to your classpath and you can quickly enjoy JVMs clustering
and start building scalable applications.</p>
<p><strong>Hazelcast is Peer-to-Peer</strong></p>
<p>Unlike many NoSQL solutions, Hazelcast is peer-to-peer. There is no master and slave; there is no single point of
failure. All members store equal amounts of data and do equal amounts of processing. You can embed Hazelcast in your
existing application or use it in client and server mode where your application is a client to Hazelcast members.</p>
<p><strong>Hazelcast is Scalable</strong></p>
<p>Hazelcast is designed to scale up to hundreds and thousands of members. Simply add new members and they will
automatically discover the cluster and will linearly increase both memory and processing capacity. The members maintain
a TCP connection between each other and all communication is performed through this layer.</p>
<p><strong>Hazelcast is Fast</strong></p>
<p>Hazelcast stores everything in-memory. It is designed to perform very fast reads and updates.</p>
<p><strong>Hazelcast is Redundant</strong></p>
<p>Hazelcast keeps the backup of each data entry on multiple members. On a member failure, the data is restored from the
backup and the cluster will continue to operate without downtime.</p>

<a name="sharding-in-hazelcast"></a><h2 id="sharding-in-hazelcast">Sharding in Hazelcast</h2>
<p>Hazelcast shards are called Partitions. By default, Hazelcast has 271 partitions. Given a key, we serialize, hash
and mode it with the number of partitions to find the partition which the key belongs to. The partitions themselves are
distributed equally among the members of the cluster. Hazelcast also creates the backups of partitions and distributes
them among members for redundancy.</p>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#data-partitioning">Data Partitioning section</a> for more information on how Hazelcast partitions
your data.</em></p>

<a name="hazelcast-topology"></a><h2 id="hazelcast-topology">Hazelcast Topology</h2>
<p>You can deploy a Hazelcast cluster in two ways: Embedded or Client/Server.</p>
<p>If you have an application whose main focal point is asynchronous or high performance computing and lots of task
executions, then Embedded deployment is useful. In Embedded deployment, members include both the application and Hazelcast data and services. The advantage of the Embedded deployment is having a low-latency data access.</p>
<p>See the below illustration.</p>
<p><img src="images/P2Pcluster.jpg" alt="Embedded Deployment"></p>
<p>In the Client/Server deployment, Hazelcast data and services are centralized in one or more server members and they are accessed by the application through clients. 
You can have a cluster of server members that can be independently created and scaled. Your clients communicate with
these members to reach to Hazelcast data and services on them. Hazelcast provides native clients (Java, .NET and C++), Memcache
clients and REST clients. 
See the illustration at the end of this section.</p>
<p><img src="images/CSCluster.jpg" alt="Client/Server Deployment"></p>
<p>Client/Server deployment has advantages including more predictable and reliable Hazelcast performance, easier identification of problem causes, and most importantly, better scalability. 
When you need to scale in this deployment type, just add more Hazelcast server members. You can address client and server scalability concerns separately.</p>
<p>If you want low-latency data access, as in the Embedded deployment, and you also want the scalability advantages of the Client/Server deployment, you can consider defining near caches for your clients. This enables the frequently used data to be kept in the client&#39;s local memory. Please refer to <a href="#configuring-client-near-cache">Configuring Client Near Cache</a>.</p>

<a name="why-hazelcast"></a><h2 id="why-hazelcast-">Why Hazelcast?</h2>
<p><strong>A Glance at Traditional Data Persistence</strong></p>
<p>Data is at the core of software systems. In conventional architectures, a relational database persists and provides access to data. Applications are talking directly with a database which has its backup as another machine. To increase performance, tuning or a faster machine is required. This can cost a large amount of money or effort.</p>
<p>There is also the idea of keeping copies of data next to the database, which is performed using technologies like external key-value stores or second level caching that help offload the database. However, when the database is saturated or the applications perform mostly &quot;put&quot; operations (writes), this approach is of no use because it insulates the database only from the &quot;get&quot; loads (reads). Even if the applications are read-intensive there can be consistency problems--when data changes, what happens to the cache, and how are the changes handled? This is when concepts like time-to-live (TTL) or write-through come in.</p>
<p>In the case of TTL, if the access is less frequent than the TTL, the result will always be a cache miss. On the other hand, in the case of write-through caches, if there are more than one of these caches in a cluster, we again will have consistency issues. This can be avoided by having the nodes communicate with each other so that entry invalidations can be propagated.</p>
<p>We can conclude that an ideal cache would combine TTL and write-through features. There are several cache servers and in-memory database solutions in this field. However, these are stand-alone single instances with a distribution mechanism that is provided by other technologies to an extent. So, we are back to square one; we experience saturation or capacity issues if the product is a single instance or if consistency is not provided by the distribution.</p>
<p><strong>And, there is Hazelcast</strong></p>
<p>Hazelcast, a brand new approach to data, is designed around the concept of distribution. Hazelcast shares data around the cluster for flexibility and performance. It is an in-memory data grid for clustering and highly scalable data distribution.</p>
<p>One of the main features of Hazelcast is that it does not have a master member. Each cluster member is configured to be the same in terms of functionality. The oldest member (the first member created in the cluster) automatically performs the data assignment to cluster members. If the oldest member dies, the second oldest member takes over.</p>
<p>Another main feature of Hazelcast is that the data is held entirely in-memory. This is fast. In the case of a failure, such as a member crash, no data will be lost since Hazelcast distributes copies of the data across all the cluster members.</p>
<p>As shown in the feature list in the <a href="#hazelcast-overview">Hazelcast Overview</a>, Hazelcast supports a number of distributed data structures and distributed computing utilities. These provide powerful ways of accessing distributed clustered memory and accessing CPUs for true distributed computing. </p>
<p><strong>Hazelcast&#39;s Distinctive Strengths</strong></p>
<ul>
<li>Hazelcast is open source.</li>
<li>Hazelcast is only a JAR file. You do not need to install software.</li>
<li>Hazelcast is a library, it does not impose an architecture on Hazelcast users.</li>
<li>Hazelcast provides out of the box distributed data structures, such as Map, Queue, MultiMap, Topic, Lock and Executor.</li>
<li>There is no &quot;master,&quot; meaning no single point of failure in a Hazelcast cluster; each member in the cluster is configured to be functionally the same.</li>
<li>When the size of your memory and compute requirements increase, new members can be dynamically joined to the Hazelcast cluster to scale elastically.</li>
<li>Data is resilient to member failure. Data backups are distributed across the cluster. This is a big benefit when a member in the cluster crashes as data will not be lost.</li>
<li>Members are always aware of each other unlike in traditional key-value caching solutions.</li>
<li>You can build your own custom-distributed data structures using the Service Programming Interface (SPI) if you are not happy with the data structures provided.</li>
</ul>
<p>Finally, Hazelcast has a vibrant open source community enabling it to be continuously developed.</p>
<p>Hazelcast is a fit when you need:</p>
<ul>
<li>analytic applications requiring big data processing by partitioning the data.</li>
<li>to retain frequently accessed data in the grid.</li>
<li>a cache, particularly an open source JCache provider with elastic distributed scalability.</li>
<li>a primary data store for applications with utmost performance, scalability and low-latency requirements.</li>
<li>an In-Memory NoSQL Key Value Store.</li>
<li>publish/subscribe communication at highest speed and scalability between applications.</li>
<li>applications that need to scale elastically in distributed and cloud environments.</li>
<li>a highly available distributed cache for applications.</li>
<li>an alternative to Coherence and Terracotta.</li>
</ul>

<a name="data-partitioning"></a><h2 id="data-partitioning">Data Partitioning</h2>
<p>As you read in the <a href="#sharding-in-hazelcast">Sharding in Hazelcast section</a>, Hazelcast shards are called Partitions. Partitions are memory segments that can contain hundreds or thousands of data entries each, depending on the memory capacity of your system. </p>
<p>By default, Hazelcast offers 271 partitions. When you start a cluster member, it starts with these 271 partitions. The following illustration shows the partitions in a Hazelcast cluster with single member.</p>
<p><img src="images/NodePartition.jpg" alt="Single Member with Partitions"></p>
<p>When you start a second member on that cluster (creating a Hazelcast cluster with two members), the partitions are distributed as shown in the illustration here.</p>
<p><img src="images/BackupPartitions.jpg" alt="Cluster with Two Members - Backups are Created"></p>
<p>In the illustration, the partitions with black text are primary partitions and the partitions with blue text are replica partitions (backups). The first member has 135 primary partitions (black), and each of these partitions are backed up in the second member (blue). At the same time, the first member also has the replica partitions of the second member&#39;s primary partitions.</p>
<p>As you add more members, Hazelcast moves some of the primary and replica partitions to the new members one by one, making all members equal and redundant. Only the minimum amount of partitions will be moved to scale out Hazelcast. The following is an illustration of the partition distributions in a Hazelcast cluster with four members.</p>
<p><img src="images/4NodeCluster.jpg" alt="Cluster with Four Members"></p>
<p>Hazelcast distributes the partitions equally among the members of the cluster. Hazelcast creates the backups of partitions and distributes them among the members for redundancy.</p>
<p>Partition distributions in the above illustrations are for your convenience and descriptive purposes. Normally, the partitions are not distributed in an order (as they are shown in these illustrations), but are distributed randomly. The important point here is that Hazelcast equally distributes the partitions and their backups among the members.</p>
<p>Starting with Hazelcast 3.6, lite members are introduced. Lite members are a new type of members that do not own any partition. Lite members are intended for use in computationally-heavy task executions and listener registrations. Although they do not own any partitions,
they can access partitions that are owned by other members in the cluster.</p>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#enabling-lite-members">Enabling Lite Members section</a>.</em>
<br></br> </p>
<a name="how-the-data-is-partitioned"></a><h3 id="how-the-data-is-partitioned">How the Data is Partitioned</h3>
<p>Hazelcast distributes data entries into the partitions using a hashing algorithm. Given an object key (for example, for a map) or an object name (for example, for a topic or list):</p>
<ul>
<li>the key or name is serialized (converted into a byte array),</li>
<li>this byte array is hashed, and</li>
<li>the result of the hash is mod by the number of partitions.</li>
</ul>
<p>The result of this modulo - <em>MOD(hash result, partition count)</em> -  is the partition in which the data will be stored, that is the <strong>partition ID</strong>. For ALL members you have in your cluster, the partition ID for a given key will always be the same.</p>
<a name="partition-table"></a><h3 id="partition-table">Partition Table</h3>
<p>When you start a member, a partition table is created within it. This table stores the partition IDs and the cluster members to which they belong. The purpose of this table is to make all members (including lite members) in the cluster aware of this information, making sure that each member knows where the data is.</p>
<p>The oldest member in the cluster (the one that started first) periodically sends the partition table to all members. In this way each member in the cluster is informed about any changes to partition ownership. The ownerships may be changed when, for example, a new member joins the cluster, or when a member leaves the cluster.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If the oldest member of the cluster goes down, the next oldest member sends the partition table information to the other ones.</em></p>
<p>You can configure the frequency (how often) that the member sends the partition table the information by using the <code>hazelcast.partition.table.send.interval</code> system property. The property is set to every 15 seconds by default. </p>
<a name="repartitioning"></a><h3 id="repartitioning">Repartitioning</h3>
<p>Repartitioning is the process of redistribution of partition ownerships. Hazelcast performs the repartitioning in the following cases:</p>
<ul>
<li>When a member joins to the cluster.</li>
<li>When a member leaves the cluster.</li>
</ul>
<p>In these cases, the partition table in the oldest member is updated with the new partition ownerships. </p>
<p>Note that if a lite member joins or leaves a cluster, repartitioning is not triggered since lite members do not own any partitions.</p>

<a name="use-cases"></a><h2 id="use-cases">Use Cases</h2>
<p>Hazelcast can be used:</p>
<ul>
<li>to share server configuration/information to see how a cluster performs.</li>
<li>to cluster highly changing data with event notifications (e.g., user based events), and to queue and distribute background tasks.</li>
<li>as a simple Memcache with near cache.</li>
<li>as a cloud-wide scheduler of certain processes that need to be performed on some members.</li>
<li>to share information (user information, queues, maps, etc.) on the fly with multiple members in different installations under OSGI environments.</li>
<li>to share thousands of keys in a cluster where there is a web service interface on an application server and some validation.</li>
<li>as a distributed topic (publish/subscribe server) to build scalable chat servers for smartphones.</li>
<li>as a front layer for a Cassandra back-end.</li>
<li>to distribute user object states across the cluster, to pass messages between objects, and to share system data structures (static initialization state, mirrored objects, object identity generators).</li>
<li>as a multi-tenancy cache where each tenant has its own map.</li>
<li>to share datasets (e.g., table-like data structure) to be used by applications.</li>
<li>to distribute the load and collect status from Amazon EC2 servers where the front-end is developed using, for example, Spring framework.</li>
<li>as a real-time streamer for performance detection.</li>
<li>as storage for session data in web applications (enables horizontal scalability of the web application).</li>
</ul>

<a name="resources"></a><h2 id="resources">Resources</h2>
<ul>
<li>Hazelcast source code can be found at <a href="https://github.com/hazelcast/hazelcast" target="_blank">Github/Hazelcast</a>.</li>
<li>Hazelcast API can be found at <a href="http://www.hazelcast.org/docs/latest-dev/javadoc/" target="_blank">Hazelcast.org/docs/Javadoc</a>.</li>
<li>Code samples can be downloaded from <a href="http://hazelcast.org/download/" target="_blank">Hazelcast.org/download</a>.</li>
<li>More use cases and resources can be found at <a href="http://www.hazelcast.com" target="_blank">Hazelcast.com</a>.</li>
<li>Questions and discussions can be posted at the <a href="https://groups.google.com/forum/#!forum/hazelcast" target="_blank">Hazelcast mail group</a>.</li>
</ul>
<p><br> </br></p>

<a name="understanding-configuration"></a><h1 id="understanding-configuration">Understanding Configuration</h1>
<p>This chapter describes the options to configure your Hazelcast applications and explains the utilities which you can make use of while configuring. You can configure Hazelcast using one or mix of the following options: </p>
<ul>
<li>Declarative way</li>
<li>Programmatic way</li>
<li>Using Hazelcast system properties</li>
<li>Within the Spring context</li>
</ul>
<a name="configuring-declaratively"></a><h2 id="configuring-declaratively">Configuring Declaratively</h2>
<p>This is the configuration option where you use an XML configuration file. When you download and unzip <code>hazelcast-&lt;version&gt;.zip</code>, you will see the following files present in <code>/bin</code> folder, which are standard XML-formatted configuration files:</p>
<ul>
<li><code>hazelcast.xml</code>: Default declarative configuration file for Hazelcast. The configuration in this XML file should be fine for most of the Hazelcast users. If not, you can tailor this XML file according to your needs by adding/removing/modifying properties.</li>
<li><code>hazelcast-full-example.xml</code>: Configuration file which includes all Hazelcast configuration elements and attributes with their descriptions. It is the &quot;superset&quot; of <code>hazelcast.xml</code>. You can use <code>hazelcast-full-example.xml</code> as a reference document to learn about any element or attribute, or you can change its name to <code>hazelcast.xml</code> and start to use it as your Hazelcast configuration file.</li>
</ul>
<p>A part of <code>hazelcast.xml</code> is shown as an example below.</p>
<pre><code class="lang-xml">&lt;group&gt;
    &lt;name&gt;dev&lt;/name&gt;
    &lt;password&gt;dev-pass&lt;/password&gt;
&lt;/group&gt;
&lt;management-center enabled=&quot;false&quot;&gt;http://localhost:8080/mancenter&lt;/management-center&gt;
&lt;network&gt;
    &lt;port auto-increment=&quot;true&quot; port-count=&quot;100&quot;&gt;5701&lt;/port&gt;
    &lt;outbound-ports&gt;
        &lt;!--
        Allowed port range when connecting to other members.
        0 or * means the port provided by the system.
        --&gt;
        &lt;ports&gt;0&lt;/ports&gt;
    &lt;/outbound-ports&gt;
    &lt;join&gt;
        &lt;multicast enabled=&quot;true&quot;&gt;
        &lt;multicast-group&gt;224.2.2.3&lt;/multicast-group&gt;
        &lt;multicast-port&gt;54327&lt;/multicast-port&gt;
        &lt;/multicast&gt;
        &lt;tcp-ip enabled=&quot;false&quot;&gt;
</code></pre>
<a name="composing-declarative-configuration"></a><h3 id="composing-declarative-configuration">Composing Declarative Configuration</h3>
<p>You can compose the declarative configuration of your Hazelcast member or Hazelcast client from multiple declarative configuration snippets. In order to compose a declarative configuration, you can use the <code>&lt;import/&gt;</code> element to load different declarative configuration files.</p>
<p>Let&#39;s say you want to compose the declarative configuration for Hazelcast out of two configurations: <code>development-group-config.xml</code> and <code>development-network-config.xml</code>. These two configurations are shown below.</p>
<p><code>development-group-config.xml</code>:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;group&gt;
      &lt;name&gt;dev&lt;/name&gt;
      &lt;password&gt;dev-pass&lt;/password&gt;
  &lt;/group&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><br></br></p>
<p><code>development-network-config.xml</code>:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;network&gt;
    &lt;port auto-increment=&quot;true&quot; port-count=&quot;100&quot;&gt;5701&lt;/port&gt;
    &lt;join&gt;
        &lt;multicast enabled=&quot;true&quot;&gt;
            &lt;multicast-group&gt;224.2.2.3&lt;/multicast-group&gt;
            &lt;multicast-port&gt;54327&lt;/multicast-port&gt;
        &lt;/multicast&gt;
    &lt;/join&gt;
  &lt;/network&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>To get your example Hazelcast declarative configuration out of the above two, use the <code>&lt;import/&gt;</code> element as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;import resource=&quot;development-group-config.xml&quot;/&gt;
  &lt;import resource=&quot;development-network-config.xml&quot;/&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>This feature also applies to the declarative configuration of Hazelcast client. Please see the following examples.</p>
<p><code>client-group-config.xml</code>:</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  &lt;group&gt;
      &lt;name&gt;dev&lt;/name&gt;
      &lt;password&gt;dev-pass&lt;/password&gt;
  &lt;/group&gt;
&lt;/hazelcast-client&gt;
</code></pre>
<p><br></br></p>
<p><code>client-network-config.xml</code>:</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
    &lt;network&gt;
        &lt;cluster-members&gt;
            &lt;address&gt;127.0.0.1:7000&lt;/address&gt;
        &lt;/cluster-members&gt;
    &lt;/network&gt;
&lt;/hazelcast-client&gt;
</code></pre>
<p>To get a Hazelcast client declarative configuration from the above two examples, use the <code>&lt;import/&gt;</code> element as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  &lt;import resource=&quot;client-group-config.xml&quot;/&gt;
  &lt;import resource=&quot;client-network-config.xml&quot;/&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Use <code>&lt;import/&gt;</code> element on top level of the XML hierarchy.</em>
<br></br></p>
<p>Using the element <code>&lt;import&gt;</code>, you can also load XML resources from classpath and file system:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;import resource=&quot;file:///etc/hazelcast/development-group-config.xml&quot;/&gt; &lt;!-- loaded from filesystem --&gt;
  &lt;import resource=&quot;classpath:development-network-config.xml&quot;/&gt;  &lt;!-- loaded from classpath --&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>The element <code>&lt;import&gt;</code> supports placeholders too. Please see the following example snippet:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;import resource=&quot;${environment}-group-config.xml&quot;/&gt;
  &lt;import resource=&quot;${environment}-network-config.xml&quot;/&gt;
&lt;/hazelcast&gt;
</code></pre>
<a name="configuring-programmatically"></a><h2 id="configuring-programmatically">Configuring Programmatically</h2>
<p>Besides declarative configuration, you can configure your cluster programmatically. For this you can create a <code>Config</code> object, set/change its properties and attributes, and use this <code>Config</code> object to create a new Hazelcast member. Following is an example code which configures some network and Hazelcast Map properties.</p>
<pre><code class="lang-java">Config config = new Config();
config.getNetworkConfig().setPort( 5900 )
                    .setPortAutoIncrement( false );

MapConfig mapConfig = new MapConfig();
mapConfig.setName( &quot;testMap&quot; )
                    .setBackupCount( 2 );
                    .setTimeToLiveSeconds( 300 );

config.addMapConfig( mapConfig );
</code></pre>
<p>To create a Hazelcast member with the above example configuration, pass the configuration object as shown below:</p>
<pre><code>HazelcastInstance hazelcast = Hazelcast.newHazelcastInstance( config );
</code></pre><p><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The <code>Config</code> must not be modified after the Hazelcast instance is started. In other words, all configuration must be completed before creating the <code>HazelcastInstance</code>.</em>
<br></p>
<p>You can also create a named Hazelcast member. In this case, you should set <code>instanceName</code> of <code>Config</code> object as shown below:</p>
<pre><code class="lang-java">Config config = new Config();
config.setInstanceName( &quot;my-instance&quot; );
Hazelcast.newHazelcastInstance( config );
</code></pre>
<p>To retrieve an existing Hazelcast member by its name, use the following:</p>
<pre><code>Hazelcast.getHazelcastInstanceByName( &quot;my-instance&quot; );
</code></pre><p>To retrieve all existing Hazelcast members, use the following:</p>
<pre><code>Hazelcast.getAllHazelcastInstances();
</code></pre><p><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Hazelcast performs schema validation through the file <code>hazelcast-config-&lt;version&gt;.xsd</code> which comes with your Hazelcast libraries. Hazelcast throws a meaningful exception if there is an error in the declarative or programmatic configuration.</em></p>
<p><br></p>
<p>If you want to specify your own configuration file to create <code>Config</code>, Hazelcast supports several ways including filesystem, classpath, InputStream, and URL:</p>
<ul>
<li><code>Config cfg = new XmlConfigBuilder(xmlFileName).build();</code></li>
<li><code>Config cfg = new XmlConfigBuilder(inputStream).build();</code></li>
<li><code>Config cfg = new ClasspathXmlConfig(xmlFileName);</code></li>
<li><code>Config cfg = new FileSystemXmlConfig(configFilename);</code></li>
<li><code>Config cfg = new UrlXmlConfig(url);</code></li>
<li><code>Config cfg = new InMemoryXmlConfig(xml);</code></li>
</ul>
<a name="configuring-with-system-properties"></a><h2 id="configuring-with-system-properties">Configuring with System Properties</h2>
<p>You can use system properties to configure some aspects of Hazelcast. You set these properties as name and value pairs through declarative configuration, programmatic configuration or JVM system property. Following are examples for each option.</p>
<p><strong>Declaratively:</strong></p>
<pre><code class="lang-xml">  ....
  &lt;properties&gt;
    &lt;property name=&quot;hazelcast.property.foo&quot;&gt;value&lt;/property&gt;
    ....
  &lt;/properties&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatically:</strong></p>
<pre><code class="lang-java">Config config = new Config() ;
config.setProperty( &quot;hazelcast.property.foo&quot;, &quot;value&quot; );
</code></pre>
<p><strong>Using JVM&#39;s <code>System</code> class or <code>-D</code> argument:</strong></p>
<p><code>System.setProperty( &quot;hazelcast.property.foo&quot;, &quot;value&quot; );</code></p>
<p>or</p>
<p><code>java -Dhazelcast.property.foo=value</code></p>
<p>You will see Hazelcast system properties mentioned throughout this Reference Manual as required in some of the chapters and sections. All Hazelcast system properties are listed in the <a href="#system-properties">System Properties appendix</a> with their descriptions, default values and property types as a reference for you.</p>
<a name="configuring-within-spring-context"></a><h2 id="configuring-within-spring-context">Configuring within Spring Context</h2>
<p>If you use Hazelcast with <a href="https://spring.io/">Spring</a> you can declare beans using the namespace <code>hazelcast</code>. When you add the namespace declaration to the element <code>beans</code> in the Spring context file, you can start to use the namespace shortcut <code>hz</code> to be used as a bean declaration. Following is an example Hazelcast configuration when integrated with Spring:</p>
<pre><code>&lt;hz:hazelcast id=&quot;instance&quot;&gt;
  &lt;hz:config&gt;
    &lt;hz:group name=&quot;dev&quot; password=&quot;password&quot;/&gt;
    &lt;hz:network port=&quot;5701&quot; port-auto-increment=&quot;false&quot;&gt;
      &lt;hz:join&gt;
        &lt;hz:multicast enabled=&quot;false&quot;/&gt;
        &lt;hz:tcp-ip enabled=&quot;true&quot;&gt;
          &lt;hz:members&gt;10.10.1.2, 10.10.1.3&lt;/hz:members&gt;
        &lt;/hz:tcp-ip&gt;
      &lt;/hz:join&gt;
    &lt;/hz:network&gt;
  &lt;/hz:config&gt;
&lt;/hz:hazelcast&gt;
</code></pre><p>Please see the <a href="#spring-integration">Spring Integration section</a> for more information on Hazelcast-Spring integration.</p>
<a name="checking-configuration"></a><h2 id="checking-configuration">Checking Configuration</h2>
<p>When you start a Hazelcast member without passing a <code>Config</code> object, as explained in the <a href="#configuring-programmatically">Configuring Programmatically section</a>, Hazelcast checks the member&#39;s configuration as follows:</p>
<ul>
<li><p>First, it looks for the <code>hazelcast.config</code> system property. If it is set, its value is used as the path. This is useful if you want to be able to change your Hazelcast configuration; you can do this because it is not embedded within the application. You can set the <code>config</code> option with the following command:</p>
<p><code>- Dhazelcast.config=</code><em><code>&lt;path to the hazelcast.xml&gt;</code></em>.</p>
<p>The path can be a regular one or a classpath reference with the prefix <code>classpath:</code>.</p>
</li>
<li>If the above system property is not set, Hazelcast then checks whether there is a <code>hazelcast.xml</code> file in the working directory.</li>
<li>If not, it then checks whether <code>hazelcast.xml</code> exists on the classpath.</li>
<li>If none of the above works, Hazelcast loads the default configuration (<code>hazelcast.xml</code>) that comes with your Hazelcast package.</li>
</ul>
<p>Before configuring Hazelcast, please try to work with the default configuration to see if it works for you. This default configuration should be fine for most of the users. If not, you can consider to modify the configuration to be more suitable for your environment.</p>
<a name="using-wildcards"></a><h2 id="using-wildcards">Using Wildcards</h2>
<p>Hazelcast supports wildcard configuration for all distributed data structures that can be configured using <code>Config</code>, that is, for all except <code>IAtomicLong</code>, <code>IAtomicReference</code>. Using an asterisk (*) character in the name, different instances of maps, queues, topics, semaphores, etc. can be configured by a single configuration.</p>
<p>A single asterisk (*) can be placed anywhere inside the configuration name.</p>
<p>For instance, a map named <code>com.hazelcast.test.mymap</code> can be configured using one of the following configurations.</p>
<pre><code class="lang-xml">&lt;map name=&quot;com.hazelcast.test.*&quot;&gt;
...
&lt;/map&gt;
</code></pre>
<pre><code class="lang-xml">&lt;map name=&quot;com.hazel*&quot;&gt;
...
&lt;/map&gt;
</code></pre>
<pre><code class="lang-xml">&lt;map name=&quot;*.test.mymap&quot;&gt;
...
&lt;/map&gt;
</code></pre>
<pre><code class="lang-xml">&lt;map name=&quot;com.*test.mymap&quot;&gt;
...
&lt;/map&gt;
</code></pre>
<p>Or a queue &#39;<code>com.hazelcast.test.myqueue</code>&#39;:</p>
<pre><code class="lang-xml">&lt;queue name=&quot;*hazelcast.test.myqueue&quot;&gt;
...
&lt;/queue&gt;
</code></pre>
<pre><code class="lang-xml">&lt;queue name=&quot;com.hazelcast.*.myqueue&quot;&gt;
...
&lt;/queue&gt;
</code></pre>
<a name="using-variables"></a><h2 id="using-variables">Using Variables</h2>
<p>In your Hazelcast and/or Hazelcast Client declarative configuration, you can use variables to set the values of the elements. This is valid when you set a system property programmatically or you use the command line interface. You can use a variable in the declarative configuration to access the values of the system properties you set.</p>
<p>For example, see the following command that sets two system properties.</p>
<pre><code>-Dgroup.name=dev -Dgroup.password=somepassword
</code></pre><p>Let&#39;s get the values of these system properties in the declarative configuration of Hazelcast, as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;group&gt;
    &lt;name&gt;${group.name}&lt;/name&gt;
    &lt;password&gt;${group.password}&lt;/password&gt;
  &lt;/group&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>This also applies to the declarative configuration of Hazelcast Client, as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  &lt;group&gt;
    &lt;name&gt;${group.name}&lt;/name&gt;
    &lt;password&gt;${group.password}&lt;/password&gt;
  &lt;/group&gt;
&lt;/hazelcast-client&gt;
</code></pre>
<p>If you do not want to rely on the system properties, you can use the <code>XmlConfigBuilder</code> and explicitly set a <code>Properties</code> instance, as shown below.</p>
<pre><code class="lang-java">Properties properties = new Properties();

// fill the properties, e.g. from database/LDAP, etc.

XmlConfigBuilder builder = new XmlConfigBuilder();
builder.setProperties(properties)
Config config = builder.build();
HazelcastInstance hz = Hazelcast.newHazelcastInstance(config);
</code></pre>

<a name="setting-up-clusters"></a><h1 id="setting-up-clusters">Setting Up Clusters</h1>
<p>This chapter describes Hazelcast clusters and the methods cluster members and native clients use to form a Hazelcast cluster. </p>
<a name="discovering-cluster-members"></a><h2 id="discovering-cluster-members">Discovering Cluster Members</h2>
<p>A Hazelcast cluster is a network of cluster members that run Hazelcast. Cluster members (also called nodes) automatically join together to form a cluster. This automatic joining takes place with various discovery mechanisms that the cluster members use to find each other. Hazelcast uses the following discovery mechanisms:</p>
<ul>
<li><a href="#discovering-members-by-multicast">Multicast</a></li>
<li><a href="#discovering-members-by-tcp">TCP</a></li>
<li><a href="#discovering-members-within-ec2-cloud">EC2 Cloud</a></li>
<li><a href="#discovering-members-with-jclouds">jclouds&reg;</a></li>
</ul>
<p>Each discovery mechanism is explained in the following sections.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>After a cluster is formed, communication between cluster members is always via TCP/IP, regardless of the discovery mechanism used.</em></p>

<a name="discovering-members-by-multicast"></a><h3 id="discovering-members-by-multicast">Discovering Members by Multicast</h3>
<p>With the multicast auto-discovery mechanism, Hazelcast allows cluster members to find each other using multicast communication. The cluster members do not need to know the concrete addresses of the other members, as they just multicast to all the other members for listening. Whether multicast is possible or allowed depends on your environment.</p>
<p>To set your Hazelcast to multicast auto-discovery, set the following configuration elements. Please refer to the <a href="#multicast-element">multicast element section</a> for the full description of the multicast discovery configuration elements.</p>
<ul>
<li>Set the <code>enabled</code> attribute of the <code>multicast</code> element to &quot;true&quot;.</li>
<li>Set <code>multicast-group</code>, <code>multicast-port</code>, <code>multicast-time-to-live</code>, etc. to your multicast values.</li>
<li>Set the <code>enabled</code> attribute of both <code>tcp-ip</code> and <code>aws</code> elements to &quot;false&quot;.</li>
</ul>
<p>The following is an example declarative configuration.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
  &lt;network&gt;
    ...
        &lt;join&gt;
            &lt;multicast enabled=&quot;true&quot;&gt;
                &lt;multicast-group&gt;224.2.2.3&lt;/multicast-group&gt;
                &lt;multicast-port&gt;54327&lt;/multicast-port&gt;
                &lt;multicast-time-to-live&gt;32&lt;/multicast-time-to-live&gt;
                &lt;multicast-timeout-seconds&gt;2&lt;/multicast-timeout-seconds&gt;
                &lt;trusted-interfaces&gt;
                   &lt;interface&gt;192.168.1.102&lt;/interface&gt;
                &lt;/trusted-interfaces&gt;   
            &lt;/multicast&gt;
            &lt;tcp-ip enabled=&quot;false&quot;&gt;
            &lt;/tcp-ip&gt;
            &lt;aws enabled=&quot;false&quot;&gt;
            &lt;/aws&gt;
        &lt;/join&gt;
  &lt;network&gt;
</code></pre>
<p>Pay attention to the <code>multicast-timeout-seconds</code> element. <code>multicast-timeout-seconds</code> specifies the time in seconds that a member should wait for a valid multicast response from another member running in the network before declaring itself the leader member (the first member joined to the cluster) and creating its own cluster. This only applies to the startup of members where no leader has been assigned yet. If you specify a high value to <code>multicast-timeout-seconds</code>, such as 60 seconds, it means that until a leader is selected, each member will wait 60 seconds before moving on. Be careful when providing a high value. Also, be careful not to set the value too low, or the members might give up too early and create their own cluster.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Multicast auto-discovery is not supported for Hazelcast native clients yet. However, we offer Multicast Discovery Plugin for this purpose. Please refer to the <a href="#discovering-native-clients">Discovering Native Clients</a> section.</em></p>

<a name="discovering-members-by-tcp"></a><h3 id="discovering-members-by-tcp">Discovering Members by TCP</h3>
<p>If multicast is not the preferred way of discovery for your environment, then you can configure Hazelcast to be a full TCP/IP cluster. When you configure Hazelcast to discover members by TCP/IP, you must list all or a subset of the members&#39; hostnames and/or IP addresses as cluster members. You do not have to list all of these cluster members, but at least one of the listed members has to be active in the cluster when a new member joins.</p>
<p>To set your Hazelcast to be a full TCP/IP cluster, set the following configuration elements. Please refer to the <a href="#tcp-ip-element">tcp-ip element section</a> for the full description of the TCP/IP discovery configuration elements.</p>
<ul>
<li>Set the <code>enabled</code> attribute of the <code>multicast</code> element to &quot;false&quot;.</li>
<li>Set the <code>enabled</code> attribute of the <code>aws</code> element to &quot;false&quot;.</li>
<li>Set the <code>enabled</code> attribute of the <code>tcp-ip</code> element to &quot;true&quot;.</li>
<li>Set your <code>member</code> elements within the <code>tcp-ip</code> element.</li>
</ul>
<p>The following is an example declarative configuration.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
  &lt;network&gt;
    ...
    &lt;join&gt;
      &lt;multicast enabled=&quot;false&quot;&gt;
      &lt;/multicast&gt;
      &lt;tcp-ip enabled=&quot;true&quot;&gt;
        &lt;member&gt;machine1&lt;/member&gt;
        &lt;member&gt;machine2&lt;/member&gt;
        &lt;member&gt;machine3:5799&lt;/member&gt;
        &lt;member&gt;192.168.1.0-7&lt;/member&gt;
        &lt;member&gt;192.168.1.21&lt;/member&gt;
      &lt;/tcp-ip&gt;
      ...
    &lt;/join&gt;
    ...
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>As shown above, you can provide IP addresses or hostnames for <code>member</code> elements. You can also give a range of IP addresses, such as <code>192.168.1.0-7</code>.</p>
<p>Instead of providing members line-by-line as shown above, you also have the option to use the <code>members</code> element and write comma-separated IP addresses, as shown below.</p>
<p><code>&lt;members&gt;192.168.1.0-7,192.168.1.21&lt;/members&gt;</code></p>
<p>If you do not provide ports for the members, Hazelcast automatically tries the ports 5701, 5702, and so on.</p>
<p>By default, Hazelcast binds to all local network interfaces to accept incoming traffic. You can change this behavior using the system property <code>hazelcast.socket.bind.any</code>. If you set this property to <code>false</code>, Hazelcast uses the interfaces specified in the <code>interfaces</code> element (please refer to the <a href="#interfaces">Interfaces Configuration section</a>). If no interfaces are provided, then it will try to resolve one interface to bind from the <code>member</code> elements.</p>

<a name="discovering-members-within-ec2-cloud"></a><h3 id="discovering-members-within-ec2-cloud">Discovering Members within EC2 Cloud</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Hazelcast supports EC2 auto-discovery. It is useful when you do not want to provide or you cannot provide the list of possible IP addresses. This discovery feature is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-aws" target="_blank">Hazelcast AWS</a> for information on configuring and using it.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>hazelcast-cloud module has been renamed as hazelcast-aws module (starting with Hazelcast 3.7.3). If you want to use AWS Discovery, you should add the library hazelcast-aws JAR to your environment. For more information please look see the README of <a href="https://github.com/hazelcast/hazelcast-aws/blob/master/README.md">Hazelcast AWS repo</a>.</em></p>

<a name="discovering-members-within-azure-cloud"></a><h3 id="discovering-members-within-azure-cloud">Discovering Members within Azure Cloud</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Hazelcast offers a discovery strategy for your Hazelcast applications running on Azure. This strategy provides all of your Hazelcast instances by returning the virtual machines within your Azure resource group that are tagged with a specified value. This discovery feature is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-azure" target="_blank">Hazelcast Azure</a> for information on configuring and using it.</p>

<a name="discovering-members-with-jclouds"></a><h3 id="discovering-members-with-jclouds">Discovering Members with jclouds</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Hazelcast members and native clients support jclouds&reg; for discovery. It is useful when you do not want to provide or you cannot provide the list of possible IP addresses on various cloud providers. This discovery feature is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-jclouds" target="_blank">Hazelcast JClouds</a> for information on configuring and using it.</p>

<a name="discovering-native-clients"></a><h3 id="discovering-native-clients">Discovering Native Clients</h3>
<p>Hazelcast members and native clients can find each other with multicast discovery plugin. This plugin is implemented using <a href="#discovery-spi">Hazelcast Discovery SPI</a>. You should configure the plugin both at Hazelcast members and clients in order to use multicast discovery.</p>
<p>To configure your cluster to have the multicast discovery plugin, follow these steps:</p>
<ul>
<li>Disable the multicast and TCP/IP join mechanisms. To do this, set the <code>enabled</code> attributes of the <code>multicast</code> and <code>tcp-ip</code> elements to <code>false</code> in your <code>hazelcast.xml</code> configuration file</li>
<li>Set the <code>enabled</code> attribute of the <code>hazelcast.discovery.enabled</code> property to <code>true</code>.</li>
<li>Add multicast discovery strategy configuration to your XML file, i.e., <code>&lt;discovery-strategies&gt;</code> element.</li>
</ul>
<p>The following is an example declarative configuration.</p>
<pre><code class="lang-xml"> ...
  &lt;properties&gt;
    &lt;property name=&quot;hazelcast.discovery.enabled&quot;&gt;true&lt;/property&gt;
  &lt;/properties&gt;
   ....
 &lt;join&gt;
    &lt;multicast enabled=&quot;false&quot;&gt;
    &lt;/multicast&gt;
    &lt;tcp-ip enabled=&quot;false&quot;&gt;
    &lt;/tcp-ip&gt;
    &lt;discovery-strategies&gt;
        &lt;discovery-strategy class=&quot;com.hazelcast.spi.discovery.multicast.MulticastDiscoveryStrategy&quot; enabled=&quot;true&quot;&gt;
          &lt;properties&gt;
          &lt;property name=&quot;group&quot;&gt;224.2.2.3&lt;/property&gt;
          &lt;property name=&quot;port&quot;&gt;54327&lt;/property&gt;
          &lt;/properties&gt;
        &lt;/discovery-strategy&gt;
    &lt;/discovery-strategies&gt;
&lt;/join&gt;
...
</code></pre>
<p>The table below lists the multicast discovery plugin configuration properties with their descriptions.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>group</code></td>
<td style="text-align:left">String</td>
<td style="text-align:left">String value that is used to set the multicast group, so that you can isolate your clusters.</td>
</tr>
<tr>
<td style="text-align:left"><code>port</code></td>
<td style="text-align:left">Integer</td>
<td style="text-align:left">Integer value that is used to set the multicast port.</td>
</tr>
</tbody>
</table>

<a name="creating-cluster-groups"></a><h2 id="creating-cluster-groups">Creating Cluster Groups</h2>
<p>You can create cluster groups. To do this, use the <code>group</code> configuration element. </p>
<p>By specifying a group name and group password, you can separate your clusters in a simple way. Example groupings can be by <em>development</em>, <em>production</em>, <em>test</em>, <em>app</em>, etc. The following is an example declarative configuration.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;group&gt;
    &lt;name&gt;app1&lt;/name&gt;
    &lt;password&gt;app1-pass&lt;/password&gt;
  &lt;/group&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>You can also define the cluster groups using the programmatic configuration. A JVM can host multiple Hazelcast instances. Each Hazelcast instance can only participate in one group. Each Hazelcast instance only joins to its own group and does not interact with other groups. The following code example creates three separate Hazelcast instances--<code>h1</code> belongs to the <code>app1</code> cluster, while <code>h2</code> and <code>h3</code> belong to the <code>app2</code> cluster.</p>
<pre><code class="lang-java">Config configApp1 = new Config();
configApp1.getGroupConfig().setName( &quot;app1&quot; ).setPassword( &quot;app1-pass&quot; );

Config configApp2 = new Config();
configApp2.getGroupConfig().setName( &quot;app2&quot; ).setPassword( &quot;app2-pass&quot; );

HazelcastInstance h1 = Hazelcast.newHazelcastInstance( configApp1 );
HazelcastInstance h2 = Hazelcast.newHazelcastInstance( configApp2 );
HazelcastInstance h3 = Hazelcast.newHazelcastInstance( configApp2 );
</code></pre>

<a name="partition-group-configuration"></a><h2 id="partition-group-configuration">Partition Group Configuration</h2>
<p>Hazelcast distributes key objects into partitions using a consistent hashing algorithm. Those partitions are assigned to members. An entry is stored in the member that owns the partition to which the entry&#39;s key is assigned. The total partition count is 271 by default; you can change it with the configuration property <code>hazelcast.partition.count</code>. Please see the <a href="#system-properties">System Properties section</a>.</p>
<p>Along with those partitions, there are also copies of the partitions as backups. Backup partitions can have multiple copies due to the backup count defined in configuration, such as first backup partition, second backup partition, etc. A member cannot hold more than one copy of a partition (ownership or backup). By default, Hazelcast distributes partitions and their backup copies randomly and equally among cluster members, assuming all members in the cluster are identical.</p>
<p>But what if some members share the same JVM or physical machine or chassis and you want backups of these members to be assigned to members in another machine or chassis? What if processing or memory capacities of some members are different and you do not want an equal number of partitions to be assigned to all members?</p>
<p>You can group members in the same JVM (or physical machine) or members located in the same chassis. Or you can group members to create identical capacity. We call these groups <strong>partition groups</strong>. Partitions are assigned to those partition groups instead of to single members. Backups of these partitions are located in another partition group.</p>
<a name="grouping-types"></a><h3 id="grouping-types">Grouping Types</h3>
<p>When you enable partition grouping, Hazelcast presents the following choices for you to configure partition groups.</p>
<p><strong>1. HOST_AWARE:</strong> </p>
<p>You can group members automatically using the IP addresses of members, so members sharing the same network interface will be grouped together. All members on the same host (IP address or domain name) will be a single partition group. This helps to avoid data loss when a physical server crashes, because multiple replicas of the same partition are not stored on the same host. But if there are multiple network interfaces or domain names per physical machine, that will make this assumption invalid.</p>
<p>Following are declarative and programmatic configuration snippets that show how to enable HOST_AWARE grouping.</p>
<pre><code class="lang-xml">&lt;partition-group enabled=&quot;true&quot; group-type=&quot;HOST_AWARE&quot; /&gt;
</code></pre>
<pre><code class="lang-java">Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled( true )
    .setGroupType( MemberGroupType.HOST_AWARE );
</code></pre>
<p><strong>2. CUSTOM:</strong></p>
<p>You can do custom grouping using Hazelcast&#39;s interface matching configuration. This way, you can add different and multiple interfaces to a group. You can also use wildcards in the interface addresses. For example, the users can create rack-aware or data warehouse partition groups using custom partition grouping.</p>
<p>Following are declarative and programmatic configuration examples that show how to enable and use CUSTOM grouping.</p>
<pre><code class="lang-xml">&lt;partition-group enabled=&quot;true&quot; group-type=&quot;CUSTOM&quot;&gt;
&lt;member-group&gt;
  &lt;interface&gt;10.10.0.*&lt;/interface&gt;
  &lt;interface&gt;10.10.3.*&lt;/interface&gt;
  &lt;interface&gt;10.10.5.*&lt;/interface&gt;
&lt;/member-group&gt;
&lt;member-group&gt;
  &lt;interface&gt;10.10.10.10-100&lt;/interface&gt;
  &lt;interface&gt;10.10.1.*&lt;/interface&gt;
  &lt;interface&gt;10.10.2.*&lt;/interface&gt;
&lt;/member-group
&lt;/partition-group&gt;
</code></pre>
<pre><code class="lang-java">Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled( true )
    .setGroupType( MemberGroupType.CUSTOM );

MemberGroupConfig memberGroupConfig = new MemberGroupConfig();
memberGroupConfig.addInterface( &quot;10.10.0.*&quot; )
.addInterface( &quot;10.10.3.*&quot; ).addInterface(&quot;10.10.5.*&quot; );

MemberGroupConfig memberGroupConfig2 = new MemberGroupConfig();
memberGroupConfig2.addInterface( &quot;10.10.10.10-100&quot; )
.addInterface( &quot;10.10.1.*&quot;).addInterface( &quot;10.10.2.*&quot; );

partitionGroupConfig.addMemberGroupConfig( memberGroupConfig );
partitionGroupConfig.addMemberGroupConfig( memberGroupConfig2 );
</code></pre>
<p><strong>3. PER_MEMBER:</strong></p>
<p>You can give every member its own group. Each member is a group of its own and primary and backup partitions are distributed randomly (not on the same physical member). This gives the least amount of protection and is the default configuration for a Hazelcast cluster. This grouping type provides good redundancy when Hazelcast members are on separate hosts. However, if multiple instances run on the same host, this type is not a good option. </p>
<p>Following are declarative and programmatic configuration snippets that show how to enable PER_MEMBER grouping.</p>
<pre><code class="lang-xml">&lt;partition-group enabled=&quot;true&quot; group-type=&quot;PER_MEMBER&quot; /&gt;
</code></pre>
<pre><code class="lang-java">Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled( true )
    .setGroupType( MemberGroupType.PER_MEMBER );
</code></pre>
<p><strong>4. ZONE_AWARE:</strong></p>
<p>You can use ZONE_AWARE configuration with Hazelcast jclouds or Hazelcast Azure Discovery Service plugins. </p>
<p>As discovery services, these plugins put zone, rack, and host information to the Hazelcast <a href="#defining-member-attributes">member attributes</a> map during the discovery process. Hazelcast creates the partition groups with respect to member attributes map entries that include zone, rack, and host information. </p>
<p>When using ZONE_AWARE configuration, backups are created in the other zones. Each zone will be accepted as one partition group.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Some cloud providers have rack information instead of zone information. In such cases, Hazelcast looks for zone, rack, and host information in the given order.</em>
<br></br></p>
<p>Following are declarative and programmatic configuration snippets that show how to enable ZONE_AWARE grouping.</p>
<pre><code class="lang-xml">&lt;partition-group enabled=&quot;true&quot; group-type=&quot;ZONE_AWARE&quot; /&gt;
</code></pre>
<pre><code class="lang-java">Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled( true )
    .setGroupType( MemberGroupType.ZONE_AWARE );
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Currently ZONE_AWARE configuration works only with Hazelcast jclouds and Hazelcast Azure Discovery Service plugins. Please refer to their GitHub repositories at <a href="https://github.com/hazelcast/hazelcast-jclouds">Hazelcast jclouds</a> and <a href="https://github.com/hazelcast/hazelcast-azure">Hazelcast Azure</a> for more information on these plugins.</em> </p>
<p><strong>5. SPI:</strong></p>
<p>You can provide your own partition group implementation using the SPI configuration. To create your partition group implementation, you need to first extend the <code>DiscoveryStrategy</code> class of the discovery service plugin, override the method <code>public PartitionGroupStrategy getPartitionGroupStrategy()</code>, and return the <code>PartitionGroupStrategy</code> configuration in that overridden method. </p>
<p>Following is a sample code covering the implementation steps mentioned in the above paragraph: </p>
<pre><code>public class CustomDiscovery extends JCloudsDiscoveryStrategy {

    public CustomDiscovery(Map&lt;String, Comparable&gt; properties) {
        super(properties);
    }

    @Override
    public PartitionGroupStrategy getPartitionGroupStrategy() {
        return new CustomPartitionGroupStrategy();
    }

    private class CustomPartitionGroupStrategy implements PartitionGroupStrategy {
        @Override
        public Iterable&lt;MemberGroup&gt; getMemberGroups() {
            ...
            ...
        }
    }
}
</code></pre>
<a name="logging-configuration"></a><h2 id="logging-configuration">Logging Configuration</h2>
<p>Hazelcast has a flexible logging configuration and does not depend on any logging framework except JDK logging. It has built-in adapters for a number of logging frameworks and it also supports custom loggers by providing logging interfaces.</p>
<p>To use built-in adapters, set the <code>hazelcast.logging.type</code> property to one of the predefined types below.</p>
<ul>
<li><p><strong>jdk</strong>: JDK logging (default)</p>
</li>
<li><p><strong>log4j</strong>: Log4j</p>
</li>
<li><p><strong>slf4j</strong>: Slf4j</p>
</li>
<li><p><strong>none</strong>: disable logging</p>
</li>
</ul>
<p>You can set <code>hazelcast.logging.type</code> through declarative configuration, programmatic configuration, or JVM system property.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you choose to use <code>log4j</code> or <code>slf4j</code>, you should include the proper dependencies in the classpath.</em>
<br></br></p>
<p><strong>Declarative Configuration</strong></p>
<pre><code class="lang-xml">  ....
  &lt;properties&gt;
    &lt;property name=&quot;hazelcast.logging.type&quot;&gt;jdk&lt;/property&gt;
    ....
  &lt;/properties&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic Configuration</strong></p>
<pre><code class="lang-java">Config config = new Config() ;
config.setProperty( &quot;hazelcast.logging.type&quot;, &quot;log4j&quot; );
</code></pre>
<p><strong>System Property</strong></p>
<ul>
<li>Using JVM parameter: <code>java -Dhazelcast.logging.type=slf4j</code></li>
<li>Using System class: <code>System.setProperty( &quot;hazelcast.logging.type&quot;, &quot;none&quot; );</code></li>
</ul>
<p>If the provided logging mechanisms are not satisfactory, you can implement your own using the custom logging feature. To use it, implement the <code>com.hazelcast.logging.LoggerFactory</code> and <code>com.hazelcast.logging.ILogger</code> interfaces and set the system property <code>hazelcast.logging.class</code> as your custom <code>LoggerFactory</code> class name.</p>
<pre><code class="lang-plain">-Dhazelcast.logging.class=foo.bar.MyLoggingFactory
</code></pre>
<p>You can also listen to logging events generated by Hazelcast runtime by registering <code>LogListener</code>s to <code>LoggingService</code>.</p>
<pre><code class="lang-java">LogListener listener = new LogListener() {
  public void log( LogEvent logEvent ) {
    // do something
  }
}
HazelcastInstance instance = Hazelcast.newHazelcastInstance();
LoggingService loggingService = instance.getLoggingService();
loggingService.addLogListener( Level.INFO, listener );
</code></pre>
<p>Through the <code>LoggingService</code>, you can get the currently used ILogger implementation and log your own messages too.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you are not using command line for configuring logging, you should be careful about Hazelcast classes. They may be defaulted to <code>jdk</code> logging before newly configured logging is read. When logging mechanism is selected, it will not change.</em></p>
<p><br></br></p>

<a name="other-network-configurations"></a><h2 id="other-network-configurations">Other Network Configurations</h2>
<p>All network related configurations are performed via the <code>network</code> element in the Hazelcast XML configuration file or the class <code>NetworkConfig</code> when using programmatic configuration. Following subsections describe the available configurations that you can perform under the <code>network</code> element.</p>
<a name="public-address"></a><h3 id="public-address">Public Address</h3>
<p><code>public-address</code> overrides the public address of a member. By default, a member selects its socket address as its public address. But behind a network address translation (NAT), two endpoints (members) may not be able to see/access each other. If both members set their public addresses to their defined addresses on NAT, then that way they can communicate with each other. In this case, their public addresses are not an address of a local network interface but a virtual address defined by NAT. It is optional to set and useful when you have a private cloud. Note that, the value for this element should be given in the format <em><code>host IP address:port number</code></em>. See the following examples.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;network&gt;
    &lt;public-address&gt;11.22.33.44:5555&lt;/public-address&gt;
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
config.getNetworkConfig()
      .setPublicAddress( &quot;11.22.33.44&quot;, &quot;5555&quot; );
</code></pre>
<a name="port"></a><h3 id="port">Port</h3>
<p>You can specify the ports that Hazelcast will use to communicate between cluster members. Its default value is <code>5701</code>. The following are example configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;network&gt;
  &lt;port port-count=&quot;20&quot; auto-increment=&quot;false&quot;&gt;5701&lt;/port&gt;
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
config.getNetworkConfig().setPort( &quot;5701&quot; ); 
             .setPortCount( &quot;20&quot; ).setPortAutoIncrement( false );
</code></pre>
<p><code>port</code> has the following attributes.</p>
<ul>
<li><code>port-count</code>: By default, Hazelcast will try 100 ports to bind. Meaning that, if you set the value of port as 5701, as members are joining to the cluster, Hazelcast tries to find ports between 5701 and 5801. You can choose to change the port count in the cases like having large instances on a single machine or willing to have only a few ports to be assigned. The parameter <code>port-count</code> is used for this purpose, whose default value is 100.</li>
</ul>
<ul>
<li><code>auto-increment</code>: According to the above example, Hazelcast will try to find free ports between 5701 and 5801. Normally, you will not need to change this value, but it will come very handy when needed. You may also want to choose to use only one port. In that case, you can disable the auto-increment feature of <code>port</code> by setting <code>auto-increment</code> to <code>false</code>.</li>
</ul>
<p>The parameter <code>port-count</code> is ignored when the above configuration is made.</p>
<a name="outbound-ports"></a><h3 id="outbound-ports">Outbound Ports</h3>
<p>By default, Hazelcast lets the system pick up an ephemeral port during socket bind operation. But security policies/firewalls may require you to restrict outbound ports to be used by Hazelcast-enabled applications. To fulfill this requirement, you can configure Hazelcast to use only defined outbound ports. The following are example configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">  &lt;network&gt;
    &lt;outbound-ports&gt;
      &lt;!-- ports between 33000 and 35000 --&gt;
      &lt;ports&gt;33000-35000&lt;/ports&gt;
      &lt;!-- comma separated ports --&gt;
      &lt;ports&gt;37000,37001,37002,37003&lt;/ports&gt; 
      &lt;ports&gt;38000,38500-38600&lt;/ports&gt;
    &lt;/outbound-ports&gt;
  &lt;/network&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">...
NetworkConfig networkConfig = config.getNetworkConfig();
// ports between 35000 and 35100
networkConfig.addOutboundPortDefinition(&quot;35000-35100&quot;);
// comma separated ports
networkConfig.addOutboundPortDefinition(&quot;36001, 36002, 36003&quot;);
networkConfig.addOutboundPort(37000);
networkConfig.addOutboundPort(37001);
...
</code></pre>
<p><strong><em>Note:</em></strong> <em>You can use port ranges and/or comma separated ports.</em></p>
<p>As shown in the programmatic configuration, you use the method <code>addOutboundPort</code> to add only one port. If you need to add a group of ports, then use the method <code>addOutboundPortDefinition</code>. </p>
<p>In the declarative configuration, the element <code>ports</code> can be used for both single and multiple port definitions.</p>
<a name="reuse-address"></a><h3 id="reuse-address">Reuse Address</h3>
<p>When you shutdown a cluster member, the server socket port will be in the <code>TIME_WAIT</code> state for the next couple of minutes. If you start the member right after shutting it down, you may not be able to bind it to the same port because it is in the <code>TIME_WAIT</code> state. If you set the <code>reuse-address</code> element to <code>true</code>, the <code>TIME_WAIT</code> state is ignored and you can bind the member to the same port again.</p>
<p>The following are example configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">  &lt;network&gt;
    &lt;reuse-address&gt;true&lt;/reuse-address&gt;
  &lt;/network&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">...
NetworkConfig networkConfig = config.getNetworkConfig();

networkConfig.setReuseAddress( true );
...
</code></pre>
<a name="join"></a><h3 id="join">Join</h3>
<p>The <code>join</code> configuration element is used to discover Hazelcast members and enable them to form a cluster. Hazelcast provides multicast, TCP/IP, EC2, and jclouds&reg; discovery mechanisms. These mechanisms are explained the <a href="#discovering-cluster-members">Discovering Cluster Members section</a>. This section describes all the sub-elements and attributes of <code>join</code> element. The following are example configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">   &lt;network&gt;
        &lt;join&gt;
            &lt;multicast enabled=&quot;true&quot;&gt;
                &lt;multicast-group&gt;224.2.2.3&lt;/multicast-group&gt;
                &lt;multicast-port&gt;54327&lt;/multicast-port&gt;
                &lt;multicast-time-to-live&gt;32&lt;/multicast-time-to-live&gt;
                &lt;multicast-timeout-seconds&gt;2&lt;/multicast-timeout-seconds&gt;
                &lt;trusted-interfaces&gt;
                   &lt;interface&gt;192.168.1.102&lt;/interface&gt;
                &lt;/trusted-interfaces&gt;   
            &lt;/multicast&gt;
            &lt;tcp-ip enabled=&quot;false&quot;&gt;
                &lt;required-member&gt;192.168.1.104&lt;/required-member&gt;
                &lt;member&gt;192.168.1.104&lt;/member&gt;
                &lt;members&gt;192.168.1.105,192.168.1.106&lt;/members&gt;
            &lt;/tcp-ip&gt;
            &lt;aws enabled=&quot;false&quot;&gt;
                &lt;access-key&gt;my-access-key&lt;/access-key&gt;
                &lt;secret-key&gt;my-secret-key&lt;/secret-key&gt;
                &lt;region&gt;us-west-1&lt;/region&gt;
                &lt;host-header&gt;ec2.amazonaws.com&lt;/host-header&gt;
                &lt;security-group-name&gt;hazelcast-sg&lt;/security-group-name&gt;
                &lt;tag-key&gt;type&lt;/tag-key&gt;
                &lt;tag-value&gt;hz-members&lt;/tag-value&gt;
            &lt;/aws&gt;
            &lt;discovery-strategies&gt;
              &lt;discovery-strategy ... /&gt;
            &lt;/discovery-strategies&gt;
        &lt;/join&gt;
   &lt;network&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
NetworkConfig network = config.getNetworkConfig();
JoinConfig join = network.getJoin();
join.getMulticastConfig().setEnabled( false )
            .addTrustedInterface( &quot;192.168.1.102&quot; );
join.getTcpIpConfig().addMember( &quot;10.45.67.32&quot; ).addMember( &quot;10.45.67.100&quot; )
            .setRequiredMember( &quot;192.168.10.100&quot; ).setEnabled( true );
</code></pre>
<p>The <code>join</code> element has the following sub-elements and attributes.</p>
<a name="multicast-element"></a><h4 id="multicast-element">multicast element</h4>
<p>The <code>multicast</code> element includes parameters to fine tune the multicast join mechanism.</p>
<ul>
<li><code>enabled</code>: Specifies whether the multicast discovery is enabled or not, <code>true</code> or <code>false</code>.</li>
<li><code>multicast-group</code>: The multicast group IP address. Specify it when you want to create clusters within the same network. Values can be between 224.0.0.0 and 239.255.255.255. Default value is 224.2.2.3.</li>
<li><code>multicast-port</code>: The multicast socket port that the Hazelcast member listens to and sends discovery messages through. Default value is 54327.</li>
<li><code>multicast-time-to-live</code>: Time-to-live value for multicast packets sent out to control the scope of multicasts. See more information <a href="http://www.tldp.org/HOWTO/Multicast-HOWTO-2.html">here</a>.</li>
<li><code>multicast-timeout-seconds</code>: Only when the members are starting up, this timeout (in seconds) specifies the period during which a member waits for a multicast response from another member. For example, if you set it as 60 seconds, each member will wait for 60 seconds until a leader member is selected. Its default value is 2 seconds. </li>
<li><code>trusted-interfaces</code>: Includes IP addresses of trusted members. When a member wants to join to the cluster, its join request will be rejected if it is not a trusted member. You can give an IP addresses range using the wildcard (*) on the last digit of IP address (e.g. 192.168.1.* or 192.168.1.100-110).</li>
</ul>
<a name="tcp-ip-element"></a><h4 id="tcp-ip-element">tcp-ip element</h4>
<p>The <code>tcp-ip</code> element includes parameters to fine tune the TCP/IP join mechanism.</p>
<ul>
<li><code>enabled</code>: Specifies whether the TCP/IP discovery is enabled or not. Values can be <code>true</code> or <code>false</code>.</li>
<li><code>required-member</code>: IP address of the required member. Cluster will only formed if the member with this IP address is found.</li>
<li><code>member</code>: IP address(es) of one or more well known members. Once members are connected to these well known ones, all member addresses will be communicated with each other. You can also give comma separated IP addresses using the <code>members</code> element.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em><code>tcp-ip</code> element also accepts the <code>interface</code> parameter. Please refer to the <a href="#interfaces">Interfaces element description</a>.</em></p>
<ul>
<li><code>connection-timeout-seconds</code>: Defines the connection timeout. This is the maximum amount of time Hazelcast is going to try to connect to a well known member before giving up. Setting it to a too low value could mean that a member is not able to connect to a cluster. Setting it to a too high value means that member startup could slow down because of longer timeouts (e.g. when a well known member is not up). Increasing this value is recommended if you have many IPs listed and the members cannot properly build up the cluster. Its default value is 5.</li>
</ul>
<a name="aws-element"></a><h4 id="aws-element">aws element</h4>
<p>The <code>aws</code> element includes parameters to allow the members to form a cluster on the Amazon EC2 environment.</p>
<ul>
<li><code>enabled</code>: Specifies whether the EC2 discovery is enabled or not, <code>true</code> or <code>false</code>.</li>
<li><code>access-key</code>, <code>secret-key</code>: Access and secret keys of your account on EC2.</li>
<li><code>region</code>: The region where your members are running. Default value is <code>us-east-1</code>. You need to specify this if the region is other than the default one.</li>
<li><code>host-header</code>: The URL that is the entry point for a web service. It is optional.</li>
<li><code>security-group-name</code>: Name of the security group you specified at the EC2 management console. It is used to narrow the Hazelcast members to be within this group. It is optional.</li>
<li><code>tag-key</code>, <code>tag-value</code>: To narrow the members in the cloud down to only Hazelcast members, you can set these parameters as the ones you specified in the EC2 console. They are optional.</li>
<li><code>connection-timeout-seconds</code>: The maximum amount of time Hazelcast will try to connect to a well known member before giving up. Setting this value too low could mean that a member is not able to connect to a cluster. Setting the value too high means that member startup could slow down because of longer timeouts (for example, when a well known member is not up). Increasing this value is recommended if you have many IPs listed and the members cannot properly build up the cluster. Its default value is 5.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you are using a cloud provider other than AWS, you can use the programmatic configuration to specify a TCP/IP cluster. The members will need to be retrieved from that provider (e.g. JClouds).</em></p>
<a name="discovery-strategies-element"></a><h4 id="discovery-strategies-element">discovery-strategies element</h4>
<p>The <code>discovery-strategies</code> element configures internal or external discovery strategies based on the Hazelcast Discovery SPI. For further information, please refer to the <a href="#discovery-spi">Discovery SPI section</a> and the vendor documentation of the used discovery strategy.</p>
<a name="awsclient-configuration"></a><h5 id="awsclient-configuration">AWSClient Configuration</h5>
<p>To make sure EC2 instances are found correctly, you can use the <code>AWSClient</code> class. It determines the private IP addresses of EC2 instances to be connected. Give the <code>AWSClient</code> class the values for the parameters that you specified in the <code>aws</code> element, as shown below. You will see whether your EC2 instances are found.</p>
<pre><code class="lang-java">public static void main( String[] args )throws Exception{ 
  AwsConfig config = new AwsConfig(); 
  config.setSecretKey( ... ) ;
  config.setSecretKey( ... );
  config.setRegion( ... );
  config.setSecurityGroupName( ... );
  config.setTagKey( ... );
  config.setTagValue( ... );
  config.setEnabled( true );
  AWSClient client = new AWSClient( config );
  List&lt;String&gt; ipAddresses = client.getPrivateIpAddresses();
  System.out.println( &quot;addresses found:&quot; + ipAddresses ); 
  for ( String ip: ipAddresses ) {
    System.out.println( ip ); 
  }
}
</code></pre>
<a name="interfaces"></a><h3 id="interfaces">Interfaces</h3>
<p>You can specify which network interfaces that Hazelcast should use. Servers mostly have more than one network interface, so you may want to list the valid IPs. Range characters (&#39;*&#39; and &#39;-&#39;) can be used for simplicity. For instance, 10.3.10.* refers to IPs between 10.3.10.0 and 10.3.10.255. Interface 10.3.10.4-18 refers to IPs between 10.3.10.4 and 10.3.10.18 (4 and 18 included). If network interface configuration is enabled (it is disabled by default) and if Hazelcast cannot find an matching interface, then it will print a message on the console and will not start on that member.</p>
<p>The following are example configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    ...
    &lt;interfaces enabled=&quot;true&quot;&gt;
      &lt;interface&gt;10.3.16.*&lt;/interface&gt; 
      &lt;interface&gt;10.3.10.4-18&lt;/interface&gt; 
      &lt;interface&gt;192.168.1.3&lt;/interface&gt;         
    &lt;/interfaces&gt;    
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
NetworkConfig network = config.getNetworkConfig();
InterfacesConfig interface = network.getInterfaces();
interface.setEnabled( true )
            .addInterface( &quot;192.168.1.3&quot; );
</code></pre>
<a name="ipv6-support"></a><h3 id="ipv6-support">IPv6 Support</h3>
<p>Hazelcast supports IPv6 addresses seamlessly (This support is switched off by default, please see the note at the end of this section).</p>
<p>All you need is to define IPv6 addresses or interfaces in <a href="#network-configuration">network configuration</a>. The only current limitation is that you cannot define wildcard IPv6 addresses in the TCP/IP join configuration (<code>tcp-ip</code> element). <a href="#interfaces">Interfaces</a> configuration does not have this limitation, you can configure wildcard IPv6 interfaces in the same way as IPv4 interfaces.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    &lt;port auto-increment=&quot;true&quot;&gt;5701&lt;/port&gt;
    &lt;join&gt;
      &lt;multicast enabled=&quot;false&quot;&gt;
        &lt;multicast-group&gt;FF02:0:0:0:0:0:0:1&lt;/multicast-group&gt;
        &lt;multicast-port&gt;54327&lt;/multicast-port&gt;
      &lt;/multicast&gt;
      &lt;tcp-ip enabled=&quot;true&quot;&gt;
        &lt;member&gt;[fe80::223:6cff:fe93:7c7e]:5701&lt;/member&gt;
        &lt;interface&gt;192.168.1.0-7&lt;/interface&gt;
        &lt;interface&gt;192.168.1.*&lt;/interface&gt;
        &lt;interface&gt;fe80:0:0:0:45c5:47ee:fe15:493a&lt;/interface&gt;
      &lt;/tcp-ip&gt;
    &lt;/join&gt;
    &lt;interfaces enabled=&quot;true&quot;&gt;
      &lt;interface&gt;10.3.16.*&lt;/interface&gt;
      &lt;interface&gt;10.3.10.4-18&lt;/interface&gt;
      &lt;interface&gt;fe80:0:0:0:45c5:47ee:fe15:*&lt;/interface&gt;
      &lt;interface&gt;fe80::223:6cff:fe93:0-5555&lt;/interface&gt;
    &lt;/interfaces&gt;
    ...
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>JVM has two system properties for setting the preferred protocol stack (IPv4 or IPv6) as well as the preferred address family types (inet4 or inet6). On a dual stack machine, IPv6 stack is preferred by default, you can change this through the <code>java.net.preferIPv4Stack=&lt;true|false&gt;</code> system property. When querying name services, JVM prefers IPv4 addresses over IPv6 addresses and will return an IPv4 address if possible. You can change this through <code>java.net.preferIPv6Addresses=&lt;true|false&gt;</code> system property.</p>
<p>Also see additional <a href="http://docs.oracle.com/javase/1.5.0/docs/guide/net/ipv6_guide/" target="_blank">details on IPv6 support in Java</a>.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>IPv6 support has been switched off by default, since some platforms have issues using the IPv6 stack. Some other platforms such as Amazon AWS have no support at all. To enable IPv6 support, just set configuration property <code>hazelcast.prefer.ipv4.stack</code> to </em>false<em>. Please refer to the <a href="#system-properties">System Properties section</a> for details.</em></p>

<a name="distributed-data-structures"></a><h1 id="distributed-data-structures">Distributed Data Structures</h1>
<p>As mentioned in the <a href="#hazelcast-overview">Overview section</a>, Hazelcast offers distributed implementations of Java interfaces. Below is the list of these implementations with links to the corresponding sections in this manual.</p>
<ul>
<li><p><strong>Standard utility collections</strong></p>
<ul>
<li><a href="#map">Map</a> is the distributed implementation of <code>java.util.Map</code>. It lets you read from and write to a Hazelcast map with methods such as <code>get</code> and <code>put</code>.</li>
<li><a href="#queue">Queue</a> is the distributed implementation of <code>java.util.concurrent.BlockingQueue</code>. You can add an item in one member and remove it from another one.</li>
<li><a href="#ringbuffer">Ringbuffer</a> is implemented for reliable eventing system. It is also a distributed data structure.</li>
<li><a href="#set">Set</a> is the distributed and concurrent implementation of <code>java.util.Set</code>. It does not allow duplicate elements and does not preserve their order.</li>
<li><a href="#list">List</a> is similar to Hazelcast Set. The only difference is that it allows duplicate elements and preserves their order.</li>
<li><a href="#multimap">MultiMap</a> is a specialized Hazelcast map. It is a distributed data structure where you can store multiple values for a single key.</li>
<li><a href="#replicated-map">Replicated Map</a> does not partition data. It does not spread data to different cluster members. Instead, it replicates the data to all members.</li>
</ul>
</li>
<li><p><strong>Topic</strong> is the distributed mechanism for publishing messages that are delivered to multiple subscribers. It is also known as the publish/subscribe (pub/sub) messaging model. Please see the <a href="#topic">Topic section</a> for more information. Hazelcast also has a structure called Reliable Topic which uses the same interface of Hazelcast Topic. The difference is that it is backed up by the Ringbuffer data structure. Please see the <a href="#reliable-topic">Reliable Topic section</a>.</p>
</li>
<li><p><strong>Concurrency utilities</strong></p>
<ul>
<li><a href="#lock">Lock</a> is the distributed implementation of <code>java.util.concurrent.locks.Lock</code>. When you use lock, the critical section that Hazelcast Lock guards is guaranteed to be executed by only one thread in the entire cluster.</li>
<li><a href="#isemaphore">Semaphore</a> is the distributed implementation of <code>java.util.concurrent.Semaphore</code>. When performing concurrent activities, semaphores offer permits to control the thread counts.</li>
<li><a href="#iatomiclong">AtomicLong</a> is the distributed implementation of <code>java.util.concurrent.atomic.AtomicLong</code>. Most of AtomicLong&#39;s operations are available. However, these operations involve remote calls and hence their performances differ from AtomicLong, due to being distributed.</li>
<li><a href="#iatomicreference">AtomicReference</a> is the distributed implementation of <code>java.util.concurrent.atomic.AtomicReference</code>. When you need to deal with a reference in a distributed environment, you can use Hazelcast AtomicReference. </li>
<li><a href="#idgenerator">IdGenerator</a> is used to generate cluster-wide unique identifiers. ID generation occurs almost at the speed of <code>AtomicLong.incrementAndGet()</code>.</li>
<li><a href="#icountdownlatch">CountdownLatch</a> is the distributed implementation of <code>java.util.concurrent.CountDownLatch</code>. Hazelcast CountDownLatch is a gate keeper for concurrent activities. It enables the threads to wait for other threads to complete their operations.</li>
</ul>
</li>
</ul>
<p>Common Features of all Hazelcast Data Structures</p>
<ul>
<li>If a member goes down, its backup replica (which holds the same data) will dynamically redistribute the data, including the ownership and locks on them, to the remaining live members. As a result, there will not be any data loss.</li>
<li>There is no single cluster master that can be a single point of failure. Every member in the cluster has equal rights and responsibilities. No single member is superior. There is no dependency on an external &#39;server&#39; or &#39;master&#39;.</li>
</ul>
<p>Here is an example of how you can retrieve existing data structure instances (map, queue, set, lock, topic, etc.) and how you can listen for instance events, such as an instance being created or destroyed.</p>
<pre><code class="lang-java">import java.util.Collection;
import com.hazelcast.config.Config;
import com.hazelcast.core.*;

public class Sample implements DistributedObjectListener {
  public static void main(String[] args) {
    Sample sample = new Sample();

    Config config = new Config();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(config);
    hazelcastInstance.addDistributedObjectListener(sample);

    Collection&lt;DistributedObject&gt; distributedObjects = hazelcastInstance.getDistributedObjects();
    for (DistributedObject distributedObject : distributedObjects) {
      System.out.println(distributedObject.getName() + &quot;,&quot; + distributedObject.getId());
    }
  }

  @Override
  public void distributedObjectCreated(DistributedObjectEvent event) {
    DistributedObject instance = event.getDistributedObject();
    System.out.println(&quot;Created &quot; + instance.getName() + &quot;,&quot; + instance.getId());
  }

  @Override
  public void distributedObjectDestroyed(DistributedObjectEvent event) {
    DistributedObject instance = event.getDistributedObject();
    System.out.println(&quot;Destroyed &quot; + instance.getName() + &quot;,&quot; + instance.getId());
  }
}
</code></pre>

<a name="map"></a><h2 id="map">Map</h2>
<p>Hazelcast Map (<code>IMap</code>) extends the interface <code>java.util.concurrent.ConcurrentMap</code> and hence <code>java.util.Map</code>. It is the distributed implementation of Java map. You can perform operations like reading and writing from/to a Hazelcast map with the well known get and put methods.</p>
<a name="getting-a-map-and-putting-an-entry"></a><h3 id="getting-a-map-and-putting-an-entry">Getting a Map and Putting an Entry</h3>
<p>Hazelcast will partition your map entries and almost evenly distribute them onto all Hazelcast members. Each member carries approximately &quot;(1/n <code>*</code> total-data) + backups&quot;, <strong>n</strong> being the number of members in the cluster. For example, if you have a member with 1000 objects to be stored in the cluster, and then you start a second member, each member will both store 500 objects and back up the 500 objects in the other member.</p>
<p>Let&#39;s create a Hazelcast instance and fill a map named <code>Capitals</code> with key-value pairs using the following code. Use the HazelcastInstance <code>getMap</code> method to get the map, then use the map <code>put</code> method to put an entry into the map.</p>
<pre><code class="lang-java">public class FillMapMember {
  public static void main( String[] args ) { 
    HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
    Map&lt;String, String&gt; capitalcities = hzInstance.getMap( &quot;capitals&quot; ); 
    capitalcities.put( &quot;1&quot;, &quot;Tokyo&quot; );
    capitalcities.put( &quot;2&quot;, &quot;Paris );
    capitalcities.put( &quot;3&quot;, &quot;Washington&quot; );
    capitalcities.put( &quot;4&quot;, &quot;Ankara&quot; );
    capitalcities.put( &quot;5&quot;, &quot;Brussels&quot; );
    capitalcities.put( &quot;6&quot;, &quot;Amsterdam&quot; );
    capitalcities.put( &quot;7&quot;, &quot;New Delhi&quot; );
    capitalcities.put( &quot;8&quot;, &quot;London&quot; );
    capitalcities.put( &quot;9&quot;, &quot;Berlin&quot; );
    capitalcities.put( &quot;10&quot;, &quot;Oslo&quot; );
    capitalcities.put( &quot;11&quot;, &quot;Moscow&quot; );
    ...
    ...
    capitalcities.put( &quot;120&quot;, &quot;Stockholm&quot; )
  }
}
</code></pre>
<p>When you run this code, a cluster member is created with a map whose entries are distributed across the members&#39; partitions. See the below illustration. For now, this is a single member cluster.</p>
<p><img src="images/1Node.png" alt="Map Entries in a Single Member"></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Please note that some of the partitions will not contain any data entries since we only have 120 objects and the partition count is 271 by default. This count is configurable and can be changed using the system property <code>hazelcast.partition.count</code>. Please see the <a href="#system-properties">System Properties section</a>.</em></p>
<a name="creating-a-member-for-map-backup"></a><h4 id="creating-a-member-for-map-backup">Creating A Member for Map Backup</h4>
<p>Now let&#39;s create a second member by running the above code again. This will create a cluster with two members. This is also where backups of entries are created--remember the backup partitions mentioned in the <a href="#hazelcast-overview">Hazelcast Overview section</a>. The following illustration shows two members and how the data and its backup is distributed.</p>
<p><img src="images/2Nodes.png" alt="Map Entries with Backups in Two Members"></p>
<p>As you see, when a new member joins the cluster, it takes ownership and loads some of the data in the cluster. Eventually, it will carry almost &quot;(1/n <code>*</code> total-data) + backups&quot; of the data, reducing the load on other members.</p>
<p><code>HazelcastInstance::getMap</code> returns an instance of <code>com.hazelcast.core.IMap</code> which extends 
the <code>java.util.concurrent.ConcurrentMap</code> interface. Methods like 
<code>ConcurrentMap.putIfAbsent(key,value)</code> and <code>ConcurrentMap.replace(key,value)</code> can be used 
on the distributed map, as shown in the example below.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import java.util.concurrent.ConcurrentMap;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

Customer getCustomer( String id ) {
    ConcurrentMap&lt;String, Customer&gt; customers = hazelcastInstance.getMap( &quot;customers&quot; );
    Customer customer = customers.get( id );
    if (customer == null) {
        customer = new Customer( id );
        customer = customers.putIfAbsent( id, customer );
    }
    return customer;
}               

public boolean updateCustomer( Customer customer ) {
    ConcurrentMap&lt;String, Customer&gt; customers = hazelcastInstance.getMap( &quot;customers&quot; );
    return ( customers.replace( customer.getId(), customer ) != null );            
}

public boolean removeCustomer( Customer customer ) {
    ConcurrentMap&lt;String, Customer&gt; customers = hazelcastInstance.getMap( &quot;customers&quot; );
    return customers.remove( customer.getId(), customer );           
}
</code></pre>
<p>All <code>ConcurrentMap</code> operations such as <code>put</code> and <code>remove</code> might wait if the key is locked by another thread in the local or remote JVM. But, they will eventually return with success. <code>ConcurrentMap</code> operations never throw a <code>java.util.ConcurrentModificationException</code>.</p>
<p>Also see:</p>
<ul>
<li><a href="#data-affinity">Data Affinity section</a>.</li>
<li><a href="#using-wildcards">Map Configuration with wildcards</a>.</li>
</ul>

<a name="backing-up-maps"></a><h3 id="backing-up-maps">Backing Up Maps</h3>
<p>Hazelcast distributes map entries onto multiple cluster members (JVMs). Each member holds some portion of the data.</p>
<p>Distributed maps have one backup by default. If a member goes down, you do not lose data. Backup operations are synchronous, so when a <code>map.put(key, value)</code> returns, it is guaranteed that the map entry is replicated to one other member. For the reads, it is also guaranteed that <code>map.get(key)</code> returns the latest value of the entry. Consistency is strictly enforced.</p>
<a name="creating-sync-backups"></a><h4 id="creating-sync-backups">Creating Sync Backups</h4>
<p>To provide data safety, Hazelcast allows you to specify the number of backup copies you want to have. That way, data on a cluster member will be copied onto other member(s). </p>
<p>To create synchronous backups, select the number of backup copies using the <code>backup-count</code> property.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;map name=&quot;default&quot;&gt;
    &lt;backup-count&gt;1&lt;/backup-count&gt;
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>When this count is 1, a map entry will have its backup on one other member in the cluster. If you set it to 2, then a map entry will have its backup on two other members. You can set it to 0 if you do not want your entries to be backed up, e.g., if performance is more important than backing up. The maximum value for the backup count is 6.</p>
<p>Hazelcast supports both synchronous and asynchronous backups. By default, backup operations are synchronous and configured with <code>backup-count</code>. In this case, backup operations block operations until backups are successfully copied to backup members (or deleted from backup members in case of remove) and acknowledgements are received. Therefore, backups are updated before a <code>put</code> operation is completed. Sync backup operations have a blocking cost which may lead to latency issues.</p>
<a name="creating-async-backups"></a><h4 id="creating-async-backups">Creating Async Backups</h4>
<p>Asynchronous backups, on the other hand, do not block operations. They are fire &amp; forget and do not require acknowledgements; the backup operations are performed at some point in time.</p>
<p>To create asynchronous backups, select the number of async backups with the <code>async-backup-count</code> property. An example is shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;map name=&quot;default&quot;&gt;
    &lt;backup-count&gt;0&lt;/backup-count&gt;
    &lt;async-backup-count&gt;1&lt;/async-backup-count&gt;
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Backups increase memory usage since they are also kept in memory.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>A map can have both sync and aysnc backups at the same time.</em></p>
<a name="enabling-backup-reads"></a><h4 id="enabling-backup-reads">Enabling Backup Reads</h4>
<p>By default, Hazelcast has one sync backup copy. If <code>backup-count</code> is set to more than 1, then each member will carry both owned entries and backup copies of other members. So for the <code>map.get(key)</code> call, it is possible that the calling member has a backup copy of that key. By default, <code>map.get(key)</code> will always read the value from the actual owner of the key for consistency.</p>
<p>To enable backup reads (read local backup entries), set the value of the <code>read-backup-data</code> property to <strong>true</strong>. Its default value is <strong>false</strong> for strong consistency. Enabling backup reads can improve performance. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;map name=&quot;default&quot;&gt;
    &lt;backup-count&gt;0&lt;/backup-count&gt;
    &lt;async-backup-count&gt;1&lt;/async-backup-count&gt;
    &lt;read-backup-data&gt;true&lt;/read-backup-data&gt;
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>This feature is available when there is at least one sync or async backup.</p>
<p>Please note that if you are performing a read from a backup, you should take into account that your hits to the keys in the backups are not reflected as hits to the original keys on the primary members. This has an impact on IMap&#39;s maximum idle seconds or time-to-live seconds expiration. Therefore, even though there is a hit on a key in backups, your original key on the primary member may expire.</p>

<a name="map-eviction"></a><h3 id="map-eviction">Map Eviction</h3>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Starting with Hazelcast 3.7, Hazelcast Map uses a new eviction mechanism which is based on the sampling of entries. Please see the <a href="#eviction-algorithm">Eviction Algorithm section</a> for details.</em></p>
<a name="evicting-map-entries"></a><h3 id="evicting-map-entries">Evicting Map Entries</h3>
<p>Unless you delete the map entries manually or use an eviction policy, they will remain in the map. Hazelcast supports policy-based eviction for distributed maps. Currently supported policies are LRU (Least Recently Used) and LFU (Least Frequently Used).</p>
<a name="understanding-map-eviction"></a><h4 id="understanding-map-eviction">Understanding Map Eviction</h4>
<p>Hazelcast Map performs eviction based on partitions. For example, when you specify a size using the <code>PER_NODE</code> attribute for <code>max-size</code> (please see <a href="#configuring-map-eviction">Configuring Map Eviction</a>), Hazelcast internally calculates the maximum size for every partition. Hazelcast uses the following equation to calculate the maximum size of a partition:</p>
<pre><code>partition maximum size = max-size * member-count / partition-count
</code></pre><p>The eviction process starts according to this calculated partition maximum size when you try to put an entry. When entry count in that partition exceeds partition maximum size, eviction starts on that partition.</p>
<p>Assume that you have the following figures as examples:</p>
<ul>
<li>Partition count: 200</li>
<li>Entry count for each partition: 100</li>
<li><code>max-size</code> (PER_NODE): 20000</li>
</ul>
<p>The total number of entries here is 20000 (partition count * entry count for each partition). This means you are at the eviction threshold since you set the <code>max-size</code> to 20000. When you try to put an entry</p>
<ol>
<li>the entry goes to the relevant partition;</li>
<li>the partition checks whether the eviction threshold is reached (<code>max-size</code>);</li>
<li>only one entry will be evicted.</li>
</ol>
<p>As a result of this eviction process, when you check the size of your map, it is 19999. After this eviction, subsequent put operations will not trigger the next eviction until the map size is again close to the <code>max-size</code>.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The above scenario is simply an example that describes how the eviction process works. Hazelcast finds the most optimum number of entries to be evicted according to your cluster size and selected policy.</em></p>
<a name="configuring-map-eviction"></a><h4 id="configuring-map-eviction">Configuring Map Eviction</h4>
<p>The following is an example declarative configuration for map eviction.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;map name=&quot;default&quot;&gt;
    ...
    &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
    &lt;max-idle-seconds&gt;0&lt;/max-idle-seconds&gt;
    &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt;
    &lt;max-size policy=&quot;PER_NODE&quot;&gt;5000&lt;/max-size&gt;
    ...
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>Let&#39;s describe each element:</p>
<ul>
<li><code>time-to-live</code>. Maximum time in seconds for each entry to stay in the map. If it is not 0, entries that are older than this time and not updated for this time are evicted automatically. Valid values are integers between 0 and <code>Integer.MAX VALUE</code>. Default value is 0, which means infinite. If it is not 0, entries are evicted regardless of the set <code>eviction-policy</code>.</li>
<li><code>max-idle-seconds</code>. Maximum time in seconds for each entry to stay idle in the map. Entries that are idle for more than this time are evicted automatically. An entry is idle if no <code>get</code>, <code>put</code>, <code>EntryProcessor.process</code> or <code>containsKey</code> is called. Valid values are integers between 0 and <code>Integer.MAX VALUE</code>. Default value is 0, which means infinite.</li>
<li><p><code>eviction-policy</code>. Valid values are described below.</p>
<ul>
<li>NONE: Default policy. If set, no items will be evicted and the property <code>max-size</code> will be ignored. You still can combine it with <code>time-to-live-seconds</code> and <code>max-idle-seconds</code>.</li>
<li>LRU: Least Recently Used.</li>
<li>LFU: Least Frequently Used.</li>
</ul>
</li>
<li><p><code>max-size</code>. Maximum size of the map. When maximum size is reached, the map is evicted based on the policy defined. Valid values are integers between 0 and <code>Integer.MAX VALUE</code>. Default value is 0. If you want <code>max-size</code> to work, set the <code>eviction-policy</code> property to a value other than NONE. Its attributes are described below.</p>
<ul>
<li><p><code>PER_NODE</code>. Maximum number of map entries in each cluster member. This is the default policy. If you use this option, please note that you cannot set the <code>max-size</code> to a value lower than the partition count (which is 271 by default).        </p>
<p>  <code>&lt;max-size policy=&quot;PER_NODE&quot;&gt;5000&lt;/max-size&gt;</code></p>
</li>
<li><p><code>PER_PARTITION</code>. Maximum number of map entries within each partition. Storage size depends on the partition count in a cluster member. This attribute should not be used often. For instance, avoid using this attribute with a small cluster. If the cluster is small, it will be hosting more partitions, and therefore map entries, than that of a larger cluster. Thus, for a small cluster, eviction of the entries will decrease performance (the number of entries is large).</p>
<p>  <code>&lt;max-size policy=&quot;PER_PARTITION&quot;&gt;27100&lt;/max-size&gt;</code></p>
</li>
<li><p><code>USED_HEAP_SIZE</code>. Maximum used heap size in megabytes per map for each Hazelcast instance. Please note that this policy does not work when <a href="#setting-in-memory-format">in-memory format</a> is set to <code>OBJECT</code>, since the memory footprint cannot be determined when data is put as <code>OBJECT</code>.</p>
<p>  <code>&lt;max-size policy=&quot;USED_HEAP_SIZE&quot;&gt;4096&lt;/max-size&gt;</code></p>
</li>
<li><p><code>USED_HEAP_PERCENTAGE</code>. Maximum used heap size percentage per map for each Hazelcast instance. If, for example, a JVM is configured to have 1000 MB and this value is 10, then the map entries will be evicted when used heap size exceeds 100 MB. Please note that this policy does not work when <a href="#setting-in-memory-format">in-memory format</a> is set to <code>OBJECT</code>, since the memory footprint cannot be determined when data is put as <code>OBJECT</code>.</p>
<p>  <code>&lt;max-size policy=&quot;USED_HEAP_PERCENTAGE&quot;&gt;10&lt;/max-size&gt;</code></p>
</li>
<li><p><code>FREE_HEAP_SIZE</code>. Minimum free heap size in megabytes for each JVM.</p>
<p>  <code>&lt;max-size policy=&quot;FREE_HEAP_SIZE&quot;&gt;512&lt;/max-size&gt;</code></p>
</li>
<li><p><code>FREE_HEAP_PERCENTAGE</code>. Minimum free heap size percentage for each JVM. If, for example, a JVM is configured to have 1000 MB and this value is 10, then the map entries will be evicted when free heap size is below 100 MB.</p>
<p>  <code>&lt;max-size policy=&quot;FREE_HEAP_PERCENTAGE&quot;&gt;10&lt;/max-size&gt;</code></p>
</li>
<li><p><code>USED_NATIVE_MEMORY_SIZE</code>. (<font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>) Maximum used native memory size in megabytes per map for each Hazelcast instance.</p>
<p>  <code>&lt;max-size policy=&quot;USED_NATIVE_MEMORY_SIZE&quot;&gt;1024&lt;/max-size&gt;</code></p>
</li>
<li><p><code>USED_NATIVE_MEMORY_PERCENTAGE</code>. (<font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>) Maximum used native memory size percentage per map for each Hazelcast instance.</p>
<p>  <code>&lt;max-size policy=&quot;USED_NATIVE_MEMORY_PERCENTAGE&quot;&gt;65&lt;/max-size&gt;</code></p>
</li>
<li><p><code>FREE_NATIVE_MEMORY_SIZE</code>. (<font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>) Minimum free native memory size in megabytes for each Hazelcast instance.</p>
<p>  <code>&lt;max-size policy=&quot;FREE_NATIVE_MEMORY_SIZE&quot;&gt;256&lt;/max-size&gt;</code></p>
</li>
<li><p><code>FREE_NATIVE_MEMORY_PERCENTAGE</code>. (<font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>) Minimum free native memory size percentage for each Hazelcast instance.</p>
<p>  <code>&lt;max-size policy=&quot;FREE_NATIVE_MEMORY_PERCENTAGE&quot;&gt;5&lt;/max-size&gt;</code></p>
</li>
</ul>
</li>
</ul>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>As of Hazelcast 3.7, the elements <code>eviction-percentage</code> and <code>min-eviction-check-millis</code> are deprecated. They will be ignored if configured since map eviction is based on the sampling of entries. Please see the <a href="#eviction-algorithm">Eviction Algorithm section</a> for details.</em>
<br></br></p>
<a name="example-eviction-configurations"></a><h4 id="example-eviction-configurations">Example Eviction Configurations</h4>
<pre><code class="lang-xml">&lt;map name=&quot;documents&quot;&gt;
  &lt;max-size policy=&quot;PER_NODE&quot;&gt;10000&lt;/max-size&gt;
  &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt;
  &lt;max-idle-seconds&gt;60&lt;/max-idle-seconds&gt;
&lt;/map&gt;
</code></pre>
<p>In the above example, <code>documents</code> map starts to evict its entries from a member when the map size exceeds 10000 in that member. Then the entries least recently used will be evicted. The entries not used for more than 60 seconds will be evicted as well.</p>
<p>And the following is an example eviction configuration for a map having <code>NATIVE</code> as the in-memory format:</p>
<pre><code class="lang-xml">&lt;map name=&quot;nativeMap*&quot;&gt;
    &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
    &lt;eviction-policy&gt;LFU&lt;/eviction-policy&gt;
    &lt;max-size policy=&quot;USED_NATIVE_MEMORY_PERCENTAGE&quot;&gt;99&lt;/max-size&gt;
&lt;/map&gt;
</code></pre>
<a name="evicting-specific-entries"></a><h4 id="evicting-specific-entries">Evicting Specific Entries</h4>
<p>The eviction policies and configurations explained above apply to all the entries of a map. The entries that meet the specified eviction conditions are evicted.</p>
<p>You may also want to evict some specific map entries.  To do this, you can use the <code>ttl</code> and <code>timeunit</code> parameters of the method <code>map.put()</code>. An example code line is given below.</p>
<p><code>myMap.put( &quot;1&quot;, &quot;John&quot;, 50, TimeUnit.SECONDS )</code></p>
<p>The map entry with the key &quot;1&quot; will be evicted 50 seconds after it is put into <code>myMap</code>.</p>
<a name="evicting-all-entries"></a><h4 id="evicting-all-entries">Evicting All Entries</h4>
<p>To evict all keys from the map except the locked ones, use the method <code>evictAll()</code>. If a MapStore is defined for the map, <code>deleteAll</code> is not called by <code>evictAll</code>. If you want to call the method <code>deleteAll</code>, use <code>clear()</code>.</p>
<p>An example is given below.</p>
<pre><code class="lang-java">public class EvictAll {

    public static void main(String[] args) {
        final int numberOfKeysToLock = 4;
        final int numberOfEntriesToAdd = 1000;

        HazelcastInstance node1 = Hazelcast.newHazelcastInstance();
        HazelcastInstance node2 = Hazelcast.newHazelcastInstance();

        IMap&lt;Integer, Integer&gt; map = node1.getMap(EvictAll.class.getCanonicalName());
        for (int i = 0; i &lt; numberOfEntriesToAdd; i++) {
            map.put(i, i);
        }

        for (int i = 0; i &lt; numberOfKeysToLock; i++) {
            map.lock(i);
        }

        // should keep locked keys and evict all others.
        map.evictAll();

        System.out.printf(&quot;# After calling evictAll...\n&quot;);
        System.out.printf(&quot;# Expected map size\t: %d\n&quot;, numberOfKeysToLock);
        System.out.printf(&quot;# Actual map size\t: %d\n&quot;, map.size());

    }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Only EVICT_ALL event is fired for any registered listeners.</em></p>
<a name="custom-eviction-policy"></a><h4 id="custom-eviction-policy">Custom Eviction Policy</h4>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>This section is valid for Hazelcast 3.7 and higher releases.</em></p>
<p>Apart from the policies such as LRU and LFU, which Hazelcast provides out of the box, you can develop and use your own eviction policy. </p>
<p>To achieve this, you need to provide an implementation of <code>MapEvictionPolicy</code> as in the following <code>OddEvictor</code> example:</p>
<pre><code class="lang-java">public class MapCustomEvictionPolicy {

    public static void main(String[] args) {
        Config config = new Config();
        config.getMapConfig(&quot;test&quot;)
                .setMapEvictionPolicy(new OddEvictor())
                .getMaxSizeConfig()
                .setMaxSizePolicy(PER_NODE).setSize(10000);

        HazelcastInstance instance = Hazelcast.newHazelcastInstance(config);
        IMap&lt;Integer, Integer&gt; map = instance.getMap(&quot;test&quot;);

        final Queue&lt;Integer&gt; oddKeys = new ConcurrentLinkedQueue&lt;Integer&gt;();
        final Queue&lt;Integer&gt; evenKeys = new ConcurrentLinkedQueue&lt;Integer&gt;();

        map.addEntryListener(new EntryEvictedListener&lt;Integer, Integer&gt;() {
            @Override
            public void entryEvicted(EntryEvent&lt;Integer, Integer&gt; event) {
                Integer key = event.getKey();
                if (key % 2 == 0) {
                    evenKeys.add(key);
                } else {
                    oddKeys.add(key);
                }
            }
        }, false);

        // Wait some more time to receive evicted events.
        parkNanos(SECONDS.toNanos(5));

        for (int i = 0; i &lt; 15000; i++) {
            map.put(i, i);
        }

        String msg = &quot;IMap uses sampling based eviction. After eviction is completed, we are expecting &quot; +
                &quot;number of evicted-odd-keys should be greater than number of evicted-even-keys&quot; +
                &quot;\nNumber of evicted-odd-keys = %d, number of evicted-even-keys = %d&quot;;
        out.println(format(msg, oddKeys.size(), evenKeys.size()));

        instance.shutdown();
    }

    /**
     * Odd evictor tries to evict odd keys first.
     */
    private static class OddEvictor extends MapEvictionPolicy {

        @Override
        public int compare(EntryView o1, EntryView o2) {
            Integer key = (Integer) o1.getKey();
            if (key % 2 != 0) {
                return -1;
            }

            return 1;
        }
    }
}
</code></pre>
<p>Then you can enable your policy by setting it via the method <code>MapConfig#setMapEvictionPolicy</code>
programmatically or via XML declaratively. Following is the example declarative configuration for the eviction policy <code>OddEvictor</code> implemented above:</p>
<pre><code class="lang-xml">&lt;map name=&quot;test&quot;&gt;
   ...
   &lt;map-eviction-policy-class-name&gt;com.package.OddEvictor&lt;/map-eviction-policy-class-name&gt;
   ....
&lt;/map&gt;
</code></pre>
<p>If you Hazelcast with Spring, you can enable your policy as shown below.</p>
<pre><code>&lt;hz:map name=&quot;test&quot;&gt;
    &lt;hz:map-eviction-policy class-name=&quot;com.package.OddEvictor&quot;/&gt;
&lt;/hz:map&gt;
</code></pre>
<a name="setting-in-memory-format"></a><h3 id="setting-in-memory-format">Setting In-Memory Format</h3>
<p>IMap (and a few other Hazelcast data structures, such as ICache) has an <code>in-memory-format</code> configuration option. By default, Hazelcast stores data into memory in binary (serialized) format. Sometimes it can be efficient to store the entries in their object form, especially in cases of local processing, such as entry processor and queries.</p>
<p>To set how the data will be stored in memory, set <code>in-memory-format</code> in the configuration. You have the following format options:</p>
<ul>
<li><p><code>BINARY</code> (default). The data will be stored in serialized binary format. You can use this option if you mostly perform regular map operations, such as <code>put</code> and <code>get</code>.</p>
</li>
<li><p><code>OBJECT</code>. The data will be stored in deserialized form. This configuration is good for maps where entry processing and queries form the majority of all operations and the objects are complex, making the serialization cost comparatively high. By storing objects, entry processing will not contain the deserialization cost.</p>
</li>
<li><p><code>NATIVE</code>: (<font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>) This option is used to enable the map to use Hazelcast&#39;s High-Density Memory Store. Please refer to the <a href="#using-high-density-memory-store-with-map">Using High-Density Memory Store with Map section</a>.</p>
</li>
</ul>
<p>Regular operations like <code>get</code> rely on the object instance. When the <code>OBJECT</code> format is used and a <code>get</code> is performed, the map does not return the stored instance, but creates a clone. Therefore, this whole <code>get</code> operation first includes a serialization on the member owning the instance, and then a deserialization on the member calling the instance. When the <code>BINARY</code> format is used, only a deserialization is required; <code>BINARY</code> is faster.</p>
<p>Similarly, a <code>put</code> operation is faster when the <code>BINARY</code> format is used. If the format was <code>OBJECT</code>, the map would create a clone of the instance, and there would first be a serialization and then a deserialization. When BINARY is used, only a deserialization is needed.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If a value is stored in <code>OBJECT</code> format, a change on a returned value does not affect the stored instance. In this case, the returned instance is not the actual one but a clone. Therefore, changes made on an object after it is returned will not reflect on the actual stored data. Similarly, when a value is written to a map and the value is stored in <code>OBJECT</code> format, it will be a copy of the <code>put</code> value. Therefore, changes made on the object after it is stored will not reflect on the stored data.</em></p>

<a name="using-high-density-memory-store-with-map"></a><h3 id="using-high-density-memory-store-with-map">Using High-Density Memory Store with Map</h3>
<p><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>
<br></br></p>
<p>Hazelcast instances are Java programs. In case of <code>BINARY</code> and <code>OBJECT</code> in-memory formats, Hazelcast stores your distributed data into the heap of its server instances. Java heap is subject to garbage collection (GC). In case of larger heaps, garbage collection might cause your application to pause for tens of seconds (even minutes for really large heaps), badly affecting your application performance and response times.</p>
<p>As the data gets bigger, you either run the application with larger heap, which would result in longer GC pauses or run multiple instances with smaller heap which can turn into an operational nightmare if the number of such instances becomes very high.</p>
<p>To overcome this challenge, Hazelcast offers High-Density Memory Store for your maps. You can configure your map to use High-Density Memory Store by setting the in-memory format to <code>NATIVE</code>. The following snippet is the declarative configuration example.</p>
<pre><code class="lang-xml">&lt;map name=&quot;nativeMap*&quot;&gt;
   &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
&lt;/map&gt;
</code></pre>
<p>Keep in mind that you should have already enabled the High-Density Memory Store usage for your cluster. Please see <a href="#configuring-high-density-memory-store">Configuring High-Density Memory Store section</a>.</p>
<a name="required-configuration-changes-when-using-native"></a><h4 id="required-configuration-changes-when-using-native">Required configuration changes when using NATIVE</h4>
<p>Note that the eviction mechanism is different for <code>NATIVE</code> in-memory format.
The new eviction algorithm for map with High-Density Memory Store is similar to that of JCache with High-Density Memory Store and is described <a href="#eviction-algorithm">here</a>.</p>
<ul>
<li><p>Eviction percentage has no effect.</p>
<pre><code class="lang-xml">&lt;map name=&quot;nativeMap*&quot;&gt;
  &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
  &lt;eviction-percentage&gt;25&lt;/eviction-percentage&gt; &lt;-- NO IMPACT with NATIVE
&lt;/map&gt;
</code></pre>
</li>
<li><p>These IMap eviction policies for <code>max-size</code> cannot be used: <code>FREE_HEAP_PERCENTAGE</code>, <code>FREE_HEAP_SIZE</code>, <code>USED_HEAP_PERCENTAGE</code>, <code>USED_HEAP_SIZE</code>.</p>
</li>
<li><p>Near cache eviction configuration is also different for <code>NATIVE</code> in-memory format.</p>
<p>For a near cache configuration with in-memory format set to <code>BINARY</code>:</p>
<pre><code class="lang-xml">    &lt;map name=&quot;nativeMap*&quot;&gt;

      &lt;near-cache&gt;
        &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
        &lt;max-size&gt;10000&lt;/max-size&gt; &lt;-- NO IMPACT with NATIVE
        &lt;eviction-policy&gt;LFU&lt;/eviction-policy&gt; &lt;-- NO IMPACT with NATIVE
      &lt;/near-cache&gt;

    &lt;/map&gt;
</code></pre>
<p> the equivalent configuration for <code>NATIVE</code> in-memory format would be similar to the following:</p>
<pre><code class="lang-xml">     &lt;map name=&quot;nativeMap*&quot;&gt;

       &lt;near-cache&gt;
         &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
         &lt;eviction size=&quot;10000&quot; eviction-policy=&quot;LFU&quot; max-size-policy=&quot;USED_NATIVE_MEMORY_SIZE&quot;/&gt;   &lt;-- Correct configuration with NATIVE
       &lt;/near-cache&gt;

     &lt;/map&gt;
</code></pre>
</li>
<li><p>Near cache eviction policy <code>ENTRY_COUNT</code> cannot be used for <code>max-size-policy</code>.</p>
</li>
</ul>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#high-density-memory-store">High-Density Memory Store section</a> for more information.</em>
<br></br></p>

<a name="loading-and-storing-persistent-data"></a><h3 id="loading-and-storing-persistent-data">Loading and Storing Persistent Data</h3>
<p>Hazelcast allows you to load and store the distributed map entries from/to a persistent data store such as a relational database. To do this, you can use Hazelcast&#39;s <code>MapStore</code> and <code>MapLoader</code> interfaces.</p>
<p>When you provide a <code>MapLoader</code> implementation and request an entry (<code>IMap.get()</code>) that does not exist in memory, <code>MapLoader</code>&#39;s <code>load</code> or <code>loadAll</code> methods will load that entry from the data store. This loaded entry is placed into the map and will stay there until it is removed or evicted.</p>
<p>When a <code>MapStore</code> implementation is provided, an entry is also put into a user defined data store. </p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Data store needs to be a centralized system that is
accessible from all Hazelcast members. Persistence to a local file system is not supported.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Also note that the <code>MapStore</code> interface extends the <code>MapLoader</code> interface as you can see in the interface <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/MapStore.java">code</a>.</em></p>
<p>Following is a <code>MapStore</code> example.</p>
<pre><code class="lang-java">public class PersonMapStore implements MapStore&lt;Long, Person&gt; {
    private final Connection con;

    public PersonMapStore() {
        try {
            con = DriverManager.getConnection(&quot;jdbc:hsqldb:mydatabase&quot;, &quot;SA&quot;, &quot;&quot;);
            con.createStatement().executeUpdate(
                    &quot;create table if not exists person (id bigint, name varchar(45))&quot;);
        } catch (SQLException e) {
            throw new RuntimeException(e);
        }
    }

    public synchronized void delete(Long key) {
        System.out.println(&quot;Delete:&quot; + key);
        try {
            con.createStatement().executeUpdate(
                    format(&quot;delete from person where id = %s&quot;, key));
        } catch (SQLException e) {
            throw new RuntimeException(e);
        }
    }

    public synchronized void store(Long key, Person value) {
        try {
            con.createStatement().executeUpdate(
                    format(&quot;insert into person values(%s,&#39;%s&#39;)&quot;, key, value.name));
        } catch (SQLException e) {
            throw new RuntimeException(e);
        }
    }

    public synchronized void storeAll(Map&lt;Long, Person&gt; map) {
        for (Map.Entry&lt;Long, Person&gt; entry : map.entrySet())
            store(entry.getKey(), entry.getValue());
    }

    public synchronized void deleteAll(Collection&lt;Long&gt; keys) {
        for (Long key : keys) delete(key);
    }

    public synchronized Person load(Long key) {
        try {
            ResultSet resultSet = con.createStatement().executeQuery(
                    format(&quot;select name from person where id =%s&quot;, key));
            try {
                if (!resultSet.next()) return null;
                String name = resultSet.getString(1);
                return new Person(name);
            } finally {
                resultSet.close();
            }
        } catch (SQLException e) {
            throw new RuntimeException(e);
        }
    }

    public synchronized Map&lt;Long, Person&gt; loadAll(Collection&lt;Long&gt; keys) {
        Map&lt;Long, Person&gt; result = new HashMap&lt;Long, Person&gt;();
        for (Long key : keys) result.put(key, load(key));
        return result;
    }

    public Iterable&lt;Long&gt; loadAllKeys() {
        return null;
    }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>During the initial loading process, MapStore uses a thread different from the partition threads that are used by the ExecutorService. After the initialization is completed, the <code>map.get</code> method looks up any nonexistent value from the database in a partition thread, or the <code>map.put</code> method looks up the database to return the previously associated value for a key also in a partition thread.</em></p>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>For more MapStore/MapLoader code samples, please see <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/distributed-map/mapstore/src/main/java" target="_blank">here</a>.</em>
<br></br></p>
<p>Hazelcast supports read-through, write-through, and write-behind persistence modes, which are explained in the subsections below.</p>
<a name="using-read-through-persistence"></a><h4 id="using-read-through-persistence">Using Read-Through Persistence</h4>
<p>If an entry does not exist in memory when an application asks for it, Hazelcast asks the loader implementation to load that entry from the data store.  If the entry exists there, the loader implementation gets it, hands it to Hazelcast, and Hazelcast puts it into memory. This is read-through persistence mode.</p>
<a name="setting-write-through-persistence"></a><h4 id="setting-write-through-persistence">Setting Write-Through Persistence</h4>
<p><code>MapStore</code> can be configured to be write-through by setting the <code>write-delay-seconds</code> property to <strong>0</strong>. This means the entries will be put to the data store synchronously.</p>
<p>In this mode, when the <code>map.put(key,value)</code> call returns:</p>
<ul>
<li><code>MapStore.store(key,value)</code> is successfully called so the entry is persisted.</li>
<li>In-Memory entry is updated.</li>
<li>In-Memory backup copies are successfully created on other cluster members (if <code>backup-count</code> is greater than 0).</li>
</ul>
<p>The same behavior goes for a <code>map.remove(key)</code> call. The only difference is that  <code>MapStore.delete(key)</code> is called when the entry will be deleted.</p>
<p>If <code>MapStore</code> throws an exception, then the exception will be propagated back to the original <code>put</code> or <code>remove</code> call in the form of <code>RuntimeException</code>.</p>
<a name="setting-write-behind-persistence"></a><h4 id="setting-write-behind-persistence">Setting Write-Behind Persistence</h4>
<p>You can configure <code>MapStore</code> as write-behind by setting the <code>write-delay-seconds</code> property to a value bigger than <strong>0</strong>. This means the modified entries will be put to the data store asynchronously after a configured delay. </p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>In write-behind mode, Hazelcast coalesces updates on a specific key by default, which means it applies only the last update on that key. However, you can set <code>MapStoreConfig#setWriteCoalescing</code> to <code>FALSE</code> and you can store all updates performed on a key to the data store.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>When you set <code>MapStoreConfig#setWriteCoalescing</code> to <code>FALSE</code>, after you reached per-node maximum write-behind-queue capacity, subsequent put operations will fail with <code>ReachedMaxSizeException</code>. This exception will be thrown to prevent uncontrolled grow of write-behind queues. You can set per-node maximum capacity using the system property <code>hazelcast.map.write.behind.queue.capacity</code>. Please refer to the <a href="#system-properties">System Properties section</a> for information on this property and how to set the system properties.</em></p>
<p>In write-behind mode, when the <code>map.put(key,value)</code> call returns:</p>
<ul>
<li>In-Memory entry is updated.</li>
<li>In-Memory backup copies are successfully created on other cluster members (if <code>backup-count</code> is greater than 0).</li>
<li>The entry is marked as dirty so that after <code>write-delay-seconds</code>, it can be persisted with <code>MapStore.store(key,value)</code> call.</li>
<li>For fault tolerance, dirty entries are stored in a queue on the primary member and also on a back-up member.</li>
</ul>
<p>The same behavior goes for the <code>map.remove(key)</code>, the only difference is that  <code>MapStore.delete(key)</code> is called when the entry will be deleted.</p>
<p>If <code>MapStore</code> throws an exception, then Hazelcast tries to store the entry again. If the entry still cannot be stored, a log message is printed and the entry is re-queued. </p>
<p>For batch write operations, which are only allowed in write-behind mode, Hazelcast will call <code>MapStore.storeAll(map)</code> and <code>MapStore.deleteAll(collection)</code> to do all writes in a single call.
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If a map entry is marked as dirty, meaning that it is waiting to be persisted to the <code>MapStore</code> in a write-behind scenario, the eviction process forces the entry to be stored. This way you have control over the number of entries waiting to be stored, and thus you can prevent a possible OutOfMemory exception.</em>
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>MapStore</code> or <code>MapLoader</code> implementations should not use Hazelcast Map/Queue/MultiMap/List/Set operations. Your implementation should only work with your data store. Otherwise, you may get into deadlock situations.</em></p>
<p>Here is a sample configuration:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;map name=&quot;default&quot;&gt;
    ...
    &lt;map-store enabled=&quot;true&quot; initial-mode=&quot;LAZY&quot;&gt;
      &lt;class-name&gt;com.hazelcast.examples.DummyStore&lt;/class-name&gt;
      &lt;write-delay-seconds&gt;60&lt;/write-delay-seconds&gt;
      &lt;write-batch-size&gt;1000&lt;/write-batch-size&gt;
      &lt;write-coalescing&gt;true&lt;/write-coalescing&gt;
    &lt;/map-store&gt;
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>The following are the descriptions of MapStore configuration elements and attributes:</p>
<ul>
<li><code>class-name</code>: Name of the class implementing MapLoader and/or MapStore.</li>
<li><code>write-delay-seconds</code>: Number of seconds to delay to call the MapStore.store(key, value). If the value is zero then it is write-through so MapStore.store(key, value) will be called as soon as the entry is updated. Otherwise it is write-behind so updates will be stored after write-delay-seconds value by calling Hazelcast.storeAll(map). Default value is 0.</li>
<li><code>write-batch-size</code>: Used to create batch chunks when writing map store. In default mode, all map entries will be tried to be written in one go. To create batch chunks, the minimum meaningful value for write-batch-size is 2. For values smaller than 2, it works as in default mode.</li>
<li><code>write-coalescing</code>: In write-behind mode, Hazelcast coalesces updates on a specific key by default; it applies only the last update on it. You can set this element to <code>false</code> to store all updates performed on a key to the data store.</li>
<li><code>enabled</code>: True to enable this map-store, false to disable. Default value is true.</li>
<li><code>initial-mode</code>: Sets the initial load mode. LAZY is the default load mode, where load is asynchronous. EAGER means load is blocked till all partitions are loaded.</li>
</ul>
<a name="storing-entries-to-multiple-maps"></a><h4 id="storing-entries-to-multiple-maps">Storing Entries to Multiple Maps</h4>
<p>A configuration can be applied to more than one map using wildcards (see <a href="#using-wildcards">Using Wildcards</a>), meaning that the configuration is shared among the maps. But <code>MapStore</code> does not know which entries to store when there is one configuration applied to multiple maps.</p>
<p>To store entries when there is one configuration applied to multiple maps, use Hazelcast&#39;s <code>MapStoreFactory</code> interface. Using the <code>MapStoreFactory</code> interface, <code>MapStore</code>s for each map can be created when a wildcard configuration is used. Example code is shown below.</p>
<pre><code class="lang-java">Config config = new Config();
MapConfig mapConfig = config.getMapConfig( &quot;*&quot; );
MapStoreConfig mapStoreConfig = mapConfig.getMapStoreConfig();
mapStoreConfig.setFactoryImplementation( new MapStoreFactory&lt;Object, Object&gt;() {
  @Override
  public MapLoader&lt;Object, Object&gt; newMapStore( String mapName, Properties properties ) {
    return null;
  }
});
</code></pre>
<p>To initialize the <code>MapLoader</code> implementation with the given map name, configuration properties, and the Hazelcast instance, implement the <code>MapLoaderLifecycleSupport</code> interface. This interface has the methods <code>init()</code> and <code>destroy()</code> as shown below.</p>
<pre><code class="lang-java">public interface MapLoaderLifecycleSupport {

  void init( HazelcastInstance hazelcastInstance, Properties properties, String mapName );

  void destroy();
}
</code></pre>
<p>The method <code>init()</code> initializes the <code>MapLoader</code> implementation. Hazelcast calls this method when the map is first used on the Hazelcast instance. The <code>MapLoader</code> implementation can initialize the required resources for implementing <code>MapLoader</code> such as reading a configuration file or creating a database connection.</p>
<p>Hazelcast calls the method <code>destroy()</code> before shutting down. You can override this method  to cleanup the resources held by this <code>MapLoader</code> implementation, such as closing the database connections.</p>
<a name="initializing-map-on-startup"></a><h4 id="initializing-map-on-startup">Initializing Map on Startup</h4>
<p>To pre-populate the in-memory map when the map is first touched/used, use the <code>MapLoader.loadAllKeys</code> API.</p>
<p>If <code>MapLoader.loadAllKeys</code> returns NULL, then nothing will be loaded. Your <code>MapLoader.loadAllKeys</code> implementation can return all or some of the keys. For example, you may select and return only the <code>hot</code> keys. <code>MapLoader.loadAllKeys</code> is the fastest way of pre-populating the map since Hazelcast will optimize the loading process by having each cluster member load its owned portion of the entries.</p>
<p>The <code>InitialLoadMode</code> configuration parameter in the class <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/config/MapStoreConfig.java" target="_blank">MapStoreConfig</a> has two values: <code>LAZY</code> and <code>EAGER</code>. If <code>InitialLoadMode</code> is set to <code>LAZY</code>, data is not loaded during the map creation. If it is set to <code>EAGER</code>, all the data is loaded while the map is created, and everything becomes ready to use. Also, if you add indices to your map with the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/config/MapIndexConfig.java" target="_blank">MapIndexConfig</a> class or the <a href="#indexing-queries"><code>addIndex</code></a> method, then <code>InitialLoadMode</code> is overridden and <code>MapStoreConfig</code> behaves as if <code>EAGER</code> mode is on.</p>
<p>Here is the <code>MapLoader</code> initialization flow:</p>
<ol>
<li>When <code>getMap()</code> is first called from any member, initialization will start depending on the value of <code>InitialLoadMode</code>. If it is set to <code>EAGER</code>, initialization starts.  If it is set to <code>LAZY</code>, initialization does not start but data is loaded each time a partition loading completes.</li>
<li>Hazelcast will call <code>MapLoader.loadAllKeys()</code> to get all your keys on one of the members.</li>
<li>That member will distribute keys to all other members in batches.</li>
<li>Each member will load values of all its owned keys by calling <code>MapLoader.loadAll(keys)</code>.</li>
<li>Each member puts its owned entries into the map by calling <code>IMap.putTransient(key,value)</code>.</li>
</ol>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If the load mode is <code>LAZY</code> and the <code>clear()</code> method is called (which triggers <code>MapStore.deleteAll()</code>), Hazelcast will remove <strong>ONLY</strong> the loaded entries from your map and datastore. Since all the data is not loaded in this case (<code>LAZY</code> mode), please note that there may still be entries in your datastore.</em>
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The return type of <code>loadAllKeys()</code> is changed from <code>Set</code> to <code>Iterable</code> with the release of Hazelcast 3.5. MapLoader implementations from previous releases are also supported and do not need to be adapted.</em></p>
<p><br></br></p>
<a name="loading-keys-incrementally"></a><h4 id="loading-keys-incrementally">Loading Keys Incrementally</h4>
<p>If the number of keys to load is large, it is more efficient to load them incrementally rather than loading them all at once. To support incremental loading, the <code>MapLoader.loadAllKeys()</code> method returns an <code>Iterable</code> which can be lazily populated with the results of a database query. </p>
<p>Hazelcast iterates over the <code>Iterable</code> and, while doing so, sends out the keys to their respective owner members. The <code>Iterator</code> obtained from <code>MapLoader.loadAllKeys()</code> may also implement the <code>Closeable</code> interface, in which case <code>Iterator</code> is closed once the iteration is over. This is intended for releasing resources such as closing a JDBC result set. </p>
<a name="forcing-all-keys-to-be-loaded"></a><h4 id="forcing-all-keys-to-be-loaded">Forcing All Keys To Be Loaded</h4>
<p>The method <code>loadAll</code> loads some or all keys into a data store in order to optimize the multiple load operations. The method has two signatures; the same method can take two different parameter lists. One signature loads the given keys and the other loads all keys. Please see the example code below.</p>
<pre><code class="lang-java">public class LoadAll {

    public static void main(String[] args) {
        final int numberOfEntriesToAdd = 1000;
        final String mapName = LoadAll.class.getCanonicalName();
        final Config config = createNewConfig(mapName);
        final HazelcastInstance node = Hazelcast.newHazelcastInstance(config);
        final IMap&lt;Integer, Integer&gt; map = node.getMap(mapName);

        populateMap(map, numberOfEntriesToAdd);
        System.out.printf(&quot;# Map store has %d elements\n&quot;, numberOfEntriesToAdd);

        map.evictAll();
        System.out.printf(&quot;# After evictAll map size\t: %d\n&quot;, map.size());

        map.loadAll(true);
        System.out.printf(&quot;# After loadAll map size\t: %d\n&quot;, map.size());
    }
}
</code></pre>
<a name="post-processing-objects-in-map-store"></a><h4 id="post-processing-objects-in-map-store">Post-Processing Objects in Map Store</h4>
<p>In some scenarios, you may need to modify the object after storing it into the map store.
For example, you can get an ID or version auto-generated by your database and then need to modify your object stored in the distributed map, but not to break the synchronization between the database and the data grid. </p>
<p>To post-process an object in the map store, implement the <code>PostProcessingMapStore</code> interface to put the modified object into the distributed map. This will trigger an extra step of <code>Serialization</code>, so use it only when needed. (This is only valid when using the <code>write-through</code> map store configuration.)</p>
<p>Here is an example of post processing map store:</p>
<pre><code class="lang-java">class ProcessingStore implements MapStore&lt;Integer, Employee&gt;, PostProcessingMapStore {
  @Override
  public void store( Integer key, Employee employee ) {
    EmployeeId id = saveEmployee();
    employee.setId( id.getId() );
  }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Please note that if you are using a post processing map store in combination with entry processors, post-processed values will not be carried to backups.</em></p>

<a name="creating-near-cache-for-map"></a><h3 id="creating-near-cache-for-map">Creating Near Cache for Map</h3>
<p>Map entries in Hazelcast are partitioned across the cluster. Suppose you read the key <code>k</code> a number of times and <code>k</code> is owned by another member in your cluster. Each <code>map.get(k)</code> will be a remote operation, meaning lots of network trips. If you have a map that is read-mostly, then you should consider creating a near cache for the map so that reads can be much faster and consume less network traffic. These benefits do not come free; when using near cache, you should consider the following issues:</p>
<ul>
<li>Cluster members will have to hold extra cached data, which increases memory consumption.</li>
<li>If invalidation is turned on and entries are updated frequently, then invalidations will be costly.</li>
<li>Near cache breaks the strong consistency guarantees; you might be reading stale data.</li>
</ul>
<p>Near cache is highly recommended for the maps that are read-mostly. The following is the configuration example for map&#39;s near cache in the Hazelcast configuration file.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;map name=&quot;my-read-mostly-map&quot;&gt;
    ...
    &lt;near-cache name=&quot;default&quot;&gt;
      &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
      &lt;max-size&gt;5000&lt;/max-size&gt;
      &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
      &lt;max-idle-seconds&gt;60&lt;/max-idle-seconds&gt;
      &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt;
      &lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;
      &lt;cache-local-entries&gt;false&lt;/cache-local-entries&gt;
    &lt;/near-cache&gt;
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>The element <code>&lt;near-cache&gt;</code> has an optional attribute &quot;name&quot; whose default value is <code>default</code>. Following are the descriptions of all configuration elements:</p>
<ul>
<li><code>&lt;max-size&gt;</code>: Maximum size of the near cache. When this is reached, near cache is evicted based on the policy defined. Any integer between 0 and Integer.MAX_VALUE. 0 means <code>Integer.MAX_VALUE</code>. Its default value is 0.</li>
<li><code>&lt;time-to-live-seconds&gt;</code>: Maximum number of seconds for each entry to stay in the near cache. Entries that are older than this period are automatically evicted from the near cache. Regardless of the eviction policy used, <code>&lt;time-to-live-seconds&gt;</code> still applies. Any integer between 0 and <code>Integer.MAX_VALUE</code>. 0 means infinite. Its default value is 0.</li>
<li><code>&lt;max-idle-seconds&gt;</code>: Maximum number of seconds each entry can stay in the near cache as untouched (not read). Entries that are not read more than this period are removed from the near cache. Any integer between 0 and <code>Integer.MAX_VALUE</code>. 0 means <code>Integer.MAX_VALUE</code>. Its default value is 0.</li>
<li><code>&lt;eviction-policy&gt;</code>: Eviction policy configuration. Its default values is NONE. Available values are as follows:<ul>
<li>NONE: No items will be evicted and the property max-size will be ignored. You still can combine it with time-to-live-seconds and max-idle-seconds.</li>
<li>LRU:     Least Recently Used.</li>
<li>LFU:     Least Frequently Used.</li>
</ul>
</li>
<li><code>&lt;invalidate-on-change&gt;</code>: Specifies whether the cached entries are evicted when the entries are updated or removed. Its default value is true.</li>
<li><code>&lt;in-memory-format&gt;</code>: Specifies in which format data will be stored in your near cache. Note that a map&#39;s in-memory format can be different from that of its near cache. Available values are as follows:<ul>
<li>BINARY: Data will be stored in serialized binary format. It is the default option.</li>
<li>OBJECT: Data will be stored in deserialized form.</li>
<li>NATIVE: Data will be stored in the near cache that uses Hazelcast&#39;s High-Density Memory Store feature. This option is available only in Hazelcast Enterprise HD. Note that a map and its near cache can independently use High-Density Memory Store. For example, while your map does not use High-Density Memory Store, its near cache can use it.</li>
</ul>
</li>
<li><code>&lt;cache-local-entries&gt;</code>: Specifies whether the local entries will be cached. It can be useful when in-memory format for near cache is different from that of the map. By default, it is disabled.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you use High-Density Memory Store for your near cache, the elements <code>&lt;max-size&gt;</code> and <code>&lt;eviction-policy&gt;</code> do not have any impact. In this case, you need to use the element <code>&lt;eviction&gt;</code> to specify the eviction behavior. Please refer to the <a href="#using-high-density-memory-store-with-near-cache">Using High-Density Memory Store with Near Cache section</a>.</em>
<br></br></p>
<p>Programmatically, you configure near cache by using the class <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/config/NearCacheConfig.java" target="_blank">NearCacheConfig</a>. This class is used both in the cluster members and clients. In a client/server system, you must enable the near cache separately on the client, without you needing to configure it on the member. For information on how to create a near cache on a client (native Java client), please see <a href="#configuring-client-near-cache">Configuring Client Near Cache</a>. Please note that near cache configuration is specific to the member or client itself, a map in a member may not have near cache configured while the same map in a client may have near cache configured.</p>
<p>If you are using near cache, you should take into account that your hits to the keys in near cache are not reflected as hits to the original keys on the primary members; this has an impact on IMap&#39;s maximum idle seconds or time-to-live seconds expiration. Therefore, even though there is a hit on a key in near cache, your original key on the primary member may expire.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Near cache works only when you access data via <code>map.get(k)</code> methods.  Data returned using a predicate is not stored in the near cache.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Even though lite members do not store any data for Hazelcast data structures, you can enable near cache on lite members for faster reads.</em></p>
<a name="near-cache-invalidation"></a><h4 id="near-cache-invalidation">Near Cache Invalidation</h4>
<p>When you enable invalidations on near cache, either programmatically via <code>NearCacheConfig#setInvalidateOnChange</code> or declaratively via <code>&lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;</code>, when entires are updated or removed from an entry in the underlying IMap, corresponding entries are removed from near caches to prevent stale reads.
This is called near cache invalidation. </p>
<p>Invalidation can be sent from members to client near caches or to member near caches, either individually or in batches.
Default behavior is sending in batches. If there are lots of mutating operations such as put/remove on IMap, it is advised that you make invalidations in batches.
This reduces the network traffic and keeps the eventing system less busy.</p>
<p>You can use the following system properties to configure the near cache invalidation:</p>
<ul>
<li><code>hazelcast.map.invalidation.batch.enabled</code>: Enable or disable batching. Default value is true. When it is set to false, all invalidations are sent immediately.</li>
<li><code>hazelcast.map.invalidation.batch.size</code>: Maximum number of invalidations in a batch. Default value is 100.</li>
<li><code>hazelcast.map.invalidation.batchfrequency.seconds</code>: If we cannot reach the configured batch size, a background process sends invalidations periodically. Default value is 10 seconds.</li>
</ul>
<p>If there are a lot of clients or many mutating operations, batching should remain enabled and the batch size should be configured with the <code>hazelcast.map.invalidation.batch.size</code> system property to a suitable value.</p>

<a name="using-high-density-memory-store-with-near-cache"></a><h3 id="using-high-density-memory-store-with-near-cache">Using High-Density Memory Store with Near Cache</h3>
<p><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>
<br></br></p>
<p>Hazelcast offers High-Density Memory Store for the near caches in your maps. You can enable your near cache to use the High-Density Memory Store by setting the in-memory format to <code>NATIVE</code>. The following snippet is the declarative configuration example.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;map name=&quot;my-read-mostly-map&quot;&gt;
    ...
    &lt;near-cache&gt;
       ...
       &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
       &lt;eviction size=&quot;1000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LFU&quot;/&gt;
       ...
    &lt;/near-cache&gt;
    ...  
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>The element <code>&lt;eviction&gt;</code> is used to specify the eviction behavior when you use High-Density Memory Store for your near cache. It has the following attributes:</p>
<ul>
<li><code>size</code>: Maximum size (entry count) of the near cache.</li>
<li><code>max-size-policy</code>: Maximum size policy for eviction of the near cache. Available values are as follows:<ul>
<li>ENTRY_COUNT: Maximum entry count per member.</li>
<li>USED_NATIVE_MEMORY_SIZE: Maximum used native memory size in megabytes.</li>
<li>USED_NATIVE_MEMORY_PERCENTAGE: Maximum used native memory percentage.</li>
<li>FREE_NATIVE_MEMORY_SIZE: Minimum free native memory size to trigger cleanup.</li>
<li>FREE_NATIVE_MEMORY_PERCENTAGE: Minimum free native memory percentage to trigger cleanup.</li>
</ul>
</li>
<li><code>eviction-policy</code>: Eviction policy configuration. Its default values is NONE. Available values are as follows:<ul>
<li>NONE: No items will be evicted and the property max-size will be ignored. You still can combine it with time-to-live-seconds and max-idle-seconds.</li>
<li>LRU:     Least Recently Used.</li>
<li>LFU:     Least Frequently Used.</li>
</ul>
</li>
</ul>
<p>Keep in mind that you should have already enabled the High-Density Memory Store usage for your cluster. Please see the <a href="#configuring-high-density-memory-store">Configuring High-Density Memory Store section</a>.</p>
<p>Note that a map and its near cache can independently use High-Density Memory Store. For example, if your map does not use High-Density Memory Store, its near cache can still use it.</p>

<a name="locking-maps"></a><h3 id="locking-maps">Locking Maps</h3>
<p>Hazelcast Distributed Map (IMap) is thread-safe to meet your thread safety requirements. When these requirements increase or you want to have more control on the concurrency, consider the Hazelcast solutions described here.</p>
<p>Let&#39;s work on a sample case as shown below.</p>
<pre><code class="lang-java">public class RacyUpdateMember {
    public static void main( String[] args ) throws Exception {
        HazelcastInstance hz = Hazelcast.newHazelcastInstance();
        IMap&lt;String, Value&gt; map = hz.getMap( &quot;map&quot; );
        String key = &quot;1&quot;;
        map.put( key, new Value() );
        System.out.println( &quot;Starting&quot; );
        for ( int k = 0; k &lt; 1000; k++ ) {
            if ( k % 100 == 0 ) System.out.println( &quot;At: &quot; + k );
            Value value = map.get( key );
            Thread.sleep( 10 );
            value.amount++;
            map.put( key, value );
        }
        System.out.println( &quot;Finished! Result = &quot; + map.get(key).amount );
    }

    static class Value implements Serializable {
        public int amount;
    }
}
</code></pre>
<p>If the above code is run by more than one cluster member simultaneously, a race condition is likely. You can solve this condition with Hazelcast using either pessimistic locking or optimistic locking. </p>
<a name="pessimistic-locking"></a><h4 id="pessimistic-locking">Pessimistic Locking</h4>
<p>One way to solve the race issue is by using pessimistic locking--lock the map entry until you are finished with it.</p>
<p>To perform pessimistic locking, use the lock mechanism provided by the Hazelcast distributed map, i.e., the <code>map.lock</code> and <code>map.unlock</code> methods. See the below example code.</p>
<pre><code class="lang-java">public class PessimisticUpdateMember {
    public static void main( String[] args ) throws Exception {
        HazelcastInstance hz = Hazelcast.newHazelcastInstance();
        IMap&lt;String, Value&gt; map = hz.getMap( &quot;map&quot; );
        String key = &quot;1&quot;;
        map.put( key, new Value() );
        System.out.println( &quot;Starting&quot; );
        for ( int k = 0; k &lt; 1000; k++ ) {
            map.lock( key );
            try {
                Value value = map.get( key );
                Thread.sleep( 10 );
                value.amount++;
                map.put( key, value );
            } finally {
                map.unlock( key );
            }
        }
        System.out.println( &quot;Finished! Result = &quot; + map.get( key ).amount );
    }

    static class Value implements Serializable {
        public int amount;
    }
}
</code></pre>
<p>The IMap lock will automatically be collected by the garbage collector when the lock is released and no other waiting conditions exist on the lock.</p>
<p>The IMap lock is reentrant, but it does not support fairness.</p>
<p>Another way to solve the race issue is by acquiring a predictable <code>Lock</code> object from Hazelcast. This way, every value in the map can be given a lock, or you can create a stripe of locks.</p>
<a name="optimistic-locking"></a><h4 id="optimistic-locking">Optimistic Locking</h4>
<p>In Hazelcast, you can apply the optimistic locking strategy with the map&#39;s <code>replace</code> method. This method compares values in object or data forms depending on the in-memory format configuration. If the values are equal, it replaces the old value with the new one. If you want to use your defined <code>equals</code> method, <code>in-memory-format</code> should be <code>OBJECT</code>. Otherwise, Hazelcast serializes objects to <code>BINARY</code> forms and compares them.</p>
<p>See the below example code. </p>
<pre><code class="lang-java">public class OptimisticMember {
    public static void main( String[] args ) throws Exception {
        HazelcastInstance hz = Hazelcast.newHazelcastInstance();
        IMap&lt;String, Value&gt; map = hz.getMap( &quot;map&quot; );
        String key = &quot;1&quot;;
        map.put( key, new Value() );
        System.out.println( &quot;Starting&quot; );
        for ( int k = 0; k &lt; 1000; k++ ) {
            if ( k % 10 == 0 ) System.out.println( &quot;At: &quot; + k );
            for (; ; ) {
                Value oldValue = map.get( key );
                Value newValue = new Value( oldValue );
                Thread.sleep( 10 );
                newValue.amount++;
                if ( map.replace( key, oldValue, newValue ) )
                    break;
            }
        }
        System.out.println( &quot;Finished! Result = &quot; + map.get( key ).amount );
    }

    static class Value implements Serializable {
        public int amount;

        public Value() {
        }

        public Value( Value that ) {
            this.amount = that.amount;
        }

        public boolean equals( Object o ) {
            if ( o == this ) return true;
            if ( !( o instanceof Value ) ) return false;
            Value that = ( Value ) o;
            return that.amount == this.amount;
        }
    }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The above example code is intentionally broken.</em></p>
<a name="pessimistic-vs-optimistic-locking"></a><h4 id="pessimistic-vs-optimistic-locking">Pessimistic vs. Optimistic Locking</h4>
<p>The locking strategy you choose will depend on your locking requirements.</p>
<p>Optimistic locking is better for mostly read-only systems. It has a performance boost over pessimistic locking.</p>
<p>Pessimistic locking is good if there are lots of updates on the same key. It is more robust than optimistic locking from the perspective of data consistency.</p>
<p>In Hazelcast, use <code>IExecutorService</code> to submit a task to a key owner, or to a member or members. This is the recommended way to perform task executions, rather than using pessimistic or optimistic locking techniques. <code>IExecutorService</code> will have fewer network hops and less data over wire, and tasks will be executed very near to the data. Please refer to the <a href="#data-affinity">Data Affinity section</a>.</p>
<a name="solving-the-aba-problem"></a><h4 id="solving-the-aba-problem">Solving the ABA Problem</h4>
<p>The ABA problem occurs in environments when a shared resource is open to change by multiple threads. Even if one thread sees the same value for a particular key in consecutive reads, it does not mean that nothing has changed between the reads. Another thread may change the value, do work, and change the value back, while the first thread thinks that nothing has changed.</p>
<p>To prevent these kind of problems, you can assign a version number and check it before any write to be sure that nothing has changed between consecutive reads. Although all the other fields will be equal, the version field will prevent objects from being seen as equal. This is the optimistic locking strategy, and it is used in environments that do not expect intensive concurrent changes on a specific key.</p>
<p>In Hazelcast, you can apply the <a href="#optimistic-locking">optimistic locking</a> strategy with the map <code>replace</code> method.</p>

<a name="accessing-entry-statistics"></a><h3 id="accessing-entry-statistics">Accessing Entry Statistics</h3>
<p>Hazelcast keeps statistics about each map entry, such as creation time, last update time, last access time, number of hits, and version. To access the map entry statistics, use an <code>IMap.getEntryView(key)</code> call. Here is an example.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.EntryView;

HazelcastInstance hz = Hazelcast.newHazelcastInstance();
EntryView entry = hz.getMap( &quot;quotes&quot; ).getEntryView( &quot;1&quot; );
System.out.println ( &quot;size in memory  : &quot; + entry.getCost() );
System.out.println ( &quot;creationTime    : &quot; + entry.getCreationTime() );
System.out.println ( &quot;expirationTime  : &quot; + entry.getExpirationTime() );
System.out.println ( &quot;number of hits  : &quot; + entry.getHits() );
System.out.println ( &quot;lastAccessedTime: &quot; + entry.getLastAccessTime() );
System.out.println ( &quot;lastUpdateTime  : &quot; + entry.getLastUpdateTime() );
System.out.println ( &quot;version         : &quot; + entry.getVersion() );
System.out.println ( &quot;key             : &quot; + entry.getKey() );
System.out.println ( &quot;value           : &quot; + entry.getValue() );
</code></pre>

<a name="map-listener"></a><h3 id="map-listener">Map Listener</h3>
<p>Please refer to the <a href="#listening-for-map-events">Listening for Map Events section</a>.</p>

<a name="listening-to-map-entries-with-predicates"></a><h3 id="listening-to-map-entries-with-predicates">Listening to Map Entries with Predicates</h3>
<p>You can listen to the modifications performed on specific map entries. You can think of it as an entry listener with predicates. Please see the <a href="#listening-for-map-events">Listening for Map Events section</a> for information on how to add entry listeners to a map.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>IMPORTANT:</em></strong> <em>The default backwards-compatible event publishing strategy only publishes
<code>UPDATED</code> events when map entries are updated to a value that matches the predicate with which the listener was registered.
This implies that when using the default event publishing strategy, your listener will not be notified about an entry whose
value is updated from one that matches the predicate to a new value that does not match the predicate.</em></p>
<p>Since version 3.7, when you configure Hazelcast members with property <code>hazelcast.map.entry.filtering.natural.event.types</code> set to <code>true</code>,
handling of entry updates conceptually treats value transition as entry, update or exit with regards to the predicate value space.
The following table compares how a listener is notified about an update to a map entry value under the default
backwards-compatible Hazelcast behavior (when property <code>hazelcast.map.entry.filtering.natural.event.types</code> is not set or is set
to <code>false</code>) versus when set to <code>true</code>:</p>
<table>
<thead>
<tr>
<th style="text-align:left">&nbsp;</th>
<th style="text-align:left">Default</th>
<th style="text-align:left"><code>hazelcast.map.entry.filtering.natural.event.types = true</code></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">When old value matches predicate,<br/>new value does not match predicate</td>
<td style="text-align:left">No event is delivered to entry listener</td>
<td style="text-align:left"><code>REMOVED</code> event is delivered to entry listener</td>
</tr>
<tr>
<td style="text-align:left">When old value matches predicate,<br/>new value matches predicate</td>
<td style="text-align:left"><code>UPDATED</code> event is delivered to entry listener</td>
<td style="text-align:left"><code>UPDATED</code> event is delivered to entry listener</td>
</tr>
<tr>
<td style="text-align:left">When old value does not match predicate,<br/>new value does not match predicate</td>
<td style="text-align:left">No event is delivered to entry listener</td>
<td style="text-align:left">No event is delivered to entry listener</td>
</tr>
<tr>
<td style="text-align:left">When old value does not match predicate,<br/>new value matches predicate</td>
<td style="text-align:left"><code>UPDATED</code> event is delivered to entry listener</td>
<td style="text-align:left"><code>ADDED</code> event is delivered to entry listener  </td>
</tr>
</tbody>
</table>
<p>As an example, let&#39;s listen to the changes made on an employee with the surname &quot;Smith&quot;. First, let&#39;s create the <code>Employee</code> class.</p>
<pre><code class="lang-java">import java.io.Serializable;

public class Employee implements Serializable {

    private final String surname;

    public Employee(String surname) {
        this.surname = surname;
    }

    @Override
    public String toString() {
        return &quot;Employee{&quot; +
                &quot;surname=&#39;&quot; + surname + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }
}
</code></pre>
<p>Then, let&#39;s create a listener with predicate by adding a listener that tracks <code>ADDED</code>, <code>UPDATED</code> and <code>REMOVED</code> entry events with the <code>surname</code> predicate.</p>
<pre><code class="lang-java">import com.hazelcast.core.*;
import com.hazelcast.query.SqlPredicate;

public class ListenerWithPredicate {

    public static void main(String[] args) {
        Config config = new Config();
        config.setProperty(&quot;hazelcast.map.entry.filtering.natural.event.types&quot;, &quot;true&quot;);
        HazelcastInstance hz = Hazelcast.newHazelcastInstance(config);
        IMap&lt;String, String&gt; map = hz.getMap(&quot;map&quot;);
        map.addEntryListener(new MyEntryListener(),
                new SqlPredicate(&quot;surname=smith&quot;), true);
        System.out.println(&quot;Entry Listener registered&quot;);
    }

    static class MyEntryListener
            implements EntryAddedListener&lt;String, String&gt;,
                       EntryUpdatedListener&lt;String, String&gt;,
                       EntryRemovedListener&lt;String, String&gt; {
        @Override
        public void entryAdded(EntryEvent&lt;String, String&gt; event) {
            System.out.println(&quot;Entry Added:&quot; + event);
        }

        @Override
        public void entryRemoved(EntryEvent&lt;String, String&gt; event); {
            System.out.println(&quot;Entry Removed:&quot; + event);
        }

        @Override
        public void entryUpdated(EntryEvent&lt;String, String&gt; event) {
            System.out.println(&quot;Entry Updated:&quot; + event);
        }

    }
}
</code></pre>
<p>And now, let&#39;s play with the employee &quot;smith&quot; and see how that employee will be listened to.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IMap;

public class Modify {

    public static void main(String[] args) {
        Config config = new Config();
        config.setProperty(&quot;hazelcast.map.entry.filtering.natural.event.types&quot;, &quot;true&quot;);
        HazelcastInstance hz = Hazelcast.newHazelcastInstance(config);
        IMap&lt;String, Employee&gt; map = hz.getMap(&quot;map&quot;);

        map.put(&quot;1&quot;, new Employee(&quot;smith&quot;));
        map.put(&quot;2&quot;, new Employee(&quot;jordan&quot;));
        System.out.println(&quot;done&quot;);
        System.exit(0);
    }
}
</code></pre>
<p>When you first run the class <code>ListenerWithPredicate</code> and then run <code>Modify</code>, you will see output similar to the listing below.</p>
<pre><code>entryAdded:EntryEvent {Address[192.168.178.10]:5702} key=1,oldValue=null,
value=Person{name= smith }, event=ADDED, by Member [192.168.178.10]:5702
</code></pre><p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to <a href="#continuous-query-cache">Continuous Query Cache</a> for more information.</em></p>

<a name="adding-interceptors"></a><h3 id="adding-interceptors">Adding Interceptors</h3>
<p>You can add intercept operations and execute your own business logic synchronously blocking the operations. You can change the returned value from a <code>get</code> operation, change the value in <code>put</code>, or <code>cancel</code> operations by throwing an exception.</p>
<p>Interceptors are different from listeners. With listeners, you take an action after the operation has been completed. Interceptor actions are synchronous and you can alter the behavior of operation, change its values, or totally cancel it.</p>
<p>Map interceptors are chained, so adding the same interceptor multiple times to the same map can result in duplicate effects. This can easily happen when the interceptor is added to the map at member initialization, so that each member adds the same interceptor. When you add the interceptor in this way, be sure to implement the <code>hashCode()</code> method to return the same value for every instance of the interceptor. It is not strictly necessary, but it is a good idea to also implement <code>equals()</code> as this will ensure that the map interceptor can be removed reliably.</p>
<p>The IMap API has two methods for adding and removing an interceptor to the map: <code>addInterceptor</code> and <code>removeInterceptor</code>.</p>
<pre><code class="lang-java">/**
 * Adds an interceptor for the map. Added interceptor intercepts operations
 * and executes user defined methods and cancels operations if 
 * user defined methods throw exceptions. 
 *
 * @param interceptor map interceptor.
 * @return id of registered interceptor.
 */
String addInterceptor( MapInterceptor interceptor );

/**
 * Removes the given interceptor for this map. So it does not 
 * intercept operations anymore. 
 *
 * @param id registration ID of the map interceptor.
 */
void removeInterceptor( String id );
</code></pre>
<p>Here is the <code>MapInterceptor</code> interface:</p>
<pre><code class="lang-java">public interface MapInterceptor extends Serializable {

  /**
   * Intercept the get operation before it returns a value.
   * Return another object to change the return value of get().
   * Returning null causes the get() operation to return the original value,
   * namely return null if you do not want to change anything.
   * 
   *
   * @param value the original value to be returned as the result of get() operation.
   * @return the new value that is returned by get() operation.
   */
  Object interceptGet( Object value );

  /**
   * Called after get() operation is completed.
   * 
   *
   * @param value the value returned as the result of get() operation.
   */
  void afterGet( Object value );

  /**
   * Intercept put operation before modifying map data.
   * Return the object to be put into the map.
   * Returning null causes the put() operation to operate as expected,
   * namely no interception. Throwing an exception cancels the put operation.
   * 
   *
   * @param oldValue the value currently existing in the map.
   * @param newValue the new value to be put.
   * @return new value after intercept operation.
   */
  Object interceptPut( Object oldValue, Object newValue );

  /**
   * Called after put() operation is completed.
   * 
   *
   * @param value the value returned as the result of put() operation.
   */
  void afterPut( Object value );

  /**
   * Intercept remove operation before removing the data.
   * Return the object to be returned as the result of remove operation.
   * Throwing an exception cancels the remove operation.
   * 
   *
   * @param removedValue the existing value to be removed.
   * @return the value to be returned as the result of remove operation.
   */
  Object interceptRemove( Object removedValue );

  /**
   * Called after remove() operation is completed.
   * 
   *
   * @param value the value returned as the result of remove(.) operation
   */
  void afterRemove( Object value );
}
</code></pre>
<p><strong>Example Usage:</strong></p>
<pre><code class="lang-java">public class InterceptorTest {

  @Test
  public void testMapInterceptor() throws InterruptedException {
    HazelcastInstance hazelcastInstance1 = Hazelcast.newHazelcastInstance();
    HazelcastInstance hazelcastInstance2 = Hazelcast.newHazelcastInstance();
    IMap&lt;Object, Object&gt; map = hazelcastInstance1.getMap( &quot;testMapInterceptor&quot; );
    SimpleInterceptor interceptor = new SimpleInterceptor();
    map.addInterceptor( interceptor );
    map.put( 1, &quot;New York&quot; );
    map.put( 2, &quot;Istanbul&quot; );
    map.put( 3, &quot;Tokyo&quot; );
    map.put( 4, &quot;London&quot; );
    map.put( 5, &quot;Paris&quot; );
    map.put( 6, &quot;Cairo&quot; );
    map.put( 7, &quot;Hong Kong&quot; );

    try {
      map.remove( 1 );
    } catch ( Exception ignore ) {
    }
    try {
      map.remove( 2 );
    } catch ( Exception ignore ) {
    }

    assertEquals( map.size(), 6) ;

    assertEquals( map.get( 1 ), null );
    assertEquals( map.get( 2 ), &quot;ISTANBUL:&quot; );
    assertEquals( map.get( 3 ), &quot;TOKYO:&quot; );
    assertEquals( map.get( 4 ), &quot;LONDON:&quot; );
    assertEquals( map.get( 5 ), &quot;PARIS:&quot; );
    assertEquals( map.get( 6 ), &quot;CAIRO:&quot; );
    assertEquals( map.get( 7 ), &quot;HONG KONG:&quot; );

    map.removeInterceptor( interceptor );
    map.put( 8, &quot;Moscow&quot; );

    assertEquals( map.get( 8 ), &quot;Moscow&quot; );
    assertEquals( map.get( 1 ), null );
    assertEquals( map.get( 2 ), &quot;ISTANBUL&quot; );
    assertEquals( map.get( 3 ), &quot;TOKYO&quot; );
    assertEquals( map.get( 4 ), &quot;LONDON&quot; );
    assertEquals( map.get( 5 ), &quot;PARIS&quot; );
    assertEquals( map.get( 6 ), &quot;CAIRO&quot; );
    assertEquals( map.get( 7 ), &quot;HONG KONG&quot; );
  }

  static class SimpleInterceptor implements MapInterceptor, Serializable {

    @Override
    public Object interceptGet( Object value ) {
      if (value == null)
        return null;
      return value + &quot;:&quot;;
    }

    @Override
    public void afterGet( Object value ) {
    }

    @Override
    public Object interceptPut( Object oldValue, Object newValue ) {
      return newValue.toString().toUpperCase();
    }

    @Override
    public void afterPut( Object value ) {
    }

    @Override
    public Object interceptRemove( Object removedValue ) {
      if(removedValue.equals( &quot;ISTANBUL&quot; ))
        throw new RuntimeException( &quot;you can not remove this&quot; );
      return removedValue;
    }

    @Override
    public void afterRemove( Object value ) {
      // do something
    }
  }
}
</code></pre>

<a name="preventing-out-of-memory-exceptions"></a><h3 id="preventing-out-of-memory-exceptions">Preventing Out of Memory Exceptions</h3>
<p>It is very easy to trigger an out of memory exception (OOME) with query-based map methods, especially with large clusters or heap sizes. For example, on a cluster with five members having 10 GB of data and 25 GB heap size per member, a single call of <code>IMap.entrySet()</code> fetches 50 GB of data and crashes the calling instance.</p>
<p>A call of <code>IMap.values()</code> may return too much data for a single member. This can also happen with a real query and an unlucky choice of predicates, especially when the parameters are chosen by a user of your application.</p>
<p>To prevent this, you can configure a maximum result size limit for query based operations. This is not a limit like <code>SELECT * FROM map LIMIT 100</code>, which you can achieve by a <a href="#filtering-with-paging-predicates">Paging Predicate</a>. A maximum result size limit for query based operations is meant to be a last line of defense to prevent your members from retrieving more data than they can handle.</p>
<p>The Hazelcast component which calculates this limit is the <code>QueryResultSizeLimiter</code>.</p>
<a name="setting-query-result-size-limit"></a><h4 id="setting-query-result-size-limit">Setting Query Result Size Limit</h4>
<p>If the <code>QueryResultSizeLimiter</code> is activated, it calculates a result size limit per partition. Each <code>QueryOperation</code> runs on all partitions of a member, so it collects result entries as long as the member limit is not exceeded. If that happens, a <code>QueryResultSizeExceededException</code> is thrown and propagated to the calling instance.</p>
<p>This feature depends on an equal distribution of the data on the cluster members to calculate the result size limit per member. Therefore, there is a minimum value defined in <code>QueryResultSizeLimiter.MINIMUM_MAX_RESULT_LIMIT</code>. Configured values below the minimum will be increased to the minimum.</p>
<a name="local-pre-check"></a><h5 id="local-pre-check">Local Pre-check</h5>
<p>In addition to the distributed result size check in the <code>QueryOperations</code>, there is a local pre-check on the calling instance. If you call the method from a client, the pre-check is executed on the member that invokes the <code>QueryOperations</code>.</p>
<p>Since the local pre-check can increase the latency of a <code>QueryOperation</code>, you can configure how many local partitions should be considered for the pre-check, or you can deactivate the feature completely.</p>
<a name="scope-of-result-size-limit"></a><h5 id="scope-of-result-size-limit">Scope of Result Size Limit</h5>
<p>Besides the designated query operations, there are other operations that use predicates internally. Those method calls will throw the <code>QueryResultSizeExceededException</code> as well. Please see the following matrix to see the methods that are covered by the query result size limit.</p>
<p><img src="images/Map-QueryResultSizeLimiterScope.png" alt="Methods Covered by Query Result Size Limit"></p>
<a name="configuring-query-result-size"></a><h5 id="configuring-query-result-size">Configuring Query Result Size</h5>
<p>The query result size limit is configured via the following system properties.</p>
<ul>
<li><code>hazelcast.query.result.size.limit</code>: Result size limit for query operations on maps. This value defines the maximum number of returned elements for a single query result. If a query exceeds this number of elements, a QueryResultSizeExceededException is thrown.</li>
<li><code>hazelcast.query.max.local.partition.limit.for.precheck</code>: Maximum value of local partitions to trigger local pre-check for TruePredicate query operations on maps.</li>
</ul>
<p>Please refer to the <a href="#system-properties">System Properties section</a> to see the full descriptions of these properties and how to set them.</p>

<a name="queue"></a><h2 id="queue">Queue</h2>
<p>Hazelcast distributed queue is an implementation of <code>java.util.concurrent.BlockingQueue</code>. Being distributed, Hazelcast distributed queue enables all cluster members to interact with it. Using Hazelcast distributed queue, you can add an item in one cluster member and remove it from another one.</p>
<a name="getting-a-queue-and-putting-items"></a><h3 id="getting-a-queue-and-putting-items">Getting a Queue and Putting Items</h3>
<p>Use the Hazelcast instance&#39;s <code>getQueue</code> method to get the queue, then use the queue&#39;s <code>put</code> method to put items into the queue.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.TimeUnit;

public class SampleQueue {

  public static void main(String[] args) throws Exception {

   HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
   BlockingQueue&lt;MyTask&gt; queue = hazelcastInstance.getQueue( &quot;tasks&quot; );
   queue.put( new MyTask() );
   MyTask task = queue.take();

   boolean offered = queue.offer( new MyTask(), 10, TimeUnit.SECONDS );
   task = queue.poll( 5, TimeUnit.SECONDS );
   if ( task != null ) {
     //process task
   }
  }
}
</code></pre>
<p>FIFO ordering will apply to all queue operations across the cluster. The user objects (such as <code>MyTask</code> in the example above) that are enqueued or dequeued have to be <code>Serializable</code>.</p>
<p>Hazelcast distributed queue performs no batching while iterating over the queue. All items will be copied locally and iteration will occur locally.</p>
<p>Hazelcast distributed queue uses <code>ItemListener</code> to listen to the events that occur when items are added to and removed from the queue. Please refer to the <a href="#listening-for-item-events">Listening for Item Events section</a> for information on how to create an item listener class and register it.</p>

<a name="creating-an-example-queue"></a><h3 id="creating-an-example-queue">Creating an Example Queue</h3>
<p>The following example code illustrates a distributed queue that connects a producer and consumer.</p>
<a name="putting-items-on-the-queue"></a><h4 id="putting-items-on-the-queue">Putting Items on the Queue</h4>
<p>Let&#39;s <code>put</code> one integer on the queue every second, 100 integers total.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IQueue;

public class ProducerMember {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    IQueue&lt;Integer&gt; queue = hz.getQueue( &quot;queue&quot; );
    for ( int k = 1; k &lt; 100; k++ ) {
      queue.put( k );
      System.out.println( &quot;Producing: &quot; + k );
      Thread.sleep(1000);
    }
    queue.put( -1 );
    System.out.println( &quot;Producer Finished!&quot; );
  }
}
</code></pre>
<p><code>Producer</code> puts a <strong>-1</strong> on the queue to show that the <code>put</code>s are finished. </p>
<a name="taking-items-off-the-queue"></a><h4 id="taking-items-off-the-queue">Taking Items off the Queue</h4>
<p>Now, let&#39;s create a <code>Consumer</code> class to <code>take</code> a message from this queue, as shown below.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IQueue;

public class ConsumerMember {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    IQueue&lt;Integer&gt; queue = hz.getQueue( &quot;queue&quot; );
    while ( true ) {
      int item = queue.take();
      System.out.println( &quot;Consumed: &quot; + item );
      if ( item == -1 ) {
        queue.put( -1 );
        break;
      }
      Thread.sleep( 5000 );
    }
    System.out.println( &quot;Consumer Finished!&quot; );
  }
}
</code></pre>
<p>As seen in the above example code, <code>Consumer</code> waits five seconds before it consumes the next message. It stops once it receives <strong>-1</strong>. Also note that <code>Consumer</code> puts <strong>-1</strong> back on the queue before the loop is ended. </p>
<p>When you first start <code>Producer</code> and then start <code>Consumer</code>, items produced on the queue will be consumed from the same queue.</p>
<a name="balancing-the-queue-operations"></a><h4 id="balancing-the-queue-operations">Balancing the Queue Operations</h4>
<p>From the above example code, you can see that an item is produced every second and consumed every five seconds. Therefore, the consumer keeps growing. To balance the produce/consume operation, let&#39;s start another consumer. This way, consumption is distributed to these two consumers, as seen in the sample outputs below. </p>
<p>The second consumer is started. After a while, here is the first consumer output:</p>
<pre><code class="lang-plain">...
Consumed 13 
Consumed 15
Consumer 17
...
</code></pre>
<p>Here is the second consumer output:</p>
<pre><code class="lang-plain">...
Consumed 14 
Consumed 16
Consumer 18
...
</code></pre>
<p>In the case of a lot of producers and consumers for the queue, using a list of queues may solve the queue bottlenecks. In this case, be aware that the order of the messages sent to different queues is not guaranteed. Since in most cases strict ordering is not important, a list of queues is a good solution.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The items are taken from the queue in the same order they were put on the queue. However, if there is more than one consumer, this order is not guaranteed.</em></p>
<a name="itemids-when-offering-items"></a><h4 id="itemids-when-offering-items">ItemIDs When Offering Items</h4>
<p>Hazelcast gives an <code>itemId</code> for each item you offer, which is an incrementing sequence identification for the queue items. You should consider the following to understand the <code>itemId</code> assignment behavior:</p>
<ul>
<li>When a Hazelcast member has a queue, and that queue is configured to have at least one backup, and that member is restarted, the <code>itemId</code> assignment resumes from the last known highest <code>itemId</code> before the restart; <code>itemId</code> assignment does not start from the beginning for the new items.</li>
<li>When the whole cluster is restarted, the same behavior explained in the above consideration applies if your queue has a persistent data store (<code>QueueStore</code>). If the queue has <code>QueueStore</code>, the <code>itemId</code> for the new items are given, starting from the highest <code>itemId</code> found in the IDs returned by the method <code>loadAllKeys</code>. If the method <code>loadAllKeys</code> does not return anything, the <code>itemId</code>s will started from the beginning after a cluster restart.</li>
<li>The above two considerations mean there will be no duplicated <code>itemId</code>s in the memory or in the persistent data store.</li>
</ul>

<a name="setting-a-bounded-queue"></a><h3 id="setting-a-bounded-queue">Setting a Bounded Queue</h3>
<p>A bounded queue is a queue with a limited capacity. When the bounded queue is full, no more items can be put into the queue until some items are taken out.</p>
<p>To turn a Hazelcast distributed queue into a bounded queue, set the capacity limit with the <code>max-size</code> property. You can set the <code>max-size</code> property in the configuration, as shown below. <code>max-size</code> specifies the maximum size of the queue. Once the queue size reaches this value, <code>put</code> operations will be blocked until the queue size goes below <code>max-size</code>, which happens when a consumer removes items from the queue.</p>
<p>Let&#39;s set <strong>10</strong> as the maximum size of our example queue in <a href="#creating-an-example-queue">Creating an Example Queue</a>.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;queue name=&quot;queue&quot;&gt;
    &lt;max-size&gt;10&lt;/max-size&gt;
  &lt;/queue&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>When the producer is started, ten items are put into the queue and then the queue will not allow more <code>put</code> operations. When the consumer is started, it will remove items from the queue. This means that the producer can <code>put</code> more items into the queue until there are ten items in the queue again, at which point the <code>put</code> operation again becomes blocked.</p>
<p>In this example code, the producer is five times faster than the consumer. It will effectively always be waiting for the consumer to remove items before it can put more on the queue. For this example code, if maximum throughput is the goal, it would be a good option to start multiple consumers to prevent the queue from filling up.</p>

<a name="queueing-with-persistent-datastore"></a><h3 id="queueing-with-persistent-datastore">Queueing with Persistent Datastore</h3>
<p>Hazelcast allows you to load and store the distributed queue items from/to a persistent datastore using the interface <code>QueueStore</code>. If queue store is enabled, each item added to the queue will also be stored at the configured queue store. When the number of items in the queue exceeds the memory limit, the subsequent items are persisted in the queue store, they are not stored in the queue memory.</p>
<p>The <code>QueueStore</code> interface enables you to store, load, and delete queue items with methods like <code>store</code>, <code>storeAll</code>, <code>load</code> and <code>delete</code>. The following example class includes all of the <code>QueueStore</code> methods.</p>
<pre><code class="lang-java">public class TheQueueStore implements QueueStore&lt;Item&gt; {
    @Override
    public void delete(Long key) {
        System.out.println(&quot;delete&quot;);
    }

    @Override
    public void store(Long key, Item value) {
        System.out.println(&quot;store&quot;);
    }

    @Override
    public void storeAll(Map&lt;Long, Item&gt; map) {
        System.out.println(&quot;store all&quot;);
    }

    @Override
    public void deleteAll(Collection&lt;Long&gt; keys) {
        System.out.println(&quot;deleteAll&quot;);
    }

    @Override
    public Item load(Long key) {
        System.out.println(&quot;load&quot;);
        return null;
    }

    @Override
    public Map&lt;Long, Item&gt; loadAll(Collection&lt;Long&gt; keys) {
        System.out.println(&quot;loadAll&quot;);
        return null;
    }

    @Override
    public Set&lt;Long&gt; loadAllKeys() {
        System.out.println(&quot;loadAllKeys&quot;);
        return null;
    }
</code></pre>
<p><code>Item</code> must be serializable. The following is an example queue store configuration.</p>
<pre><code class="lang-xml">&lt;queue-store&gt;
  &lt;class-name&gt;com.hazelcast.QueueStoreImpl&lt;/class-name&gt;
  &lt;properties&gt;
    &lt;property name=&quot;binary&quot;&gt;false&lt;/property&gt;
    &lt;property name=&quot;memory-limit&quot;&gt;1000&lt;/property&gt;
    &lt;property name=&quot;bulk-load&quot;&gt;500&lt;/property&gt;
  &lt;/properties&gt;
&lt;/queue-store&gt;
</code></pre>
<p>Let&#39;s explain the queue store properties.</p>
<ul>
<li><p><strong>Binary</strong>: By default, Hazelcast stores the queue items in serialized form, and before it inserts the queue items into the , it deserializes them. If you are not reaching the queue store from an external application, you might prefer that the items be inserted in binary form. Do this by setting the <code>binary</code> property to true: then you can get rid of the deserialization step, which is a performance optimization. The <code>binary</code> property is false by default.</p>
</li>
<li><p><strong>Memory Limit</strong>: This is the number of items after which Hazelcast will store items only to the datastore. For example, if the memory limit is 1000, then the 1001st item will be put only to the datastore. This feature is useful when you want to avoid out-of-memory conditions. If you want to always use memory, you can set it to <code>Integer.MAX_VALUE</code>. The default number for <code>memory-limit</code> is 1000.</p>
</li>
<li><p><strong>Bulk Load</strong>: When the queue is initialized, items are loaded from <code>QueueStore</code> in bulks. Bulk load is the size of these bulks. The default value of <code>bulk-load</code> is 250.</p>
</li>
</ul>

<a name="configuring-queue"></a><h3 id="configuring-queue">Configuring Queue</h3>
<p>The following are examples of queue configurations. It includes the <code>QueueStore</code> configuration, which is explained in the <a href="#queueing-with-persistent-datastore">Queueing with Persistent Datastore</a> section.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;queue name=&quot;default&quot;&gt;
    &lt;max-size&gt;0&lt;/max-size&gt;
    &lt;backup-count&gt;1&lt;/backup-count&gt;
    &lt;async-backup-count&gt;0&lt;/async-backup-count&gt;
    &lt;empty-queue-ttl&gt;-1&lt;/empty-queue-ttl&gt;
    &lt;item-listeners&gt;
       &lt;item-listener&gt;
          com.hazelcast.examples.ItemListener
       &lt;/item-listener&gt;
    &lt;item-listeners&gt;
&lt;/queue&gt;
&lt;queue-store&gt;
    &lt;class-name&gt;com.hazelcast.QueueStoreImpl&lt;/class-name&gt;
    &lt;properties&gt;
       &lt;property name=&quot;binary&quot;&gt;false&lt;/property&gt;
       &lt;property name=&quot;memory-limit&quot;&gt;10000&lt;/property&gt;
       &lt;property name=&quot;bulk-load&quot;&gt;500&lt;/property&gt;
    &lt;/properties&gt;
&lt;/queue-store&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
QueueConfig queueConfig = config.getQueueConfig();
queueConfig.setName( &quot;MyQueue&quot; ).setBackupCount( &quot;1&quot; )
        .setMaxSize( &quot;0&quot; ).setStatisticsEnabled( &quot;true&quot; );
queueConfig.getQueueStoreConfig()
        .setEnabled ( &quot;true&quot; )
        .setClassName( &quot;com.hazelcast.QueueStoreImpl&quot; )
        .setProperty( &quot;binary&quot;, &quot;false&quot; );
</code></pre>
<p>Hazelcast distributed queue has one synchronous backup by default. By having this backup, when a cluster member with a queue goes down, another member having the backup of that queue will continue. Therefore, no items are lost. You can define the number of synchronous backups for a queue using the <code>backup-count</code> element in the declarative configuration. A queue can also have asynchronous backups: you can define the number of asynchronous backups using the <code>async-backup-count</code> element.</p>
<p>To set the maximum size of the queue, use the <code>max-size</code> element. To purge unused or empty queues after a period of time, use the <code>empty-queue-ttl</code> element. If you define a value (time in seconds) for the <code>empty-queue-ttl</code> element, then your queue will be destroyed if it stays empty or unused for the time in seconds that you give.</p>
<p>The following is the full list of queue configuration elements with their descriptions.</p>
<ul>
<li><code>max-size</code>: Maximum number of items in the queue. It is used to set an upper bound for the queue. You will not be able to put more items when the queue reaches to this maximum size whether you have a queue store configured or not.</li>
<li><code>backup-count</code>: Number of synchronous backups. Queue is a non-partitioned data structure, so all entries of a queue reside in one partition. When this parameter is &#39;1&#39;, it means there will be one backup of that queue in another member in the cluster. When it is &#39;2&#39;, two members will have the backup.</li>
<li><code>async-backup-count</code>: Number of asynchronous backups.</li>
<li><code>empty-queue-ttl</code>: Used to purge unused or empty queues. If you define a value (time in seconds) for this element, then your queue will be destroyed if it stays empty or unused for that time.</li>
<li><code>item-listeners</code>: Adds listeners (listener classes) for the queue items. You can also set the attribute <code>include-value</code> to <code>true</code> if you want the item event to contain the item values, and you can set <code>local</code> to <code>true</code> if you want to listen to the items on the local member.</li>
<li><code>queue-store</code>: Includes the queue store factory class name and the properties  <em>binary</em>, <em>memory limit</em> and <em>bulk load</em>. Please refer to <a href="#queueing-with-persistent-datastore">Queueing with Persistent Datastore</a>.</li>
<li><code>statistics-enabled</code>: If set to <code>true</code>, you can retrieve statistics for this queue using the method <code>getLocalQueueStats()</code>.</li>
</ul>

<a name="multimap"></a><h2 id="multimap">MultiMap</h2>
<p>Hazelcast <code>MultiMap</code> is a specialized map where you can store multiple values under a single key. Just like any other distributed data structure implementation in Hazelcast, <code>MultiMap</code> is distributed and thread-safe.</p>
<p>Hazelcast <code>MultiMap</code> is not an implementation of <code>java.util.Map</code> due to the difference in method signatures. It supports most features of Hazelcast Map except for indexing, predicates and MapLoader/MapStore. Yet, like Hazelcast Map, entries are almost evenly distributed onto all cluster members. When a new member joins the cluster, the same ownership logic used in the distributed map applies.</p>
<a name="getting-a-multimap-and-putting-an-entry"></a><h3 id="getting-a-multimap-and-putting-an-entry">Getting a MultiMap and Putting an Entry</h3>
<p>The following example creates a MultiMap and puts items into it. Use the HazelcastInstance <code>getMultiMap</code> method to get the MultiMap, then use the MultiMap <code>put</code> method to put an entry into the MultiMap.</p>
<pre><code class="lang-java">public class PutMember {
  public static void main( String[] args ) {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    MultiMap &lt;String , String &gt; map = hazelcastInstance.getMultiMap( &quot;map&quot; );

    map.put( &quot;a&quot;, &quot;1&quot; );
    map.put( &quot;a&quot;, &quot;2&quot; );
    map.put( &quot;b&quot;, &quot;3&quot; ); 
    System.out.println( &quot;PutMember:Done&quot; );
  }
}
</code></pre>
<p>Now let&#39;s print the entries in this MultiMap.</p>
<pre><code class="lang-java">public class PrintMember {
  public static void main( String[] args ) { 
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    MultiMap &lt;String, String &gt; map = hazelcastInstance.getMultiMap( &quot;map&quot; );
    for ( String key : map.keySet() ){
      Collection &lt;String &gt; values = map.get( key );
      System.out.println( &quot;%s -&gt; %s\n&quot;,key, values );
    }
  }
}
</code></pre>
<p>After you run the first code sample, run the <code>PrintMember</code> sample. You will see the key <strong><code>a</code></strong> has two values, as shown below.</p>
<p><code>b -&gt; [3]</code></p>
<p><code>a -&gt; [2, 1]</code></p>
<p>Hazelcast MultiMap uses <code>EntryListener</code> to listen to events which occur when entries are added to, updated in or removed from the MultiMap. Please refer to the <a href="#listening-for-multimap-events">Listening for MultiMap Events section</a> for information on how to create an entry listener class and register it.</p>
<a name="configuring-multimap"></a><h3 id="configuring-multimap">Configuring MultiMap</h3>
<p>When using MultiMap, the collection type of the values can be either <strong>Set</strong> or <strong>List</strong>. Configure the collection type with the <code>valueCollectionType</code> parameter. If you choose <code>Set</code>, duplicate and null values are not allowed in your collection and ordering is irrelevant. If you choose <code>List</code>, ordering is relevant and your collection can include duplicate and null values.</p>
<p>You can also enable statistics for your MultiMap with the <code>statisticsEnabled</code> parameter. If you enable <code>statisticsEnabled</code>, statistics can be retrieved with <code>getLocalMultiMapStats()</code> method.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Currently, eviction is not supported for the MultiMap data structure.</em>
<br></br></p>
<p>The following are the example MultiMap configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;multimap name=&quot;default&quot;&gt;
    &lt;backup-count&gt;0&lt;/backup-count&gt;
    &lt;async-backup-count&gt;1&lt;/async-backup-count&gt;
    &lt;value-collection-type&gt;SET&lt;/value-collection-type&gt;
    &lt;entry-listeners&gt;
        &lt;entry-listener include-value=&quot;false&quot; local=&quot;false&quot;&gt;
           com.hazelcast.examples.EntryListener
        &lt;/entry-listener&gt;
    &lt;/entry-listeners&gt;   
  &lt;/map&gt;
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">MultiMapConfig mmConfig = new MultiMapConfig();
mmConfig.setName( &quot;default&quot; );

mmConfig.setBackupCount( &quot;0&quot; ).setAsyncBackupCount( &quot;1&quot; );

mmConfig.setValueCollectionType( &quot;SET&quot; );
</code></pre>
<p>The following are the configuration elements and their descriptions:</p>
<ul>
<li><code>backup-count</code>: Defines the number of asynchronous backups. For example, if it is set to 1, backup of a partition will be
placed on one other member. If it is 2, it will be placed on two other members.</li>
<li><code>async-backup-count</code>: The number of synchronous backups. Behavior is the same as that of the <code>backup-count</code> element.</li>
<li><code>statistics-enabled</code>: You can retrieve some statistics such as owned entry count, backup entry count, last update time, and locked entry count by setting this parameter&#39;s value as &quot;true&quot;. The method for retrieving the statistics is <code>getLocalMultiMapStats()</code>.</li>
<li><code>value-collection-type</code>: Type of the value collection. It can be <code>Set</code> or <code>List</code>.</li>
<li><code>entry-listeners</code>: Lets you add listeners (listener classes) for the map entries. You can also set the attribute
include-value to true if you want the item event to contain the entry values, and you can set
local to true if you want to listen to the entries on the local member.</li>
</ul>

<a name="set"></a><h2 id="set">Set</h2>
<p>Hazelcast Set is a distributed and concurrent implementation of <code>java.util.Set</code>.</p>
<ul>
<li>Hazelcast Set does not allow duplicate elements.</li>
<li>Hazelcast Set does not preserve the order of elements.</li>
<li>Hazelcast Set is a non-partitioned data structure--all the data that belongs to a set will live on one single partition in that member.</li>
<li>Hazelcast Set cannot be scaled beyond the capacity of a single machine. Since the whole set lives on a single partition, storing a large amount of data on a single set may cause memory pressure. Therefore, you should use multiple sets to store a large amount of data. This way, all the sets will be spread across the cluster, sharing the load.</li>
<li>A backup of Hazelcast Set is stored on a partition of another member in the cluster so that data is not lost in the event of a primary member failure.</li>
<li>All items are copied to the local member and iteration occurs locally.</li>
<li>The equals method implemented in Hazelcast Set uses a serialized byte version of objects, as opposed to <code>java.util.HashSet</code>.</li>
</ul>
<a name="getting-a-set-and-putting-items"></a><h3 id="getting-a-set-and-putting-items">Getting a Set and Putting Items</h3>
<p>Use the HazelcastInstance <code>getSet</code> method to get the Set, then use the set <code>put</code> method to put items into the Set.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import java.util.Set;
import java.util.Iterator;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

Set&lt;Price&gt; set = hazelcastInstance.getSet( &quot;IBM-Quote-History&quot; );
set.add( new Price( 10, time1 ) );
set.add( new Price( 11, time2 ) );
set.add( new Price( 12, time3 ) );
set.add( new Price( 11, time4 ) );
//....
Iterator&lt;Price&gt; iterator = set.iterator();
while ( iterator.hasNext() ) { 
  Price price = iterator.next(); 
  //analyze
}
</code></pre>
<p>Hazelcast Set uses <code>ItemListener</code> to listen to events that occur when items are added to and removed from the Set. Please refer to the <a href="#listening-for-item-events">Listening for Item Events section</a> for information on how to create an item listener class and register it.</p>
<a name="configuring-set"></a><h3 id="configuring-set">Configuring Set</h3>
<p>The following are the example set configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;set name=&quot;default&quot;&gt;
   &lt;backup-count&gt;1&lt;/backup-count&gt;
   &lt;async-backup-count&gt;0&lt;/async-backup-count&gt;
   &lt;max-size&gt;10&lt;/max-size&gt;
   &lt;item-listeners&gt;
      &lt;item-listener&gt;
          com.hazelcast.examples.ItemListener
      &lt;/item-listener&gt;
   &lt;item-listeners&gt;
&lt;/set&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
CollectionConfig collectionSet = config.getCollectionConfig();
collectionSet.setName( &quot;MySet&quot; ).setBackupCount( &quot;1&quot; )
        .setMaxSize( &quot;10&quot; );
</code></pre>
<p>Set configuration has the following elements.</p>
<ul>
<li><code>statistics-enabled</code>: True (default) if statistics gathering is enabled on the Set, false otherwise.</li>
<li><code>backup-count</code>: Count of synchronous backups. Set is a non-partitioned data structure, so all entries of a Set reside in one partition. When this parameter is &#39;1&#39;, it means there will be one backup of that Set in another member in the cluster. When it is &#39;2&#39;, two members will have the backup.</li>
<li><code>async-backup-count</code>: Count of asynchronous backups.</li>
<li><code>max-size</code>: The maximum number of entries for this Set.</li>
<li><code>item-listeners</code>: Lets you add listeners (listener classes) for the list items. You can also set the attributes <code>include-value</code> to <code>true</code> if you want the item event to contain the item values, and you can set <code>local</code> to <code>true</code> if you want to listen to the items on the local member.</li>
</ul>

<a name="list"></a><h2 id="list">List</h2>
<p>Hazelcast List is similar to Hazelcast Set, but Hazelcast List also allows duplicate elements.</p>
<ul>
<li>Besides allowing duplicate elements, Hazelcast List preserves the order of elements.</li>
<li>Hazelcast List is a non-partitioned data structure where values and each backup are represented by their own single partition.</li>
<li>Hazelcast List cannot be scaled beyond the capacity of a single machine.</li>
<li>All items are copied to local and iteration occurs locally.</li>
</ul>
<a name="getting-a-list-and-putting-items"></a><h3 id="getting-a-list-and-putting-items">Getting a List and Putting Items</h3>
<p>Use the HazelcastInstance <code>getList</code> method to get the list, then use the list <code>put</code> method to put items into the List.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import java.util.List;
import java.util.Iterator;

HazelcastInstance hz = Hazelcast.newHazelcastInstance();

List&lt;Price&gt; list = hz.getList( &quot;IBM-Quote-Frequency&quot; );
list.add( new Price( 10 ) );
list.add( new Price( 11 ) );
list.add( new Price( 12 ) );
list.add( new Price( 11 ) );
list.add( new Price( 12 ) );

//....
Iterator&lt;Price&gt; iterator = list.iterator();
while ( iterator.hasNext() ) { 
  Price price = iterator.next(); 
  //analyze
}
</code></pre>
<p>Hazelcast List uses <code>ItemListener</code> to listen to events that occur when items are added to and removed from the List. Please refer to the <a href="#listening-for-item-events">Listening for Item Events section</a> for information on how to create an item listener class and register it.</p>
<a name="configuring-list"></a><h3 id="configuring-list">Configuring List</h3>
<p>The following are example list configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;list name=&quot;default&quot;&gt;
   &lt;backup-count&gt;1&lt;/backup-count&gt;
   &lt;async-backup-count&gt;0&lt;/async-backup-count&gt;
   &lt;max-size&gt;10&lt;/max-size&gt;
   &lt;item-listeners&gt;
      &lt;item-listener&gt;
          com.hazelcast.examples.ItemListener
      &lt;/item-listener&gt;
   &lt;/item-listeners&gt;
&lt;/list&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
CollectionConfig collectionList = config.getCollectionConfig();
collectionList.setName( &quot;MyList&quot; ).setBackupCount( &quot;1&quot; )
        .setMaxSize( &quot;10&quot; );
</code></pre>
<p>List configuration has the following elements.</p>
<ul>
<li><code>statistics-enabled</code>: True (default) if statistics gathering is enabled on the list, false otherwise.</li>
<li><code>backup-count</code>: Number of synchronous backups. List is a non-partitioned data structure, so all entries of a List reside in one partition. When this parameter is &#39;1&#39;, there will be one backup of that List in another member in the cluster. When it is &#39;2&#39;, two members will have the backup.</li>
<li><code>async-backup-count</code>: Number of asynchronous backups.</li>
<li><code>max-size</code>: The maximum number of entries for this List.</li>
<li><code>item-listeners</code>: Lets you add listeners (listener classes) for the list items. You can also set the attribute <code>include-value</code> to <code>true</code> if you want the item event to contain the item values, and you can set the attribute <code>local</code> to <code>true</code> if you want to listen the items on the local member.</li>
</ul>

<a name="ringbuffer"></a><h2 id="ringbuffer">Ringbuffer</h2>
<p>Hazelcast Ringbuffer is a distributed data structure that stores its data in a ring-like structure. You can think of it as a circular array with a 
given capacity. Each Ringbuffer has a tail and a head. The tail is where the items are added and the head is where the items are overwritten 
or expired. You can reach each element in a Ringbuffer using a sequence ID, which is mapped to the elements between the head 
and tail (inclusive) of the Ringbuffer. </p>
<a name="getting-a-ringbuffer-and-reading-items"></a><h3 id="getting-a-ringbuffer-and-reading-items">Getting a Ringbuffer and Reading Items</h3>
<p>Reading from Ringbuffer is simple: get the Ringbuffer with the HazelcastInstance <code>getRingbuffer</code> method, get its current head with
the <code>headSequence</code> method, and start reading. Use the method <code>readOne</code> to return the item at the 
given sequence; <code>readOne</code> blocks if no item is available. To read the next item, increment the sequence by one.</p>
<pre><code class="lang-java">Ringbuffer&lt;String&gt; ringbuffer = hz.getRingbuffer(&quot;rb&quot;);
long sequence = ringbuffer.headSequence();
while(true){
    String item = ringbuffer.readOne(sequence);
    sequence++;
    ... process item
}
</code></pre>
<p>By exposing the sequence, you can now move the item from the Ringbuffer as long as the item is still available. If the item is not available
any longer, <code>StaleSequenceException</code> is thrown.</p>
<a name="adding-items-to-a-ringbuffer"></a><h3 id="adding-items-to-a-ringbuffer">Adding Items to a Ringbuffer</h3>
<p>Adding an item to a Ringbuffer is also easy with the Ringbuffer <code>add</code> method:</p>
<pre><code class="lang-java">Ringbuffer&lt;String&gt; ringbuffer = hz.getRingbuffer(&quot;rb&quot;);
ringbuffer.add(&quot;someitem&quot;)
</code></pre>
<p>Use the method <code>add</code> to return the sequence of the inserted item; the sequence value will always be unique. You can use this as a 
very cheap way of generating unique IDs if you are already using Ringbuffer.</p>
<a name="iqueue-vs-ringbuffer"></a><h3 id="iqueue-vs-ringbuffer">IQueue vs. Ringbuffer</h3>
<p>Hazelcast Ringbuffer can sometimes be a better alternative than an Hazelcast IQueue. Unlike IQueue, Ringbuffer does not remove the items, it only
reads items using a certain position. There are many advantages to this approach:</p>
<ul>
<li>The same item can be read multiple times by the same thread. This is useful for realizing semantics of read-at-least-once or 
read-at-most-once.</li>
<li>The same item can be read by multiple threads. Normally you could use an IQueue per thread for the same semantic, but this is 
less efficient because of the increased remoting. A take from an IQueue is destructive, so the change needs to be applied for backup 
also, which is why a <code>queue.take()</code> is more expensive than a <code>ringBuffer.read(...)</code>.</li>
<li>Reads are extremely cheap since there is no change in the Ringbuffer. Therefore no replication is required. </li>
<li>Reads and writes can be batched to speed up performance. Batching can dramatically improve the performance of Ringbuffer.</li>
</ul>
<a name="configuring-ringbuffer-capacity"></a><h3 id="configuring-ringbuffer-capacity">Configuring Ringbuffer Capacity</h3>
<p>By default, a Ringbuffer is configured with a <code>capacity</code> of 10000 items. This creates an array with a size of 10000. If 
a <code>time-to-live</code> is configured, then an array of longs is also created that stores the expiration time for every item. 
In a lot of cases you may want to change this <code>capacity</code> number to something that better fits your needs. </p>
<p>Below is a declarative configuration example of a Ringbuffer with a <code>capacity</code> of 2000 items.</p>
<pre><code class="lang-xml">&lt;ringbuffer name=&quot;rb&quot;&gt;
    &lt;capacity&gt;2000&lt;/capacity&gt;
&lt;/ringbuffer&gt;
</code></pre>
<p>Currently, Hazelcast Ringbuffer is not a partitioned data structure; its data is stored in a single partition and the replicas
 are stored in another partition. Therefore, create a Ringbuffer that can safely fit in a single cluster member. </p>
<a name="backing-up-ringbuffer"></a><h3 id="backing-up-ringbuffer">Backing Up Ringbuffer</h3>
<p>Hazelcast Ringbuffer has a single synchronous backup by default. You can control the Ringbuffer backup just like most of the other Hazelcast 
distributed data structures by setting the synchronous and asynchronous backups: <code>backup-count</code> and <code>async-backup-count</code>. In the example below, a Ringbuffer is configured with no
synchronous backups and one asynchronous backup:</p>
<pre><code class="lang-xml">&lt;ringbuffer name=&quot;rb&quot;&gt;
    &lt;backup-count&gt;0&lt;/backup-count&gt;
    &lt;async-backup-count&gt;1&lt;/async-backup-count&gt;
&lt;/ringbuffer&gt;
</code></pre>
<p>An asynchronous backup will probably give you better performance. However, there is a chance that the item added will be lost 
when the member owning the primary crashes before the backup could complete. You may want to consider batching
methods if you need high performance but do not want to give up on consistency.</p>
<a name="configuring-ringbuffer-time-to-live"></a><h3 id="configuring-ringbuffer-time-to-live">Configuring Ringbuffer Time To Live</h3>
<p>You can configure Hazelcast Ringbuffer with a time to live in seconds. Using this setting, you can control how long the items remain in 
the Ringbuffer before they are expired. By default, the time to live is set to 0, meaning that unless the item is overwritten, 
it will remain in the Ringbuffer indefinitely. If you set a time to live and an item is added, then, depending on the Overflow Policy, 
either the oldest item is overwritten, or the call is rejected. </p>
<p>In the example below, a Ringbuffer is configured with a time to live of 180 seconds.</p>
<pre><code class="lang-xml">&lt;ringbuffer name=&quot;rb&quot;&gt;
    &lt;time-to-live-seconds&gt;180&lt;/time-to-live-seconds&gt;
&lt;/ringbuffer&gt;
</code></pre>
<a name="setting-ringbuffer-overflow-policy"></a><h3 id="setting-ringbuffer-overflow-policy">Setting Ringbuffer Overflow Policy</h3>
<p>Using the overflow policy, you can determine what to do if the oldest item in the Ringbuffer is not old enough to expire when
 more items than the configured Ringbuffer capacity are being added. The below options are currently available.</p>
<ul>
<li><code>OverflowPolicy.OVERWRITE</code>: The oldest item is overwritten. </li>
<li><code>OverflowPolicy.FAIL</code>: The call is aborted. The methods that make use of the OverflowPolicy return <code>-1</code> to indicate that adding
the item has failed. </li>
</ul>
<p>Overflow policy gives you fine control on what to do if the Ringbuffer is full. You can also use the overflow policy to apply 
a back pressure mechanism. The following example code shows the usage of an exponential backoff.</p>
<pre><code class="lang-java">long sleepMs = 100;
for (; ; ) {
    long result = ringbuffer.addAsync(item, OverflowPolicy.FAIL).get();
    if (result != -1) {
        break;
    }

    TimeUnit.MILLISECONDS.sleep(sleepMs);
    sleepMs = min(5000, sleepMs * 2);
}
</code></pre>
<a name="configuring-ringbuffer-in-memory-format"></a><h3 id="configuring-ringbuffer-in-memory-format">Configuring Ringbuffer In-Memory Format</h3>
<p>You can configure Hazelcast Ringbuffer with an in-memory format that controls the format of the Ringbuffer&#39;s stored items. By default, <code>BINARY</code> in-memory format is used, 
meaning that the object is stored in a serialized form. You can select the <code>OBJECT</code> in-memory format, which is useful when filtering is 
applied or when the <code>OBJECT</code> in-memory format has a smaller memory footprint than <code>BINARY</code>. </p>
<p>In the declarative configuration example below, a Ringbuffer is configured with the <code>OBJECT</code> in-memory format:</p>
<pre><code class="lang-xml">&lt;ringbuffer name=&quot;rb&quot;&gt;
    &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
&lt;/ringbuffer&gt;
</code></pre>
<a name="adding-batched-items"></a><h3 id="adding-batched-items">Adding Batched Items</h3>
<p>In the previous examples, the method <code>ringBuffer.add()</code> is used to add an item to the Ringbuffer. The problems with this method 
are that it always overwrites and that it does not support batching. Batching can have a huge
impact on the performance. You can use the method <code>addAllAsync</code> to support batching. </p>
<p>Please see the following example code.</p>
<pre><code class="lang-java">List&lt;String&gt; items = Arrays.asList(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;);
ICompletableFuture&lt;Long&gt; f = rb.addAllAsync(items, OverflowPolicy.OVERWRITE);
f.get()
</code></pre>
<p>In the above case, three strings are added to the Ringbuffer using the policy <code>OverflowPolicy.OVERWRITE</code>. Please see the <a href="#setting-ringbuffer-overflow-policy">Overflow Policy section</a> 
for more information.</p>
<a name="reading-batched-items"></a><h3 id="reading-batched-items">Reading Batched Items</h3>
<p>In the previous example, the <code>readOne</code> method read items from the Ringbuffer. <code>readOne</code> is simple but not very efficient for the following reasons:</p>
<ul>
<li><code>readOne</code> does not use batching.</li>
<li><code>readOne</code> cannot filter items at the source; the items need to be retrieved before being filtered.</li>
</ul>
<p>The method <code>readManyAsync</code> can read a batch of items and can filter items at the source. </p>
<p>Please see the following example code.</p>
<pre><code class="lang-java">ICompletableFuture&lt;ReadResultSet&lt;E&gt;&gt; readManyAsync(
   long startSequence, 
   int minCount,                                              
   int maxCount, 
   IFunction&lt;E, Boolean&gt; filter);
</code></pre>
<p>The meanings of the <code>readManyAsync</code> arguments are given below.</p>
<ul>
<li><code>startSequence</code>: Sequence of the first item to read.</li>
<li><code>minCount</code>: Minimum number of items to read. If you do not want to block, set it to 0. If you want to block for at least one item,
set it to 1.</li>
<li><code>maxCount</code>: Maximum number of the items to retrieve. Its value cannot exceed 1000.</li>
<li><code>filter</code>: A function that accepts an item and checks if it should be returned. If no filtering should be applied, set it to null.</li>
</ul>
<p>A full example is given below.</p>
<pre><code class="lang-java">long sequence = rb.headSequence();
for(;;) {
    ICompletableFuture&lt;ReadResultSet&lt;String&gt;&gt; f = rb.readManyAsync(sequence, 1, 10, null);
    ReadResultSet&lt;String&gt; rs = f.get();
    for (String s : rs) {
        System.out.println(s);
    }
    sequence+=rs.readCount();
}
</code></pre>
<p>Please take a careful look at how your sequence is being incremented. You cannot always rely on the number of items being returned
if the items are filtered out.</p>
<a name="using-async-methods"></a><h3 id="using-async-methods">Using Async Methods</h3>
<p>Hazelcast Ringbuffer provides asynchronous methods for more powerful operations like batched writing or batched reading with filtering. 
To make these methods synchronous, just call the method <code>get()</code> on the returned future.</p>
<p>Please see the following example code.</p>
<pre><code class="lang-java">ICompletableFuture f = ringbuffer.addAsync(item, OverflowPolicy.FAIL);
f.get();
</code></pre>
<p>However, you can also use <code>ICompletableFuture</code> to get notified when the operation has completed. The advantage of <code>ICompletableFuture</code> is that the thread used for the call is not blocked till the response is returned.</p>
<p>Please see the below code as an example of when you want to 
get notified when a batch of reads has completed.</p>
<pre><code class="lang-java">ICompletableFuture&lt;ReadResultSet&lt;String&gt;&gt; f = rb.readManyAsync(sequence, min, max, someFilter);
f.andThen(new ExecutionCallback&lt;ReadResultSet&lt;String&gt;&gt;() {
   @Override
   public void onResponse(ReadResultSet&lt;String&gt; response) {
        for (String s : response) {
            System.out.println(&quot;Received:&quot; + s);
        }
   }

   @Override
   public void onFailure(Throwable t) {
        t.printStackTrace();
   }
});
</code></pre>
<a name="ringbuffer-configuration-examples"></a><h3 id="ringbuffer-configuration-examples">Ringbuffer Configuration Examples</h3>
<p>The following shows the declarative configuration of a Ringbuffer called <code>rb</code>. The configuration is modeled after the Ringbuffer defaults.</p>
<pre><code class="lang-xml">&lt;ringbuffer name=&quot;rb&quot;&gt;
    &lt;capacity&gt;10000&lt;/capacity&gt;
    &lt;backup-count&gt;1&lt;/backup-count&gt;
    &lt;async-backup-count&gt;0&lt;/async-backup-count&gt;
    &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
    &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
&lt;/ringbuffer&gt;
</code></pre>
<p>You can also configure a Ringbuffer programmatically. The following is a programmatic version of the above declarative configuration.</p>
<pre><code class="lang-java">RingbufferConfig rbConfig = new RingbufferConfig(&quot;rb&quot;)
    .setCapacity(10000)
    .setBackupCount(1)
    .setAsyncBackupCount(0)
    .setTimeToLiveSeconds(0)
    .setInMemoryFormat(InMemoryFormat.BINARY);
Config config = new Config();
config.addRingbufferConfig(rbConfig);
</code></pre>

<a name="topic"></a><h2 id="topic">Topic</h2>
<p>Hazelcast provides a distribution mechanism for publishing messages that are delivered to multiple subscribers. This is
also known as a publish/subscribe (pub/sub) messaging model. Publishing and subscribing operations are cluster wide.
When a member subscribes to a topic, it is actually registering for messages published by any member in the cluster,
including the new members that joined after you add the listener.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Publish operation is async. It does not wait for operations to run in
remote members; it works as fire and forget.</em></p>
<a name="getting-a-topic-and-publishing-messages"></a><h3 id="getting-a-topic-and-publishing-messages">Getting a Topic and Publishing Messages</h3>
<p>Use the HazelcastInstance <code>getTopic</code> method to get the Topic, then use the topic <code>publish</code> method to publish your messages (<code>messageObject</code>).</p>
<pre><code class="lang-java">import com.hazelcast.core.Topic;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.MessageListener;

public class Sample implements MessageListener&lt;MyEvent&gt; {

  public static void main( String[] args ) {
    Sample sample = new Sample();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ITopic topic = hazelcastInstance.getTopic( &quot;default&quot; );
    topic.addMessageListener( sample );
    topic.publish( new MyEvent() );
  }

  public void onMessage( Message&lt;MyEvent&gt; message ) {
    MyEvent myEvent = message.getMessageObject();
    System.out.println( &quot;Message received = &quot; + myEvent.toString() );
    if ( myEvent.isHeavyweight() ) {
      messageExecutor.execute( new Runnable() {
          public void run() {
            doHeavyweightStuff( myEvent );
          }
      } );
    }
  }

  // ...

  private final Executor messageExecutor = Executors.newSingleThreadExecutor();
}
</code></pre>
<p>Hazelcast Topic uses the <code>MessageListener</code> interface to listen for events that occur when a message is received. Please refer to the <a href="#listening-for-topic-messages">Listening for Topic Messages section</a> for information on how to create a message listener class and register it.</p>

<a name="getting-topic-statistics"></a><h3 id="getting-topic-statistics">Getting Topic Statistics</h3>
<p>Topic has two statistic variables that you can query. These values are incremental and local to the member.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
ITopic&lt;Object&gt; myTopic = hazelcastInstance.getTopic( &quot;myTopicName&quot; );

myTopic.getLocalTopicStats().getPublishOperationCount();
myTopic.getLocalTopicStats().getReceiveOperationCount();
</code></pre>
<p><code>getPublishOperationCount</code> and <code>getReceiveOperationCount</code> returns the total number of published and received messages since the start of this member, respectively. Please note that these values are not backed up, so if the member goes down, these values will be lost.</p>
<p>You can disable this feature with topic configuration. Please see the <a href="#configuring-topic">Configuring Topic section</a>.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>These statistics values can be also viewed in Management Center. Please see <a href="#monitoring-topics">Monitoring Topics</a></em>.</p>

<a name="understanding-topic-behavior"></a><h3 id="understanding-topic-behavior">Understanding Topic Behavior</h3>
<p>Each cluster member has a list of all registrations in the cluster. When a new member is registered for a topic, it sends a registration message to all members in the cluster. Also, when a new member joins the cluster, it will receive all registrations made so far in the cluster.</p>
<p>The behavior of a topic varies depending on the value of the configuration parameter <code>globalOrderEnabled</code>.</p>
<a name="ordering-messages-as-published"></a><h4 id="ordering-messages-as-published">Ordering Messages as Published</h4>
<p>If <code>globalOrderEnabled</code> is disabled, messages are not ordered and listeners (subscribers) process the messages in the order that the messages are published. If cluster member M publishes messages <em>m1, m2, m3, ..., mn</em> to a topic <strong>T</strong>, then Hazelcast makes sure that all of the subscribers of topic <strong>T</strong> will receive and process <em>m1, m2, m3, ..., mn</em> in the given order.</p>
<p>Here is how it works. Let&#39;s say that we have three members (<em>member1</em>, <em>member2</em> and <em>member3</em>) and that <em>member1</em> and <em>member2</em> are registered to a topic named <code>news</code>. Note that all three members know that <em>member1</em> and <em>member2</em> are registered to <code>news</code>.</p>
<p>In this example, <em>member1</em> publishes two messages, <code>a1</code> and <code>a2</code>, and <em>member3</em> publishes two messages, <code>c1</code> and <code>c2</code>. When <em>member1</em> and <em>member3</em> publish a message, they will check their local list for registered members, they will discover that <em>member1</em> and <em>member2</em> are in their lists, and then they will fire messages to those members. One possible order of the messages received could be the following.</p>
<p><em>member1</em> -&gt; <code>c1</code>, <code>a1</code>, <code>a2</code>, <code>c2</code></p>
<p><em>member2</em> -&gt; <code>c1</code>, <code>c2</code>, <code>a1</code>, <code>a2</code></p>
<a name="ordering-messages-for-members"></a><h4 id="ordering-messages-for-members">Ordering Messages for Members</h4>
<p>If <code>globalOrderEnabled</code> is enabled, all members listening to the same topic will get its messages in the same order.</p>
<p>Here is how it works. Let&#39;s say that we have three members (<em>member1</em>, <em>member2</em> and <em>member3</em>) and that <em>member1</em> and <em>member2</em> are registered to a topic named <code>news</code>. Note that all three members know that <em>member1</em> and <em>member2</em> are registered to <code>news</code>.</p>
<p>In this example, <em>member1</em> publishes two messages: <code>a1</code> and <code>a2</code>, and <em>member3</em> publishes two messages: <code>c1</code> and <code>c2</code>. When a member publishes messages over the topic <code>news</code>, it first calculates which partition the <code>news</code> ID corresponds to. Then it sends an operation to the owner of the partition for that member to publish messages. Let&#39;s assume that <code>news</code> corresponds to a partition that <em>member2</em> owns. <em>member1</em> and <em>member3</em> first sends all messages to <em>member2</em>. Assume that the messages are published in the following order:</p>
<p><em>member1</em> -&gt; <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code></p>
<p><em>member2</em> then publishes these messages by looking at registrations in its local list. It sends these messages to <em>member1</em> and <em>member2</em> (it makes a local dispatch for itself).</p>
<p><em>member1</em> -&gt; <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code></p>
<p><em>member2</em> -&gt; <code>a1</code>, <code>c1</code>, <code>a2</code>, <code>c2</code></p>
<p>This way we guarantee that all members will see the events in the same order.</p>
<a name="keeping-generated-and-published-order-the-same"></a><h4 id="keeping-generated-and-published-order-the-same">Keeping Generated and Published Order the Same</h4>
<p>In both cases, there is a <code>StripedExecutor</code> in EventService that is responsible for dispatching the received message. For all events in Hazelcast, the order that events are generated and the order they are published to the user are guaranteed to be the same via this <code>StripedExecutor</code>.</p>
<p>In <code>StripedExecutor</code>, there are as many threads as are specified in the property  <code>hazelcast.event.thread.count</code> (default is five). For a specific event source (for a particular topic name), <em>hash of that source&#39;s name % 5</em> gives the ID of the responsible thread. Note that there can be another event source (entry listener of a map, item listener of a collection, etc.) corresponding to the same thread. In order not to make other messages to block, heavy processing should not be done in this thread. If there is time-consuming work that needs to be done, the work should be handed over to another thread. Please see the <a href="#getting-a-topic-and-publishing-messages">Getting a Topic and Publishing Messages section</a>.</p>

<a name="configuring-topic"></a><h3 id="configuring-topic">Configuring Topic</h3>
<p>To configure a topic, set the topic name, decide on statistics and global ordering, and set message listeners.
Default values are:</p>
<ul>
<li><code>global-ordering</code> is <strong>false</strong>, meaning that by default, there is no guarantee of global order.</li>
<li><code>statistics</code> is <strong>true</strong>, meaning that by default, statistics are calculated.</li>
</ul>
<p>You can see the example configuration snippets below. </p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;topic name=&quot;yourTopicName&quot;&gt;
    &lt;global-ordering-enabled&gt;true&lt;/global-ordering-enabled&gt;
    &lt;statistics-enabled&gt;true&lt;/statistics-enabled&gt;
    &lt;message-listeners&gt;
      &lt;message-listener&gt;MessageListenerImpl&lt;/message-listener&gt;
    &lt;/message-listeners&gt;
  &lt;/topic&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">TopicConfig topicConfig = new TopicConfig();
topicConfig.setGlobalOrderingEnabled( true );
topicConfig.setStatisticsEnabled( true );
topicConfig.setName( &quot;yourTopicName&quot; );
MessageListener&lt;String&gt; implementation = new MessageListener&lt;String&gt;() {
  @Override
  public void onMessage( Message&lt;String&gt; message ) {
    // process the message
  }
};
topicConfig.addMessageListenerConfig( new ListenerConfig( implementation ) );
HazelcastInstance instance = Hazelcast.newHazelcastInstance()
</code></pre>
<p>Topic configuration has the following elements.</p>
<ul>
<li><code>statistics-enabled</code>: Default is <code>true</code>, meaning statistics are calculated.</li>
<li><code>global-ordering-enabled</code>: Default is <code>false</code>, meaning there is no global order guarantee.</li>
<li><code>message-listeners</code>: Lets you add listeners (listener classes) for the topic messages.</li>
</ul>
<p>Besides the above elements, there are the following system properties that are topic related but not topic specific:</p>
<ul>
<li><code>hazelcast.event.queue.capacity</code> with a default value of 1,000,000</li>
<li><code>hazelcast.event.queue.timeout.millis</code> with a default value of 250</li>
<li><code>hazelcast.event.thread.count</code> with a default value of 5</li>
</ul>
<p>For a description of these parameters, please see the <a href="#global-event-configuration">Global Event Configuration section</a>.</p>

<a name="reliable-topic"></a><h2 id="reliable-topic">Reliable Topic</h2>
<p>The Reliable Topic data structure was introduced in Hazelcast 3.5. The Reliable Topic uses the same <code>ITopic</code> interface
as a regular topic. The main difference is that Reliable Topic is backed up by the Ringbuffer (also introduced with Hazelcast 
3.5) data structure. The following are the advantages of this approach:</p>
<ul>
<li>Events are not lost since the Ringbuffer is configured with one synchronous backup by default.</li>
<li>Each Reliable <code>ITopic</code> gets its own Ringbuffer; if a topic has a very fast producer, it will not lead to problems at topics that run at a slower pace.</li>
<li>Since the event system behind a regular <code>ITopic</code> is shared with other data structures (e.g., collection listeners), 
you can run into isolation problems. This does not happen with the Reliable <code>ITopic</code>.</li>
</ul>
<a name="sample-reliable-itopic-code"></a><h3 id="sample-reliable-itopic-code">Sample Reliable ITopic Code</h3>
<pre><code class="lang-java">import com.hazelcast.core.Topic;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.MessageListener;

public class Sample implements MessageListener&lt;MyEvent&gt; {

  public static void main( String[] args ) {
    Sample sample = new Sample();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ITopic topic = hazelcastInstance.getReliableTopic( &quot;default&quot; );
    topic.addMessageListener( sample );
    topic.publish( new MyEvent() );
  }

  public void onMessage( Message&lt;MyEvent&gt; message ) {
    MyEvent myEvent = message.getMessageObject();
    System.out.println( &quot;Message received = &quot; + myEvent.toString() );
  }
}
</code></pre>
<p>You can configure the Reliable <code>ITopic</code> using its Ringbuffer. If a Reliable Topic has the name <code>Foo</code>, then you can configure this topic
by adding a <code>ReliableTopicConfig</code> for a Ringbuffer with the name <code>Foo</code>. By default, a Ringbuffer does not have any TTL (time to live) and
it has a limited capacity; you may want to change that configuration.</p>
<p>By default, the Reliable <code>ITopic</code> uses a shared thread pool. If you need better isolation, you can configure a custom executor on the 
<code>ReliableTopicConfig</code>. </p>
<p>Because the reads on a Ringbuffer are not destructive, batching is easy to apply. <code>ITopic</code> uses read batching and reads
ten items at a time (if available) by default.</p>
<a name="slow-consumers"></a><h3 id="slow-consumers">Slow Consumers</h3>
<p>The Reliable <code>ITopic</code> provides control and a way to deal with slow consumers. It is unwise to keep events for a slow consumer in memory 
indefinitely since you do not know when the slow consumer is going to catch up. You can control the size of the Ringbuffer by using its capacity. For the cases when a Ringbuffer runs out of its capacity, you can specify the following policies for the <code>TopicOverloadPolicy</code> configuration:</p>
<ul>
<li><code>DISCARD_OLDEST</code>: Overwrite the oldest item, even if a TTL is set. In this case the fast producer supersedes a slow consumer.</li>
<li><code>DISCARD_NEWEST</code>: Discard the newest item.</li>
<li><code>BLOCK</code>: Wait until the items are expired in the Ringbuffer.</li>
<li><code>ERROR</code>: Immediately throw <code>TopicOverloadException</code> if there is no space in the Ringbuffer.</li>
</ul>
<a name="configuring-reliable-topic"></a><h3 id="configuring-reliable-topic">Configuring Reliable Topic</h3>
<p>The following are example Reliable Topic configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;reliable-topic name=&quot;default&quot;&gt;
    &lt;statistics-enabled&gt;true&lt;/statistics-enabled&gt;
    &lt;message-listeners&gt;
        &lt;message-listener&gt;
        ...
        &lt;/message-listener&gt;
    &lt;/message-listeners&gt;
    &lt;read-batch-size&gt;10&lt;/read-batch-size&gt;
    &lt;topic-overload-policy&gt;BLOCK&lt;/topic-overload-policy&gt;
&lt;/reliable-topic&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
ReliableTopicConfig rtConfig = config.getReliableTopicConfig();
rtConfig.setTopicOverloadPolicy( TopicOverloadPolicy.BLOCK )
        .setReadBatchSize( 10 )
        .setStatisticsEnabled( true );
</code></pre>
<p>Reliable Topic configuration has the following elements.</p>
<ul>
<li><code>statistics-enabled</code>: Enables or disables the statistics collection for the Reliable Topic. The default value is <code>true</code>.</li>
<li><code>message-listener</code>: Message listener class that listens to the messages when they are added or removed.</li>
<li><code>read-batch-size</code>: Minimum number of messages that Reliable Topic will try to read in batches. The default value is 10.</li>
<li><code>topic-overload-policy</code>: Policy to handle an overloaded topic. Available values are <code>DISCARD_OLDEST</code>, <code>DISCARD_NEWEST</code>, <code>BLOCK</code> and <code>ERROR</code>. The default value is `BLOCK. See <a href="#slow-consumers">Slow Consumers</a> for definitions of these policies.</li>
</ul>

<a name="lock"></a><h2 id="lock">Lock</h2>
<p>ILock is the distributed implementation of <code>java.util.concurrent.locks.Lock</code>, meaning that if you lock using an ILock, the critical
section that it guards is guaranteed to be executed by only one thread in the entire cluster. Even though locks are great for synchronization, they can lead to problems if not used properly. Also note that Hazelcast Lock does not support fairness.</p>
<a name="using-try-catch-blocks-with-locks"></a><h3 id="using-try-catch-blocks-with-locks">Using Try-Catch Blocks with Locks</h3>
<p>Always use locks with <em>try</em>-<em>catch</em> blocks. This will ensure that locks are released if an exception is thrown from
the code in a critical section. Also note that the <code>lock</code> method is outside the <em>try</em>-<em>catch</em> block because we do not want to unlock
if the lock operation itself fails.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import java.util.concurrent.locks.Lock;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Lock lock = hazelcastInstance.getLock( &quot;myLock&quot; );
lock.lock();
try {
  // do something here
} finally {
  lock.unlock();
}
</code></pre>
<a name="releasing-locks-with-trylock-timeout"></a><h3 id="releasing-locks-with-trylock-timeout">Releasing Locks with tryLock Timeout</h3>
<p>If a lock is not released in the cluster, another thread that is trying to get the
lock can wait forever. To avoid this, use <code>tryLock</code> with a timeout value. You can
set a high value (normally it should not take that long) for <code>tryLock</code>. You can check the return value of <code>tryLock</code> as follows:</p>
<pre><code class="lang-java">if ( lock.tryLock ( 10, TimeUnit.SECONDS ) ) {
  try {  
    // do some stuff here..  
  } finally {  
    lock.unlock();  
  }   
} else {
  // warning
}
</code></pre>
<a name="avoiding-waiting-threads-with-lease-time"></a><h3 id="avoiding-waiting-threads-with-lease-time">Avoiding Waiting Threads with Lease Time</h3>
<p>You can also avoid indefinitely waiting threads by using lock with lease time--the lock will be released in the given lease time. The lock can be safely unlocked before the lease time expires. Note that the unlock operation can
throw an <code>IllegalMonitorStateException</code> if the lock is released because the lease time expires. If that is the case, critical section guarantee is broken.</p>
<p>Please see the below example.</p>
<pre><code class="lang-java">lock.lock( 5, TimeUnit.SECONDS )
try {
  // do some stuff here..
} finally {
  try {
    lock.unlock();
  } catch ( IllegalMonitorStateException ex ){
    // WARNING Critical section guarantee can be broken
  }
}
</code></pre>
<p>You can also specify a lease time when trying to acquire a lock: <code>tryLock(time, unit, leaseTime, leaseUnit)</code>. In that case, it tries to acquire the lock within the specified lease time. If the lock is not available, the current thread becomes disabled for thread scheduling purposes until either it acquires the lock or the specified waiting time elapses. Note that this lease time cannot be longer than the time you specify with the system property <code>hazelcast.lock.max.lease.time.seconds</code>. Please see the <a href="#system-properties">System Properties section</a> to see the description of this property and to learn how to set a system property.</p>
<a name="understanding-lock-behavior"></a><h3 id="understanding-lock-behavior">Understanding Lock Behavior</h3>
<ul>
<li><p>Locks are fail-safe. If a member holds a lock and some other members go down, the cluster will keep your locks safe and available.
Moreover, when a member leaves the cluster, all the locks acquired by that dead member will be removed so that those
locks are immediately available for live members.</p>
</li>
<li><p>Locks are re-entrant. The same thread can lock multiple times on the same lock. Note that for other threads to be
able to require this lock, the owner of the lock must call <code>unlock</code> as many times as the owner called <code>lock</code>.</p>
</li>
<li><p>In the split-brain scenario, the cluster behaves as if it were two different clusters. Since two separate clusters are not aware of each other,
two members from different clusters can acquire the same lock.
For more information on places where split brain syndrome can be handled, please see <a href="#network-partitioning-split-brain-syndrome">split brain syndrome</a>.</p>
</li>
<li><p>Locks are not automatically removed. If a lock is not used anymore, Hazelcast will not automatically garbage collect the lock. 
This can lead to an <code>OutOfMemoryError</code>. If you create locks on the fly, make sure they are destroyed.</p>
</li>
<li><p>Hazelcast IMap also provides locking support on the entry level with the method <code>IMap.lock(key)</code>. Although the same infrastructure 
is used, <code>IMap.lock(key)</code> is not an ILock and it is not possible to expose it directly.</p>
</li>
</ul>
<a name="synchronizing-threads-with-icondition"></a><h3 id="synchronizing-threads-with-icondition">Synchronizing Threads with ICondition</h3>
<p><code>ICondition</code> is the distributed implementation of the <code>notify</code>, <code>notifyAll</code> and <code>wait</code> operations on the Java object. You can use it to synchronize
threads across the cluster. More specifically, you use <code>ICondition</code> when a thread&#39;s work depends on another thread&#39;s output. A good example
is producer/consumer methodology. </p>
<p>Please see the below code examples for a producer/consumer implementation.</p>
<p><strong>Producer thread:</strong></p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Lock lock = hazelcastInstance.getLock( &quot;myLockId&quot; );
ICondition condition = lock.newCondition( &quot;myConditionId&quot; );

lock.lock();
try {
  while ( !shouldProduce() ) {
    condition.await(); // frees the lock and waits for signal
                       // when it wakes up it re-acquires the lock
                       // if available or waits for it to become
                       // available
  }
  produce();
  condition.signalAll();
} finally {
  lock.unlock();
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The method <code>await()</code> takes time value and time unit as arguments. If you specify a negative value for the time, it is interpreted as infinite.</em></p>
<p><strong>Consumer thread:</strong></p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Lock lock = hazelcastInstance.getLock( &quot;myLockId&quot; );
ICondition condition = lock.newCondition( &quot;myConditionId&quot; );

lock.lock();
try {
  while ( !canConsume() ) {
    condition.await(); // frees the lock and waits for signal
                       // when it wakes up it re-acquires the lock if 
                       // available or waits for it to become
                       // available
  }
  consume();
  condition.signalAll();
} finally {
  lock.unlock();
}
</code></pre>

<a name="iatomiclong"></a><h2 id="iatomiclong">IAtomicLong</h2>
<p>Hazelcast <code>IAtomicLong</code> is the distributed implementation of <code>java.util.concurrent.atomic.AtomicLong</code>. It offers most of AtomicLong&#39;s operations such as <code>get</code>, <code>set</code>, <code>getAndSet</code>, <code>compareAndSet</code> and <code>incrementAndGet</code>. Since IAtomicLong is a distributed implementation, these operations involve remote calls and thus their performances differ from AtomicLong.</p>
<p>The following example code creates an instance, increments it by a million, and prints the count.</p>
<pre><code class="lang-java">public class Member {
  public static void main( String[] args ) {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();     
    IAtomicLong counter = hazelcastInstance.getAtomicLong( &quot;counter&quot; );
    for ( int k = 0; k &lt; 1000 * 1000; k++ ) {
      if ( k % 500000 == 0 ) {
        System.out.println( &quot;At: &quot; + k );
      }
      counter.incrementAndGet();
    }
    System.out.printf( &quot;Count is %s\n&quot;, counter.get() );
  }
}
</code></pre>
<p>When you start other instances with the code above, you will see the count as <em>member count</em> times <em>a million</em>.</p>
<a name="sending-functions-to-iatomiclong"></a><h3 id="sending-functions-to-iatomiclong">Sending Functions to IAtomicLong</h3>
<p>You can send functions to an IAtomicLong. <code>IFunction</code> is a Hazelcast owned, single method interface. The following sample <code>IFunction</code> implementation adds two to the original value.</p>
<pre><code class="lang-java">private static class Add2Function implements IFunction &lt;Long, Long&gt; { 
  @Override
  public Long apply( Long input ) { 
    return input + 2;
  }
}
</code></pre>
<a name="executing-functions-on-iatomiclong"></a><h3 id="executing-functions-on-iatomiclong">Executing Functions on IAtomicLong</h3>
<p>You can use the following methods to execute functions on IAtomicLong.</p>
<ul>
<li><code>apply</code>: Applies the function to the value in IAtomicLong without changing the actual value and returning the result.</li>
<li><code>alter</code>: Alters the value stored in the IAtomicLong by applying the function. It will not send back a result.</li>
<li><code>alterAndGet</code>: Alters the value stored in the IAtomicLong by applying the function, storing the result in the IAtomicLong and returning the result.</li>
<li><code>getAndAlter</code>: Alters the value stored in the IAtomicLong by applying the function and returning the original value.</li>
</ul>
<p>The following sample code includes these methods.</p>
<pre><code class="lang-java">public class Member {
  public static void main( String[] args ) {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();         
    IAtomicLong atomicLong = hazelcastInstance.getAtomicLong( &quot;counter&quot; );

    atomicLong.set( 1 );
    long result = atomicLong.apply( new Add2Function() );         
    System.out.println( &quot;apply.result: &quot; + result);         
    System.out.println( &quot;apply.value: &quot; + atomicLong.get() );

    atomicLong.set( 1 );
    atomicLong.alter( new Add2Function() );             
    System.out.println( &quot;alter.value: &quot; + atomicLong.get() );

    atomicLong.set( 1 );
    result = atomicLong.alterAndGet( new Add2Function() );         
    System.out.println( &quot;alterAndGet.result: &quot; + result );         
    System.out.println( &quot;alterAndGet.value: &quot; + atomicLong.get() );

    atomicLong.set( 1 );
    result = atomicLong.getAndAlter( new Add2Function() );         
    System.out.println( &quot;getAndAlter.result: &quot; + result );         
    System.out.println( &quot;getAndAlter.value: &quot; + atomicLong.get() );
  }
}
</code></pre>
<a name="reasons-to-use-functions-with-iatomic"></a><h3 id="reasons-to-use-functions-with-iatomic">Reasons to Use Functions with IAtomic</h3>
<p>The reason for using a function instead of a simple code line like <code>atomicLong.set(atomicLong.get() + 2));</code> is that the IAtomicLong read and write operations are not atomic. Since <code>IAtomicLong</code> is a distributed implementation, those operations can be remote ones, which may lead to race problems. By using functions, the data is not pulled into the code, but the code is sent to the data. This makes it more scalable.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>IAtomicLong has one synchronous backup and no asynchronous backups. Its backup count is not configurable.</em></p>

<a name="isemaphore"></a><h2 id="isemaphore">ISemaphore</h2>
<p>Hazelcast ISemaphore is the distributed implementation of <code>java.util.concurrent.Semaphore</code>. </p>
<a name="controlling-thread-counts-with-permits"></a><h3 id="controlling-thread-counts-with-permits">Controlling Thread Counts with Permits</h3>
<p>Semaphores offer <strong>permit</strong>s to control the thread counts when performing concurrent activities. To execute a concurrent activity, a thread grants a permit or waits until a permit becomes available. When the execution is completed, the permit is released.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Semaphore with a single permit may be considered a lock. Unlike the locks, however, when semaphores are used, any thread can release the permit, and semaphores can have multiple permits.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Hazelcast ISemaphore does not support fairness at all times. There are some edge cases where the fairness is not honored, e.g., when the permit becomes available at the time when an internal timeout occurs.</em> </p>
<p>When a permit is acquired on ISemaphore:</p>
<ul>
<li>if there are permits, the number of permits in the semaphore is decreased by one and the calling thread performs its activity. If there is contention, the longest waiting thread will acquire the permit before all other threads.</li>
<li>if no permits are available, the calling thread blocks until a permit becomes available. When a timeout happens during this block, the thread is interrupted. When the semaphore
is destroyed, an <code>InstanceDestroyedException</code> is thrown.</li>
</ul>
<a name="example-semaphore-code"></a><h3 id="example-semaphore-code">Example Semaphore Code</h3>
<p>The following example code uses an <code>IAtomicLong</code> resource 1000 times, increments the resource when a thread starts to use it, and decrements it when the thread completes.</p>
<pre><code class="lang-java">public class SemaphoreMember {
  public static void main( String[] args ) throws Exception{
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(); 
    ISemaphore semaphore = hazelcastInstance.getSemaphore( &quot;semaphore&quot; ); 
    IAtomicLong resource = hazelcastInstance.getAtomicLong( &quot;resource&quot; ); 
    for ( int k = 0 ; k &lt; 1000 ; k++ ) {
      System.out.println( &quot;At iteration: &quot; + k + &quot;, Active Threads: &quot; + resource.get() );
      semaphore.acquire();
      try {
        resource.incrementAndGet();
        Thread.sleep( 1000 );
        resource.decrementAndGet();
      } finally { 
        semaphore.release();
      }
    }
    System.out.println(&quot;Finished&quot;);
  }
}
</code></pre>
<p>Let&#39;s limit the concurrent access to this resource by allowing at most three threads. You can configure it declaratively by setting the <code>initial-permits</code> property, as shown below.</p>
<pre><code class="lang-xml">&lt;semaphore name=&quot;semaphore&quot;&gt; 
  &lt;initial-permits&gt;3&lt;/initial-permits&gt;
&lt;/semaphore&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If there is a shortage of permits while the semaphore is being created, value of this property can be set to a negative number.</em></p>
<p>If you execute the above <code>SemaphoreMember</code> class 5 times, the output will be similar to the following:</p>
<p><code>At iteration: 0, Active Threads: 1</code></p>
<p><code>At iteration: 1, Active Threads: 2</code></p>
<p><code>At iteration: 2, Active Threads: 3</code></p>
<p><code>At iteration: 3, Active Threads: 3</code></p>
<p><code>At iteration: 4, Active Threads: 3</code></p>
<p>As you can see, the maximum count of concurrent threads is equal or smaller than three. If you remove the semaphore acquire/release statements in <code>SemaphoreMember</code>, you will see that there is no limitation on the number of concurrent usages.</p>
<p>Hazelcast also provides backup support for <code>ISemaphore</code>. When a member goes down, you can have another member take over the semaphore with the permit information (permits are automatically released when a member goes down). To enable this, configure synchronous or asynchronous backup with the properties <code>backup-count</code> and <code>async-backup-count</code> (by default, synchronous backup is already enabled).</p>
<a name="configuring-semaphore"></a><h3 id="configuring-semaphore">Configuring Semaphore</h3>
<p>The following are example semaphore configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;semaphore name=&quot;semaphore&quot;&gt;
   &lt;backup-count&gt;1&lt;/backup-count&gt;
   &lt;async-backup-count&gt;0&lt;/async-backup-count&gt;
   &lt;initial-permits&gt;3&lt;/initial-permits&gt;
&lt;/semaphore&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
SemaphoreConfig semaphoreConfig = config.getSemaphoreConfig();
semaphoreConfig.setName( &quot;semaphore&quot; ).setBackupCount( &quot;1&quot; )
        .setInitialPermits( &quot;3&quot; );
</code></pre>
<p>Semaphore configuration has the below elements.</p>
<ul>
<li><code>initial-permits</code>: the thread count to which the concurrent access is limited. For example, if you set it to &quot;3&quot;, concurrent access to the object is limited to 3 threads.</li>
<li><code>backup-count</code>: Number of synchronous backups.</li>
<li><code>async-backup-count</code>: Number of asynchronous backups.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If high performance is more important than not losing the permit information, you can disable the backups by setting <code>backup-count</code> to 0.</em></p>
<p><br></br></p>

<a name="iatomicreference"></a><h2 id="iatomicreference">IAtomicReference</h2>
<p>The <code>IAtomicLong</code> is very useful if you need to deal with a long, but in some cases you need to deal with a reference. That is why Hazelcast also supports the <code>IAtomicReference</code> which is the distributed version of the <code>java.util.concurrent.atomic.AtomicReference</code>.</p>
<p>Here is an IAtomicReference example.</p>
<pre><code class="lang-java">public class Member {
    public static void main(String[] args) {
        Config config = new Config();

        HazelcastInstance hz = Hazelcast.newHazelcastInstance(config);

        IAtomicReference&lt;String&gt; ref = hz.getAtomicReference(&quot;reference&quot;);
        ref.set(&quot;foo&quot;);
        System.out.println(ref.get());
        System.exit(0);
    } 
}
</code></pre>
<p>When you execute the above example, you will see the following output.</p>
<p><code>foo</code></p>
<a name="sending-functions-to-iatomicreference"></a><h3 id="sending-functions-to-iatomicreference">Sending Functions to IAtomicReference</h3>
<p>Just like <code>IAtomicLong</code>, <code>IAtomicReference</code> has methods that accept a &#39;function&#39; as an argument, such as <code>alter</code>, <code>alterAndGet</code>, <code>getAndAlter</code> and <code>apply</code>. There are two big advantages of using these methods:</p>
<ul>
<li>From a performance point of view, it is better to send the function to the data then the data to the function. Often the function is a lot smaller than the data and therefore cheaper to send over the line. Also the function only needs to be transferred once to the target machine, and the data needs to be transferred twice.</li>
<li>You do not need to deal with concurrency control. If you would perform a load, transform, store, you could run into a data race since another thread might have updated the value you are about to overwrite. </li>
</ul>
<a name="using-iatomicreference"></a><h3 id="using-iatomicreference">Using IAtomicReference</h3>
<p>Below are some issues you need to know when you use IAtomicReference.</p>
<ul>
<li><code>IAtomicReference</code> works based on the byte-content and not on the object-reference. If you use the <code>compareAndSet</code> method, do not change to original value because its serialized content will then be different. 
It is also important to know that if you rely on Java serialization, sometimes (especially with hashmaps) the same object can result in different binary content.</li>
<li><code>IAtomicReference</code> will always have one synchronous backup.</li>
<li>All methods returning an object will return a private copy. You can modify the private copy, but the rest of the world will be shielded from your changes. If you want these changes to be visible to the rest of the world, you need to write the change back to the <code>IAtomicReference</code>; but be careful about introducing a data-race. </li>
<li>The &#39;in-memory format&#39; of an <code>IAtomicReference</code> is <code>binary</code>. The receiving side does not need to have the class definition available unless it needs to be deserialized on the other side (e.g., because a method like &#39;alter&#39; is executed). This deserialization is done for every call that needs to have the object instead of the binary content, so be careful with expensive object graphs that need to be deserialized.</li>
<li>If you have an object with many fields or an object graph, and you only need to calculate some information or need a subset of fields, you can use the <code>apply</code> method. With the <code>apply</code> method, the whole object does not need to be sent over the line; only the information that is relevant is sent.</li>
</ul>

<a name="icountdownlatch"></a><h2 id="icountdownlatch">ICountDownLatch</h2>
<p>Hazelcast <code>ICountDownLatch</code> is the distributed implementation of <code>java.util.concurrent.CountDownLatch</code>.</p>
<a name="gate-keeping-concurrent-activities"></a><h3 id="gate-keeping-concurrent-activities">Gate-Keeping Concurrent Activities</h3>
<p><code>CountDownLatch</code> is considered to be a gate keeper for concurrent activities. It enables the threads to wait for other threads to complete their operations.</p>
<p>The following code samples describe the mechanism of <code>ICountDownLatch</code>. Assume that there is a leader process and there are follower processes that will wait until the leader completes. Here is the leader:</p>
<pre><code class="lang-java">public class Leader {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ICountDownLatch latch = hazelcastInstance.getCountDownLatch( &quot;countDownLatch&quot; );
    System.out.println( &quot;Starting&quot; );
    latch.trySetCount( 1 );
    Thread.sleep( 30000 );
    latch.countDown();
    System.out.println( &quot;Leader finished&quot; );
    latch.destroy();
  }
}
</code></pre>
<p>Since only a single step is needed to be completed as a sample, the above code initializes the latch with 1. Then, the code sleeps for a while to simulate a process and starts the countdown. Finally, it clears up the latch. Let&#39;s write a follower:</p>
<pre><code class="lang-java">public class Follower {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ICountDownLatch latch = hazelcastInstance.getCountDownLatch( &quot;countDownLatch&quot; );
    System.out.println( &quot;Waiting&quot; );
    boolean success = latch.await( 10, TimeUnit.SECONDS );
    System.out.println( &quot;Complete: &quot; + success );
  }
}
</code></pre>
<p>The follower class above first retrieves <code>ICountDownLatch</code> and then calls the <code>await</code> method to enable the thread to listen for the latch. The method <code>await</code> has a timeout value as a parameter. This is useful when the <code>countDown</code> method fails. To see <code>ICountDownLatch</code> in action, start the leader first and then start one or more followers. You will see that the followers will wait until the leader completes.</p>
<a name="recovering-from-failure"></a><h3 id="recovering-from-failure">Recovering From Failure</h3>
<p>In a distributed environment, the counting down cluster member may go down. In this case, all listeners are notified immediately and automatically by Hazelcast. The state of the current process just before the failure should be verified and &#39;how to continue now&#39; should be decided (e.g. restart all process operations, continue with the first failed process operation, throw an exception, etc.).</p>
<a name="using-icountdownlatch"></a><h3 id="using-icountdownlatch">Using ICountDownLatch</h3>
<p>Although the <code>ICountDownLatch</code> is a very useful synchronization aid, you will probably not use it on a daily basis. Unlike Javas implementation, Hazelcasts <code>ICountDownLatch</code> count can be reset after a countdown has finished, but not during an active count.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>ICountDownLatch has 1 synchronous backup and no asynchronous backups. Its backup count is not configurable. Also, the count cannot be re-set during an active count, it should be re-set after the countdown is finished.</em></p>

<a name="idgenerator"></a><h2 id="idgenerator">IdGenerator</h2>
<p>Hazelcast IdGenerator is used to generate cluster-wide unique identifiers. Generated identifiers are long type primitive values between 0 and <code>Long.MAX_VALUE</code>.</p>
<a name="generating-cluster-wide-ids"></a><h3 id="generating-cluster-wide-ids">Generating Cluster-Wide IDs</h3>
<p>ID generation occurs almost at the speed of <code>AtomicLong.incrementAndGet()</code>. A group of 10,000 identifiers is allocated for each cluster member. In the background, this allocation takes place with an <code>IAtomicLong</code> incremented by 10,000. Once a cluster member generates IDs (allocation is done), <code>IdGenerator</code> increments a local counter. If a cluster member uses all IDs in the group, it will get another 10,000 IDs. This way, only one time of network traffic is needed, meaning that 9,999 identifiers are generated in memory instead of over the network. This is fast.</p>
<p>Let&#39;s write a sample identifier generator.</p>
<pre><code class="lang-java">public class IdGeneratorExample {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    IdGenerator idGen = hazelcastInstance.getIdGenerator( &quot;newId&quot; );
    while (true) {
      Long id = idGen.newId();
      System.err.println( &quot;Id: &quot; + id );
      Thread.sleep( 1000 );
    }
  }
}
</code></pre>
<p>Let&#39;s run the above code two times. The output will be similar to the following.</p>
<pre><code class="lang-plain">Members [1] {
  Member [127.0.0.1]:5701 this
}
Id: 1
Id: 2
Id: 3
</code></pre>
<pre><code class="lang-plain">Members [2] {
  Member [127.0.0.1]:5701
  Member [127.0.0.1]:5702 this
}
Id: 10001
Id: 10002
Id: 10003
</code></pre>
<a name="unique-ids-and-duplicate-ids"></a><h3 id="unique-ids-and-duplicate-ids">Unique IDs and Duplicate IDs</h3>
<p>You can see that the generated IDs are unique and counting upwards. If you see duplicated identifiers, it means your instances could not form a cluster. </p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Generated IDs are unique during the life cycle of the cluster. If the entire cluster is restarted, IDs start from 0, again or you can initialize to a value using the <code>init()</code> method of IdGenerator.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>IdGenerator has one synchronous backup and no asynchronous backups. Its backup count is not configurable.</em></p>

<a name="replicated-map"></a><h2 id="replicated-map">Replicated Map</h2>
<p>A Replicated Map is a distributed key-value data structure where the data is replicated to all members in the cluster. It provides full replication of entries to all members for high speed access. The following are its features:</p>
<ul>
<li>When you have a Replicated Map in the  cluster, your clients can communicate with any cluster member.</li>
<li>All cluster members are able to perform write operations.</li>
<li>It supports all methods of the interface <code>java.util.Map</code>.</li>
<li>It supports automatic initial fill up when a new member is started.</li>
<li>It provides statistics for entry access, write and update so that you can monitor it using Hazelcast Management Center.</li>
<li>New members joining to the cluster pull all the data from the existing members.</li>
<li>You can listen to entry events using listeners. Please refer to <a href="#using-entrylistener-on-replicated-map">Using EntryListener on Replicated Map</a>. </li>
</ul>
<a name="replicating-instead-of-partitioning"></a><h3 id="replicating-instead-of-partitioning">Replicating Instead of Partitioning</h3>
<p>A Replicated Map does not partition data
(it does not spread data to different cluster members); instead, it replicates the data to all members. All other data structures are partitioned in design. </p>
<p>Replication leads to higher memory consumption. However, a Replicated Map has faster read and write access since the data is available on all members.</p>
<p>Writes could take place on local/remote members in order to provide write-order, eventually being replicated to all other members.</p>
<p>Replicated Map is suitable for objects, catalog data, or idempotent calculable data (such as HTML pages). It fully implements the <code>java.util.Map</code> interface, but it lacks the methods from <code>java.util.concurrent.ConcurrentMap</code> since
there are no atomic guarantees to writes or reads.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If Replicated Map is used from a dummy client and this dummy client is connected to a lite member, the entry listeners cannot be registered/de-registered.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You cannot use Replicated Map from a lite member. A <code>com.hazelcast.replicatedmap.ReplicatedMapCantBeCreatedOnLiteMemberException</code> is thrown if <code>com.hazelcast.core.HazelcastInstance#getReplicatedMap(name)</code> is invoked on a lite member.</em></p>
<a name="example-replicated-map-code"></a><h3 id="example-replicated-map-code">Example Replicated Map Code</h3>
<p>Here is an example of Replicated Map code. The HazelcastInstance&#39;s <code>getReplicatedMap</code> method gets the Replicated Map, and the Replicated Map&#39;s <code>put</code> method creates map entries.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import java.util.Collection;
import java.util.Map;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Map&lt;String, Customer&gt; customers = hazelcastInstance.getReplicatedMap(&quot;customers&quot;);
customers.put( &quot;1&quot;, new Customer( &quot;Joe&quot;, &quot;Smith&quot; ) );
customers.put( &quot;2&quot;, new Customer( &quot;Ali&quot;, &quot;Selam&quot; ) );
customers.put( &quot;3&quot;, new Customer( &quot;Avi&quot;, &quot;Noyan&quot; ) );

Collection&lt;Customer&gt; colCustomers = customers.values();
for ( Customer customer : colCustomers ) {
  // process customer
}
</code></pre>
<p><code>HazelcastInstance::getReplicatedMap</code> returns <code>com.hazelcast.core.ReplicatedMap</code> which, as stated above, extends the
<code>java.util.Map</code> interface.</p>
<p>The <code>com.hazelcast.core.ReplicatedMap</code> interface has some additional methods for registering entry listeners or retrieving values in an expected order.</p>
<a name="considerations-for-replicated-map"></a><h3 id="considerations-for-replicated-map">Considerations for Replicated Map</h3>
<p>If you have a large cluster or very high occurrences of updates, the Replicated Map may not scale linearly as expected since it has to replicate update operations to all members in the cluster.</p>
<p>Since the replication of updates is performed in an asynchronous manner, we recommend you enable back pressure in case your system has high occurrences of updates. Please refer to the <a href="#back-pressure">Back Pressure</a> section to learn how to enable it.</p>
<p>Replicated Map has an anti-entropy system that will converge values to a common one if some of the members are missing replication updates.</p>
<p>Replicated Map does not guarantee eventual consistency because there are some edge cases that fail to provide consistency.</p>
<p>Replicated Map uses the internal partition system of Hazelcast in order to serialize updates happening on the same key at the same time. This happens by sending updates of the same key to the same Hazelcast member in the cluster.</p>
<p>Due to the asynchronous nature of replication, a Hazelcast member could die before successfully replicating a &quot;write&quot; operation to other members after sending the &quot;write completed&quot; response to its caller during the write process. In this scenario, Hazelcast&#39;s internal partition system will promote one of the replicas of the partition as the primary one. The new primary partition will not have the latest &quot;write&quot; since the dead member could not successfully replicate the update. (This will leave the system in a state that the caller is the only one that has the update and the rest of the cluster have not.) In this case even the anti-entropy system simply could not converge the value since the source of true information is lost for the update. This leads to a break in the eventual consistency because different values can be read from the system for the same key.</p>
<p>Other than the aforementioned scenario, the Replicated Map will behave like an eventually consistent system with read-your-writes consistency.</p>
<a name="configuration-design-for-replicated-map"></a><h3 id="configuration-design-for-replicated-map">Configuration Design for Replicated Map</h3>
<p>There are several technical design decisions you should consider when you configure a Replicated Map.</p>
<p><strong>Initial Provisioning</strong></p>
<p>If a new member joins the cluster, there are two ways you can handle the initial provisioning that is executed to replicate all existing
values to the new member. Each involves how you configure the async fill up.</p>
<p>First, you can configure async fill up to true, which does not block reads while the fill up operation is underway. That way,
you have immediate access on the new member, but it will take time until all the values are eventually accessible. Not yet
replicated values are returned as non-existing (null).</p>
<p>Second, you can configure for a synchronous initial fill up (by configuring the async fill up to false), which blocks every read or write access to the map until the
fill up operation is finished. Use this with caution since it might block your application from operating.</p>
<a name="configuring-replicated-map"></a><h3 id="configuring-replicated-map">Configuring Replicated Map</h3>
<p>Replicated Map can be configured programmatically or declaratively.</p>
<a name="replicated-map-declarative-configuration"></a><h4 id="replicated-map-declarative-configuration">Replicated Map Declarative Configuration</h4>
<p>You can declare your Replicated Map configuration in the Hazelcast configuration file <code>hazelcast.xml</code>. Please see the following example.</p>
<pre><code class="lang-xml">&lt;replicatedmap name=&quot;default&quot;&gt;
  &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
  &lt;async-fillup&gt;true&lt;/async-fillup&gt;
  &lt;statistics-enabled&gt;true&lt;/statistics-enabled&gt;
  &lt;entry-listeners&gt;
    &lt;entry-listener include-value=&quot;true&quot;&gt;
      com.hazelcast.examples.EntryListener
    &lt;/entry-listener&gt;
  &lt;/entry-listeners&gt;
&lt;/replicatedmap&gt;
</code></pre>
<ul>
<li><code>in-memory-format</code>: Internal storage format.  Please see the <a href="#in-memory-format-on-replicated-map">In-Memory Format section</a>. The default value is <code>OBJECT</code>.</li>
<li><code>async-fillup</code>: Specifies whether the Replicated Map is available for reads before the initial replication is completed. The default value is <code>true</code>. If set to <code>false</code> (i.e., synchronous initial fill up), no exception will be thrown when the Replicated Map is not yet ready, but <code>null</code> values can be seen until the initial replication is completed.</li>
<li><code>statistics-enabled</code>: If set to <code>true</code>, the statistics such as cache hits and misses are collected. The default value is <code>true</code>.</li>
<li><code>entry-listener</code>: Full canonical classname of the <code>EntryListener</code> implementation.<ul>
<li><code>entry-listener#include-value</code>: Specifies whether the event includes the value or not. Sometimes the key is enough to react on an event. In those situations, setting this value to <code>false</code> will save a deserialization cycle. The default value is <code>true</code>.</li>
<li><code>entry-listener#local</code>: Not used for Replicated Map since listeners are always local.</li>
</ul>
</li>
</ul>
<a name="replicated-map-programmatic-configuration"></a><h4 id="replicated-map-programmatic-configuration">Replicated Map Programmatic Configuration</h4>
<p>You can configure a Replicated Map programmatically, as you can do for all other data structures in Hazelcast. You must create the configuration upfront, when you instantiate the <code>HazelcastInstance</code>.
A basic example of how to configure the Replicated Map using the programmatic approach is shown in the following snippet.</p>
<pre><code class="lang-java">Config config = new Config();

ReplicatedMapConfig replicatedMapConfig =
    config.getReplicatedMapConfig( &quot;default&quot; );

replicatedMapConfig.setInMemoryFormat( InMemoryFormat.BINARY );
</code></pre>
<p>All properties that can be configured using the declarative configuration are also available using programmatic configuration
by transforming the tag names into getter or setter names.</p>
<a name="in-memory-format-on-replicated-map"></a><h4 id="in-memory-format-on-replicated-map">In-Memory Format on Replicated Map</h4>
<p>Currently, two <code>in-memory-format</code> values are usable with the Replicated Map.</p>
<ul>
<li><p><code>OBJECT</code> (default): The data will be stored in deserialized form. This configuration is the default choice since
the data replication is mostly used for high speed access. Please be aware that changing the values without a <code>Map::put</code> is
not reflected on the other members but is visible on the changing members for later value accesses.</p>
</li>
<li><p><code>BINARY</code>: The data is stored in serialized binary format and has to be deserialized on every request. This
option offers higher encapsulation since changes to values are always discarded as long as the newly changed object is
not explicitly <code>Map::put</code> into the map again.</p>
</li>
</ul>
<a name="using-entrylistener-on-replicated-map"></a><h3 id="using-entrylistener-on-replicated-map">Using EntryListener on Replicated Map</h3>
<p>A <code>com.hazelcast.core.EntryListener</code> used on a Replicated Map serves the same purpose as it would on other
data structures in Hazelcast. You can use it to react on add, update, and remove operations. Replicated Maps do not yet support eviction.</p>
<a name="difference-in-entrylistener-on-replicated-map"></a><h4 id="difference-in-entrylistener-on-replicated-map">Difference in EntryListener on Replicated Map</h4>
<p>The fundamental difference in Replicated Map behavior, compared to the other data structures, is that an EntryListener only reflects
changes on local data. Since replication is asynchronous, all listener events are fired only when an operation is finished
on a local member. Events can fire at different times on different members.</p>
<a name="example-of-replicated-map-entrylistener"></a><h4 id="example-of-replicated-map-entrylistener">Example of Replicated Map EntryListener</h4>
<p>Here is a code example for using EntryListener on a Replicated Map.</p>
<p>The <code>HazelcastInstance</code>&#39;s <code>getReplicatedMap</code> method gets a Replicated Map (customers), and the <code>ReplicatedMap</code>&#39;s <code>addEntryListener</code> method adds an entry listener to the Replicated Map. Then, the <code>ReplicatedMap</code>&#39;s <code>put</code> method adds a Replicated Map
entry and updates it. The method <code>remove</code> removes the entry.</p>
<pre><code class="lang-java">import com.hazelcast.core.EntryEvent;
import com.hazelcast.core.EntryListener;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.ReplicatedMap;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
ReplicatedMap&lt;String, Customer&gt; customers =
    hazelcastInstance.getReplicatedMap( &quot;customers&quot; );

customers.addEntryListener( new EntryListener&lt;String, Customer&gt;() {
  @Override
  public void entryAdded( EntryEvent&lt;String, Customer&gt; event ) {
    log( &quot;Entry added: &quot; + event );
  }

  @Override
  public void entryUpdated( EntryEvent&lt;String, Customer&gt; event ) {
    log( &quot;Entry updated: &quot; + event );
  }

  @Override
  public void entryRemoved( EntryEvent&lt;String, Customer&gt; event ) {
    log( &quot;Entry removed: &quot; + event );
  }

  @Override
  public void entryEvicted( EntryEvent&lt;String, Customer&gt; event ) {
    // Currently not supported, will never fire
  }
});

customers.put( &quot;1&quot;, new Customer( &quot;Joe&quot;, &quot;Smith&quot; ) ); // add event
customers.put( &quot;1&quot;, new Customer( &quot;Ali&quot;, &quot;Selam&quot; ) ); // update event
customers.remove( &quot;1&quot; ); // remove event
</code></pre>

<a name="distributed-events"></a><h1 id="distributed-events">Distributed Events</h1>
<p>You can register for Hazelcast entry events so you will be notified when those events occur. Event Listeners are cluster-wide--when a listener is registered in one member of cluster, it is actually registered for events that originated at any member in the cluster. When a new member joins, events originated at the new member will also be delivered.</p>
<p>An Event is created only if you registered an event listener. If no listener is registered, then no event will be created. If you provided a predicate when you registered the event listener, pass the predicate before sending the event to the listener (member/client).</p>
<p>As a rule of thumb, your event listener should not implement heavy processes in its event methods that block the thread for a long time. If needed, you can use <code>ExecutorService</code> to transfer long running processes to another thread and thus offload the current listener thread.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>In a failover scenario, events are not highly available and may get lost. Eventing mechanism is being improved for failover scenarios.</em></p>
<a name="event-listeners-for-hazelcast-members"></a><h2 id="event-listeners-for-hazelcast-members">Event Listeners for Hazelcast Members</h2>
<p>Hazelcast offers the following event listeners:</p>
<ul>
<li><strong>Membership Listener</strong> for cluster membership events.</li>
<li><strong>Distributed Object Listener</strong> for distributed object creation and destroy events.</li>
<li><strong>Migration Listener</strong> for partition migration start and complete events.</li>
<li><strong>Partition Lost Listener</strong> for partition lost events.</li>
<li><strong>Lifecycle Listener</strong> for <code>HazelcastInstance</code> lifecycle events.</li>
<li><strong>Entry Listener</strong> for <code>IMap</code> and <code>MultiMap</code> entry events.</li>
<li><strong>Item Listener</strong> for <code>IQueue</code>, <code>ISet</code> and <code>IList</code> item events.</li>
<li><strong>Message Listener</strong> for <code>ITopic</code> message events.</li>
<li><strong>Client Listener</strong> for client connection events.</li>
</ul>

<a name="listening-for-member-events"></a><h3 id="listening-for-member-events">Listening for Member Events</h3>
<p>The Membership Listener interface has methods that are invoked for the following events.</p>
<ul>
<li><code>memberAdded</code>: A new member is added to the cluster.</li>
<li><code>memberRemoved</code>: An existing member leaves the cluster.</li>
<li><code>memberAttributeChanged</code>: An attribute of a member is changed. Please refer to <a href="#defining-member-attributes">Defining Member Attributes</a> to learn about member attributes.</li>
</ul>
<p>To write a Membership Listener class, you implement the MembershipListener interface and its methods.</p>
<p>The following is an example Membership Listener class.</p>
<pre><code class="lang-java">public class ClusterMembershipListener
     implements MembershipListener {

public void memberAdded(MembershipEvent membershipEvent) {
  System.err.println(&quot;Added: &quot; + membershipEvent);
}

public void memberRemoved(MembershipEvent membershipEvent) {
       System.err.println(&quot;Removed: &quot; + membershipEvent);
     }

public void memberAttributeChanged(MemberAttributeEvent memberAttributeEvent) {
       System.err.println(&quot;Member attribute changed: &quot; + memberAttributeEvent);
     }

}
</code></pre>
<p>When a respective event is fired, the membership listener outputs the addresses of the members that joined and left, and also which attribute changed on which member.</p>
<a name="registering-membership-listeners"></a><h4 id="registering-membership-listeners">Registering Membership Listeners</h4>
<p>After you create your class, you can configure your cluster to include the membership listener. Below is an example using the method <code>addMembershipListener</code>.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
hazelcastInstance.getCluster().addMembershipListener( new ClusterMembershipListener() );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">Config config = new Config();
config.addListenerConfig(
new ListenerConfig( &quot;com.your-package.ClusterMembershipListener&quot; ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;listeners&gt;
      &lt;listener type=&quot;membership-listener&quot;&gt;
         com.your-package.ClusterMembershipListener
      &lt;/listener&gt;
   &lt;/listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:listeners&gt;
 &lt;hz:listener class-name=&quot;com.your-package.ClusterMembershipListener&quot;/&gt;
 &lt;hz:listener implementation=&quot;MembershipListener&quot;/&gt;
&lt;/hz:listeners&gt;
</code></pre>
<a name="listening-for-distributed-object-events"></a><h3 id="listening-for-distributed-object-events">Listening for Distributed Object Events</h3>
<p>The Distributed Object Listener methods <code>distributedObjectCreated</code> and <code>distributedObjectDestroyed</code> are invoked when a distributed object is created and destroyed throughout the cluster. To write a Distributed Object Listener class, you implement the DistributedObjectListener interface and its methods.</p>
<p>The following is an example Distributed Object Listener class.</p>
<pre><code class="lang-java">public class SampleDistObjListener implements DistributedObjectListener {
  public static void main(String[] args) {
    SampleDistObjListener sample = new SampleDistObjListener();

    Config config = new Config();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(config);
    hazelcastInstance.addDistributedObjectListener(sample);

    Collection&lt;DistributedObject&gt; distributedObjects = hazelcastInstance.getDistributedObjects();
    for (DistributedObject distributedObject : distributedObjects) {
      System.out.println(distributedObject.getName() + &quot;,&quot; + distributedObject.getId());
    }
  }

  @Override
  public void distributedObjectCreated(DistributedObjectEvent event) {
    DistributedObject instance = event.getDistributedObject();
    System.out.println(&quot;Created &quot; + instance.getName() + &quot;,&quot; + instance.getId());
  }

  @Override
  public void distributedObjectDestroyed(DistributedObjectEvent event) {
    DistributedObject instance = event.getDistributedObject();
    System.out.println(&quot;Destroyed &quot; + instance.getName() + &quot;,&quot; + instance.getId());
  }
}
</code></pre>
<p>When a respective event is fired, the distributed object listener outputs the event type, and the name, service (for example, if a Map service provides the distributed object, than it is a Map object), and ID of the object.</p>
<a name="registering-distributed-object-listeners"></a><h4 id="registering-distributed-object-listeners">Registering Distributed Object Listeners</h4>
<p>After you create your class, you can configure your cluster to include distributed object listeners. Below is an example using the method <code>addDistributedObjectListener</code>. You can also see this portion in the above class creation.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
SampleDistObjListener sample = new SampleDistObjListener();

hazelcastInstance.addDistributedObjectListener( sample );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register the listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">config.addListenerConfig(
new ListenerConfig( &quot;com.your-package.SampleDistObjListener&quot; ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;listeners&gt;
      &lt;listener&gt;
      com.your-package.SampleDistObjListener
      &lt;/listener&gt;
   &lt;/listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:listeners&gt;
   &lt;hz:listener class-name=&quot;com.your-package.SampleDistObjListener&quot;/&gt;
   &lt;hz:listener implementation=&quot;DistributedObjectListener&quot;/&gt;
&lt;/hz:listeners&gt;
</code></pre>
<a name="listening-for-migration-events"></a><h3 id="listening-for-migration-events">Listening for Migration Events</h3>
<p>The Migration Listener interface has methods that are invoked for the following events:</p>
<ul>
<li><code>migrationStarted</code>: A partition migration is started.</li>
<li><code>migrationCompleted</code>: A partition migration is completed.</li>
<li><code>migrationFailed</code>: A partition migration failed.</li>
</ul>
<p>To write a Migration Listener class, you implement the DistributedObjectListener interface and its methods.</p>
<p>The following is an example Migration Listener class.</p>
<pre><code class="lang-java">public class ClusterMigrationListener implements MigrationListener {
     @Override
     public void migrationStarted(MigrationEvent migrationEvent) {
       System.err.println(&quot;Started: &quot; + migrationEvent);
     }
    @Override
     public void migrationCompleted(MigrationEvent migrationEvent) {
       System.err.println(&quot;Completed: &quot; + migrationEvent);
     }
     @Override
     public void migrationFailed(MigrationEvent migrationEvent) {
       System.err.println(&quot;Failed: &quot; + migrationEvent);
     }
}
</code></pre>
<p>When a respective event is fired, the migration listener outputs the partition ID, status of the migration, the old member and the new member. The following is an example output.</p>
<pre><code>Started: MigrationEvent{partitionId=98, oldOwner=Member [127.0.0.1]:5701,
newOwner=Member [127.0.0.1]:5702 this}
</code></pre><a name="registering-migration-listeners"></a><h4 id="registering-migration-listeners">Registering Migration Listeners</h4>
<p>After you create your class, you can configure your cluster to include migration listeners. Below is an example using the method <code>addMigrationListener</code>.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

PartitionService partitionService = hazelcastInstance.getPartitionService();
partitionService.addMigrationListener( new ClusterMigrationListener );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register the listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">config.addListenerConfig( 
new ListenerConfig( &quot;com.your-package.ClusterMigrationListener&quot; ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;listeners&gt;
      &lt;listener&gt;
      com.your-package.ClusterMigrationListener
      &lt;/listener&gt;
   &lt;/listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:listeners&gt;
   &lt;hz:listener class-name=&quot;com.your-package.ClusterMigrationListener&quot;/&gt;
   &lt;hz:listener implementation=&quot;MigrationListener&quot;/&gt;
&lt;/hz:listeners&gt;
</code></pre>
<a name="listening-for-partition-lost-events"></a><h3 id="listening-for-partition-lost-events">Listening for Partition Lost Events</h3>
<p>Hazelcast provides fault-tolerance by keeping multiple copies of your data. For each partition, one of your cluster members becomes the owner and some of the other members become replica members, based on your configuration. Nevertheless, data loss may occur if a few members crash simultaneously.</p>
<p>Let`s consider the following example with three members: N1, N2, N3 for a given partition-0. N1 is owner of partition-0, and N2 and N3 are the first and second replicas respectively. If N1 and N2 crash simultaneously, partition-0 loses its data that is configured with less than two backups.
For instance, if we configure a map with one backup, that map loses its data in partition-0 since both owner and first replica of partition-0 have crashed. However, if we configure our map with two backups, it does not lose any data since a copy of partition-0&#39;s data for the given map
also resides in N3. </p>
<p>The Partition Lost Listener notifies for possible data loss occurrences with the information of how many replicas are lost for a partition. It listens to <code>PartitionLostEvent</code> instances. Partition lost events are dispatched per partition. </p>
<p>Partition loss detection is done after a member crash is detected by the other members and the crashed member is removed from the cluster. Please note that false-positive <code>PartitionLostEvent</code> instances may be fired on the network split errors. </p>
<a name="writing-a-partition-lost-listener-class"></a><h4 id="writing-a-partition-lost-listener-class">Writing a Partition Lost Listener Class</h4>
<p>To write a Partition Lost Listener, you implement the PartitionLostListener interface and its <code>partitionLost</code> method, which is invoked when a partition loses its owner and all backups.</p>
<p>The following is an example Partition Lost Listener class. </p>
<pre><code class="lang-java">    public class ConsoleLoggingPartitionLostListener implements PartitionLostListener {
        @Override
        public void partitionLost(PartitionLostEvent event) {
            System.out.println(event);
        }
    }
</code></pre>
<p>When a <code>PartitionLostEvent</code> is fired, the partition lost listener given above outputs the partition ID, the replica index that is lost, and the member that has detected the partition loss. The following is an example output.</p>
<pre><code>com.hazelcast.partition.PartitionLostEvent{partitionId=242, lostBackupCount=0, 
eventSource=Address[192.168.2.49]:5701}
</code></pre><a name="registering-partition-lost-listeners"></a><h4 id="registering-partition-lost-listeners">Registering Partition Lost Listeners</h4>
<p>After you create your class, you can configure your cluster programmatically or declaratively to include the partition lost listener. Below is an example of its programmatic configuration.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
hazelcastInstance.getPartitionService().addPartitionLostListener( new ConsoleLoggingPartitionLostListener() );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
  &lt;partition-lost-listeners&gt;
     &lt;partition-lost-listener&gt;
        com.your-package.ConsoleLoggingPartitionLostListener
     &lt;/partition-lost-listener&gt;
 &lt;/partition-lost-listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>

<a name="listening-for-lifecycle-events"></a><h3 id="listening-for-lifecycle-events">Listening for Lifecycle Events</h3>
<p>The Lifecycle Listener notifies for the following events:</p>
<ul>
<li><code>STARTING</code>: A member is starting.</li>
<li><code>STARTED</code>: A member started.</li>
<li><code>SHUTTING_DOWN</code>: A member is shutting down.</li>
<li><code>SHUTDOWN</code>: A member&#39;s shutdown has completed.</li>
<li><code>MERGING</code>: A member is merging with the cluster.</li>
<li><code>MERGED</code>: A member&#39;s merge operation has completed.</li>
<li><code>CLIENT_CONNECTED</code>: A Hazelcast Client connected to the cluster.</li>
<li><code>CLINET_DISCONNECTED</code>: A Hazelcast Client disconnected from the cluster.</li>
</ul>
<p>The following is an example Lifecycle Listener class.</p>
<pre><code class="lang-java">public class NodeLifecycleListener implements LifecycleListener {
     @Override
     public void stateChanged(LifecycleEvent event) {
       System.err.println(event);
     }
}
</code></pre>
<p>This listener is local to an individual member. It notifies the application that uses Hazelcast about the events mentioned above for a particular member. </p>
<a name="registering-lifecycle-listeners"></a><h4 id="registering-lifecycle-listeners">Registering Lifecycle Listeners</h4>
<p>After you create your class, you can configure your cluster to include lifecycle listeners. Below is an example using the method <code>addLifecycleListener</code>.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
hazelcastInstance.getLifecycleService().addLifecycleListener( new NodeLifecycleListener() );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register the listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">config.addListenerConfig(
new ListenerConfig( &quot;com.your-package.NodeLifecycleListener&quot; ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;listeners&gt;
      &lt;listener&gt;
      com.your-package.NodeLifecycleListener
      &lt;/listener&gt;
   &lt;/listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:listeners&gt;
   &lt;hz:listener class-name=&quot;com.your-package.NodeLifecycleListener&quot;/&gt;
   &lt;hz:listener implementation=&quot;LifecycleListener&quot;/&gt;
&lt;/hz:listeners&gt;
</code></pre>
<a name="listening-for-map-events"></a><h3 id="listening-for-map-events">Listening for Map Events</h3>
<p>You can listen to map-wide or entry-based events using the listeners provided by the Hazelcast&#39;s eventing framework. To listen to these events, implement a <code>MapListener</code> sub-interface.</p>
<p>A map-wide event is fired as a result of a map-wide operation. For 
example, <code>IMap#clear</code> or <code>IMap#evictAll</code>.
An entry-based event is fired after the operations that affect a 
specific entry. For example, <code>IMap#remove</code> or <code>IMap#evict</code>.</p>
<a name="catching-a-map-event"></a><h4 id="catching-a-map-event">Catching a Map Event</h4>
<p>To catch an event, you should explicitly 
implement a corresponding sub-interface of a <code>MapListener</code>, 
such as <code>EntryAddedListener</code> or <code>MapClearedListener</code>. </p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em> The <code>EntryListener</code> interface still can be implemented (we kept
it for backward compatibility reasons). However, if you need to listen to a 
different event, one that is not available in the <code>EntryListener</code> interface, you should also 
implement a relevant <code>MapListener</code> sub-interface.</em>
<br></br></p>
<p>Let&#39;s take a look at the following class example.</p>
<pre><code class="lang-java">public class Listen {

  public static void main( String[] args ) {
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    IMap&lt;String, String&gt; map = hz.getMap( &quot;somemap&quot; );
    map.addEntryListener( new MyEntryListener(), true );
     System.out.println( &quot;EntryListener registered&quot; );
  }

  static class MyEntryListener implements EntryAddedListener&lt;String, String&gt;, 
                                          EntryRemovedListener&lt;String, String&gt;, 
                                          EntryUpdatedListener&lt;String, String&gt;, 
                                          EntryEvictedListener&lt;String, String&gt; , 
                                          MapEvictedListener, 
                                          MapClearedListener   {
    @Override
    public void entryAdded( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Added:&quot; + event );
    }

    @Override
    public void entryRemoved( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Removed:&quot; + event );
    }

    @Override
    public void entryUpdated( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Updated:&quot; + event );
    }

    @Override
    public void entryEvicted( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Evicted:&quot; + event );
    }

    @Override
    public void mapEvicted( MapEvent event ) {
      System.out.println( &quot;Map Evicted:&quot; + event );
    }

    @Override
    public void mapCleared( MapEvent event ) {
      System.out.println( &quot;Map Cleared:&quot; + event );
    }

  }
}
</code></pre>
<p>Now, let&#39;s perform some modifications on the map entries using the following example code.</p>
<pre><code class="lang-java">public class Modify {

  public static void main( String[] args ) {
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    IMap&lt;String, String&gt; map = hz.getMap( &quot;somemap&quot;);
    String key = &quot;&quot; + System.nanoTime();
    String value = &quot;1&quot;;
    map.put( key, value );
    map.put( key, &quot;2&quot; );
    map.delete( key );
  }
}
</code></pre>
<p>If you execute the <code>Listen</code> class and then the <code>Modify</code> class, you get the following output 
produced by the <code>Listen</code> class. </p>
<pre><code>entryAdded:EntryEvent {Address[192.168.1.100]:5702} key=251359212222282,
    oldValue=null, value=1, event=ADDED, by Member [192.168.1.100]:5702

entryUpdated:EntryEvent {Address[192.168.1.100]:5702} key=251359212222282,
    oldValue=1, value=2, event=UPDATED, by Member [192.168.1.100]:5702

entryRemoved:EntryEvent {Address[192.168.1.100]:5702} key=251359212222282,
    oldValue=2, value=2, event=REMOVED, by Member [192.168.1.100]:5702
</code></pre><pre><code class="lang-java">public class MyEntryListener implements EntryListener{

    private Executor executor = Executors.newFixedThreadPool(5);

    @Override
    public void entryAdded(EntryEvent event) {
        executor.execute(new DoSomethingWithEvent(event));
    }
...
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Please note that the method <code>IMap.clear()</code> does not fire an &quot;EntryRemoved&quot; event, but fires a &quot;MapCleared&quot; event.</em>
<br></br></p>
<a name="partitions-and-entry-listeners"></a><h4 id="partitions-and-entry-listeners">Partitions and Entry Listeners</h4>
<p>A map listener runs on the event threads that are also used by the other listeners. For 
example, the collection listeners and pub/sub message listeners. This means that the entry 
listeners can access other partitions. Consider this when you run long tasks, since listening 
to those tasks may cause the other map/event listeners to starve.</p>
<a name="listening-for-lost-map-partitions"></a><h4 id="listening-for-lost-map-partitions">Listening for Lost Map Partitions</h4>
<p>You can listen to <code>MapPartitionLostEvent</code> instances by registering an implementation 
of <code>MapPartitionLostListener</code>, which is also a sub-interface of <code>MapListener</code>.</p>
<p>Let`s consider the following example code:</p>
<pre><code class="lang-java">  public static void main(String[] args) {
    Config config = new Config();
    config.getMapConfig(&quot;map&quot;).setBackupCount(1); // might lose data if any member crashes

    HazelcastInstance instance = HazelcastInstanceFactory.newHazelcastInstance(config);

    IMap&lt;Object, Object&gt; map = instance1.getMap(&quot;map&quot;);
    map.put(0, 0);

    map.addPartitionLostListener(new MapPartitionLostListener() {
      @Override
      public void partitionLost(MapPartitionLostEvent event) {
        System.out.println(event);
      }
    });
  }
</code></pre>
<p>Within this example code, a <code>MapPartitionLostListener</code> implementation is registered to a map 
that is configured with one backup. For this particular map and any of the partitions in the 
system, if the partition owner member and its first backup member crash simultaneously, the 
given <code>MapPartitionLostListener</code> receives a 
corresponding <code>MapPartitionLostEvent</code>. If only a single member crashes in the cluster, 
there will be no <code>MapPartitionLostEvent</code> fired for this map since backups for the partitions 
owned by the crashed member are kept on other members. </p>
<p>Please refer to <a href="#listening-for-partition-lost-events">Listening for Partition Lost Events</a> for more 
information about partition lost detection and partition lost events.</p>
<a name="registering-map-listeners"></a><h4 id="registering-map-listeners">Registering Map Listeners</h4>
<p>After you create your listener class, you can configure your cluster to include map listeners using the method <code>addEntryListener</code> (as you can see in the example <code>Listen</code> class above). Below is the related portion from this code, showing how to register a map listener.</p>
<pre><code class="lang-java">HazelcastInstance hz = Hazelcast.newHazelcastInstance();
IMap&lt;String, String&gt; map = hz.getMap( &quot;somemap&quot; );
map.addEntryListener( new MyEntryListener(), true );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register listeners in configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">mapConfig.addEntryListenerConfig(
new EntryListenerConfig( &quot;com.yourpackage.MyEntryListener&quot;, 
                                     false, false ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;map name=&quot;somemap&quot;&gt;
   ...
      &lt;entry-listeners&gt;
         &lt;entry-listener include-value=&quot;false&quot; local=&quot;false&quot;&gt;
         com.your-package.MyEntryListener
         &lt;/entry-listener&gt;
      &lt;/entry-listeners&gt;
   &lt;/map&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:map name=&quot;somemap&quot;&gt;
   &lt;hz:entry-listeners&gt;
      &lt;hz:entry-listener include-value=&quot;true&quot;
      class-name=&quot;com.hazelcast.spring.DummyEntryListener&quot;/&gt;
      &lt;hz:entry-listener implementation=&quot;dummyEntryListener&quot; local=&quot;true&quot;/&gt;
   &lt;/hz:entry-listeners&gt;
&lt;/hz:map&gt;
</code></pre><a name="map-listener-attributes"></a><h4 id="map-listener-attributes">Map Listener Attributes</h4>
<p>As you see, there are attributes of the map listeners in the above examples: <code>include-value</code> and <code>local</code>. The attribute <code>include-value</code> is a boolean attribute that is optional, and if you set it to <code>true</code>, the map event will contain the map value. Its default value is <code>true</code>.</p>
<p>The attribute <code>local</code> is also a boolean attribute that is optional, and if you set it to <code>true</code>, you can listen to the map on the local member. Its default value is <code>false</code>.</p>

<a name="listening-for-multimap-events"></a><h3 id="listening-for-multimap-events">Listening for MultiMap Events</h3>
<p>You can listen to entry-based events in the MultiMap using <code>EntryListener</code>. The following is an example listener class for MultiMap.</p>
<pre><code class="lang-java">public class Listen {

  public static void main( String[] args ) {
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    MultiMap&lt;String, String&gt; map = hz.getMultiMap( &quot;somemap&quot; );
    map.addEntryListener( new MyEntryListener(), true );
    System.out.println( &quot;EntryListener registered&quot; );
  }

  static class SampleEntryListener implements EntryListener&lt;String, String&gt;{
    @Override
    public void entryAdded( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Added:&quot; + event );
    }

    @Override
    public void entryRemoved( EntryEvent&lt;String, String&gt; event ) {
      System.out.println( &quot;Entry Removed:&quot; + event );
    }
  }
}
</code></pre>
<a name="registering-multimap-listeners"></a><h4 id="registering-multimap-listeners">Registering MultiMap Listeners</h4>
<p>After you create your listener class, you can configure your cluster to include MultiMap listeners using the method <code>addEntryListener</code> (as you can see in the example <code>Listen</code> class above). Below is the related portion from this code, showing how to register a map listener.</p>
<pre><code class="lang-java">HazelcastInstance hz = Hazelcast.newHazelcastInstance();
MultiMap&lt;String, String&gt; map = hz.getMultiMap( &quot;somemap&quot; );
map.addEntryListener( new MyEntryListener(), true );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">multiMapConfig.addEntryListenerConfig(
new EntryListenerConfig( &quot;com.your-package.SampleEntryListener&quot;,
                                     false, false ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;multimap name=&quot;somemap&quot;&gt;
      &lt;value-collection-type&gt;SET&lt;/value-collection-type&gt;
      &lt;entry-listeners&gt;
         &lt;entry-listener include-value=&quot;false&quot; local=&quot;false&quot;&gt;
            com.your-package.SampleEntryListener
         &lt;/entry-listener&gt;
      &lt;/entry-listeners&gt;
   &lt;/multimap&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:multimap name=&quot;default&quot; value-collection-type=&quot;LIST&quot;&gt;
   &lt;hz:entry-listeners&gt;
      &lt;hz:entry-listener include-value=&quot;false&quot;
         class-name=&quot;com.your-package.SampleEntryListener&quot;/&gt;
      &lt;hz:entry-listener implementation=&quot;EntryListener&quot; local=&quot;false&quot;/&gt;
   &lt;/hz:entry-listeners&gt;
&lt;/hz:multimap&gt;
</code></pre><a name="multimap-listener-attributes"></a><h4 id="multimap-listener-attributes">MultiMap Listener Attributes</h4>
<p>As you see, there are attributes of the MultiMap listeners in the above examples: <code>include-value</code> and <code>local</code>. The attribute <code>include-value</code> is a boolean attribute that is optional, and if you set it to <code>true</code>, the MultiMap event will contain the map value. Its default value is <code>true</code>.</p>
<p>The attribute <code>local</code> is also a boolean attribute that is optional, and if you set it to <code>true</code>, you can listen to the MultiMap on the local member. Its default value is <code>false</code>.</p>

<a name="listening-for-item-events"></a><h3 id="listening-for-item-events">Listening for Item Events</h3>
<p>The Item Listener is used by the Hazelcast <code>IQueue</code>, <code>ISet</code> and <code>IList</code> interfaces.</p>
<p>To write an Item Listener class, you implement the ItemListener interface and its methods <code>itemAdded</code> and <code>itemRemoved</code>. These methods
are invoked when an item is added or removed.</p>
<p>The following is an example Item Listener class for an <code>ISet</code> structure.</p>
<pre><code class="lang-java">public class SampleItemListener implements ItemListener {

  public static void main( String[] args ) { 
    SampleItemListener sampleItemListener = new SampleItemListener();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ICollection&lt;Price&gt; set = hazelcastInstance.getSet( &quot;default&quot; );
    set.addItemListener( sampleItemListener, true ); 

    Price price = new Price( 10, time1 )
    set.add( price );
    set.remove( price );
  } 

  public void itemAdded( Object item ) {
    System.out.println( &quot;Item added = &quot; + item );
  }

  public void itemRemoved( Object item ) {
    System.out.println( &quot;Item removed = &quot; + item );
  }     
}
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can use <code>ICollection</code> when creating any of the collection (queue, set and list) data structures, as shown above. You can also use <code>IQueue</code>, <code>ISet</code> or <code>IList</code> instead of <code>ICollection</code>.</em>
<br></br></p>
<a name="registering-item-listeners"></a><h4 id="registering-item-listeners">Registering Item Listeners</h4>
<p>After you create your class, you can configure your cluster to include item listeners. Below is an example using the method <code>addItemListener</code> for <code>ISet</code> (it applies also to <code>IQueue</code> and <code>IList</code>). You can also see this portion in the above class creation.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

ICollection&lt;Price&gt; set = hazelcastInstance.getSet( &quot;default&quot; );
// or ISet&lt;Prices&gt; set = hazelcastInstance.getSet( &quot;default&quot; );
default.addItemListener( sampleItemListener, true );
</code></pre>
<p>With the above approach, there is the possibility of missing events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register listeners in the configuration. You can register listeners using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">setConfig.addItemListenerConfig(
new ItemListenerConfig( &quot;com.your-package.SampleItemListener&quot;, true ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;item-listeners&gt;
     &lt;item-listener include-value=&quot;true&quot;&gt;
       com.your-package.SampleItemListener
     &lt;/item-listener&gt;
   &lt;/item-listeners&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:set name=&quot;default&quot; &gt;
  &lt;hz:item-listeners&gt;
    &lt;hz:item-listener include-value=&quot;true&quot;
      class-name=&quot;com.your-package.SampleItemListener&quot;/&gt;
  &lt;/hz:item-listeners&gt;
&lt;/hz:set&gt;
</code></pre><a name="item-listener-attributes"></a><h4 id="item-listener-attributes">Item Listener Attributes</h4>
<p>As you see, there is an attribute in the above examples: <code>include-value</code>. It is a boolean attribute that is optional, and if you set it to <code>true</code>, the item event will contain the item value. Its default value is <code>true</code>.</p>
<p>There is also another attribute called <code>local</code>, which is not shown in the above examples. It is also a boolean attribute that is optional, and if you set it to <code>true</code>, you can listen to the items on the local member. Its default value is <code>false</code>.</p>

<a name="listening-for-topic-messages"></a><h3 id="listening-for-topic-messages">Listening for Topic Messages</h3>
<p>The Message Listener is used by the <code>ITopic</code> interface. It notifies when a message is received for the registered topic.</p>
<p>To write a Message Listener class, you implement the MessageListener interface and its method <code>onMessage</code>, which is invoked
when a message is received for the registered topic.</p>
<p>The following is an example Message Listener class.</p>
<pre><code class="lang-java">public class SampleMessageListener implements MessageListener&lt;MyEvent&gt; {

  public static void main( String[] args ) {
    SampleMessageListener sample = new SampleMessageListener();
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    ITopic topic = hazelcastInstance.getTopic( &quot;default&quot; );
    topic.addMessageListener( sample );
    topic.publish( new MyEvent() );
  }

  public void onMessage( Message&lt;MyEvent&gt; message ) {
    MyEvent myEvent = message.getMessageObject();
    System.out.println( &quot;Message received = &quot; + myEvent.toString() );
    if ( myEvent.isHeavyweight() ) {
      messageExecutor.execute( new Runnable() {
          public void run() {
            doHeavyweightStuff( myEvent );
          }
      } );
    }
  }
</code></pre>
<a name="registering-message-listeners"></a><h4 id="registering-message-listeners">Registering Message Listeners</h4>
<p>After you create your class, you can configure your cluster to include message listeners. Below is an example using the method <code>addMessageListener</code>. You can also see this portion in the above class creation.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

ITopic topic = hazelcastInstance.getTopic( &quot;default&quot; );
topic.addMessageListener( sample );
</code></pre>
<p>With the above approach, there is the possibility of missing messaging events between the creation of the instance and registering the listener. To overcome this race condition, Hazelcast allows you to register this listener in the configuration. You can register it using declarative, programmatic, or Spring configuration, as shown below.</p>
<p>The following is an example programmatic configuration.</p>
<pre><code class="lang-java">topicConfig.addMessageListenerConfig(
new ListenerConfig( &quot;com.your-package.SampleMessageListener&quot; ) );
</code></pre>
<p>The following is an example of the equivalent declarative configuration. </p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;topic name=&quot;default&quot;&gt;
      &lt;message-listeners&gt;
         &lt;message-listener&gt;
         com.your-package.SampleMessageListener
         &lt;/message-listener&gt;
      &lt;/message-listeners&gt;
   &lt;/topic&gt;   
   ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is an example of the equivalent Spring configuration.</p>
<pre><code>&lt;hz:topic name=&quot;default&quot;&gt;
  &lt;hz:message-listeners&gt;
    &lt;hz:message-listener 
       class-name=&quot;com.your-package.SampleMessageListener&quot;/&gt;
  &lt;/hz:message-listeners&gt;
&lt;/hz:topic&gt;
</code></pre>
<a name="listening-for-clients"></a><h3 id="listening-for-clients">Listening for Clients</h3>
<p>The Client Listener is used by the Hazelcast cluster members. It notifies the cluster members when a client is connected to or disconnected from the cluster.</p>
<p>To write a client listener class, you implement the <code>ClientListener</code> interface and its methods <code>clientConnected</code> and <code>clientDisconnected</code>,
which are invoked when a client is connected to or disconnected from the cluster. You can add your client listener as shown below.</p>
<pre><code>hazelcast.getClientService().addClientListener(SampleClientListener);
</code></pre><p>The following is the equivalent declarative configuration.</p>
<pre><code class="lang-xml">&lt;listeners&gt;
   &lt;listener&gt;
      com.your-package.SampleClientListener
   &lt;/listener&gt;
&lt;/listeners&gt;
</code></pre>
<p>The following is the equivalent configuration in the Spring context.</p>
<pre><code class="lang-xml">&lt;hz:listeners&gt;
   &lt;hz:listener class-name=&quot;com.your-package.SampleClientListener&quot;/&gt;
   &lt;hz:listener implementation=&quot;com.your-package.SampleClientListener&quot;/&gt;
&lt;/hz:listeners&gt;
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can also add event listeners to a Hazelcast client. Please refer to <a href="#configuring-client-listeners">Client Listenerconfig</a> for the related information.</em></p>

<a name="event-listeners-for-hazelcast-clients"></a><h2 id="event-listeners-for-hazelcast-clients">Event Listeners for Hazelcast Clients</h2>
<p>You can add event listeners to a Hazelcast Java client. You can configure the following listeners to listen to the events on the client side. Please see the respective sections under the <a href="#event-listeners-for-hazelcast-members">Event Listeners for Hazelcast Members section</a> for example code.</p>
<ul>
<li><a href="#listening-for-lifecycle-events">Lifecycle Listener</a>: Notifies when the client is starting, started, shutting down, and shutdown.</li>
<li><a href="#listening-for-member-events">Membership Listener</a>: Notifies when a member joins to/leaves the cluster to which the client is connected, or when an attribute is changed in a member.</li>
<li><a href="#listening-for-distributed-object-events">DistributedObject Listener</a>: Notifies when a distributed object is created or destroyed throughout the cluster to which the client is connected.</li>
</ul>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#client-listener-configuration">Client Listenerconfig section</a> for more information.</em>
<br></br></p>

<a name="global-event-configuration"></a><h2 id="global-event-configuration">Global Event Configuration</h2>
<ul>
<li><code>hazelcast.event.queue.capacity</code>: default value is 1000000</li>
<li><code>hazelcast.event.queue.timeout.millis</code>: default value is 250</li>
<li><code>hazelcast.event.thread.count</code>: default value is 5</li>
</ul>
<p>A striped executor in each cluster member controls and dispatches the received events. This striped executor also guarantees the event order. For all events in Hazelcast, the order in which events are generated and the order in which they are published are guaranteed for given keys. For map and multimap, the order is preserved for the operations on the same key of the entry. For list, set, topic and queue, the order is preserved for events on that instance of the distributed data structure.</p>
<p>To achieve the order guarantee, you make only one thread responsible for a particular set of events (entry events of a key in a map, item events of a collection, etc.) in <code>StripedExecutor</code> (within <code>com.hazelcast.util.executor</code>).</p>
<p>If the event queue reaches its capacity (<code>hazelcast.event.queue.capacity</code>) and the last item cannot be put into the event queue for the period specified in <code>hazelcast.event.queue.timeout.millis</code>, these events will be dropped with a warning message, such as &quot;EventQueue overloaded&quot;.</p>
<p>If event listeners perform a computation that takes a long time, the event queue can reach its maximum capacity and lose events. For map and multimap, you can configure <code>hazelcast.event.thread.count</code> to a higher value so that fewer collisions occur for keys, and therefore worker threads will not block each other in <code>StripedExecutor</code>. For list, set, topic and queue, you should offload heavy work to another thread. To preserve order guarantee, you should implement similar logic with <code>StripedExecutor</code> in the offloaded thread pool.
<br> </br></p>

<a name="distributed-computing"></a><h1 id="distributed-computing">Distributed Computing</h1>
<p>This chapter explains Hazelcast&#39;s executor service, durable executor service, and entry processor implementations.</p>
<a name="executor-service"></a><h2 id="executor-service">Executor Service</h2>
<p>One of the coolest features of Java 1.5 is the Executor framework, which allows you to asynchronously execute your tasks (logical units of work), such as database queries, complex calculations, and image rendering.</p>
<p>The default implementation of this framework (<code>ThreadPoolExecutor</code>) is designed to run within a single JVM (cluster member). In distributed systems, this implementation is not desired since you may want a task submitted in one JVM and processed in another one. Hazelcast offers <code>IExecutorService</code> for you to use in distributed environments. It implements <code>java.util.concurrent.ExecutorService</code> to serve the applications requiring computational and data processing power.</p>
<p>With <code>IExecutorService</code>, you can execute tasks asynchronously and perform other useful tasks. If your task execution takes longer than expected, you can cancel the task execution. Tasks should be <code>Serializable</code> since they will be distributed.</p>
<p>In the Java Executor framework, you implement tasks two ways: Callable or Runnable.</p>
<ul>
<li>Callable: If you need to return a value and submit it to Executor, implement the task as <code>java.util.concurrent.Callable</code>.</li>
<li>Runnable: If you do not need to return a value, implement the task as <code>java.util.concurrent.Runnable</code>.</li>
</ul>
<a name="implementing-a-callable-task"></a><h3 id="implementing-a-callable-task">Implementing a Callable Task</h3>
<p>In Hazelcast, when you implement a task as <code>java.util.concurrent.Callable</code> (a task that returns a value), you implement Callable and Serializable.</p>
<p>Below is an example of a Callable task. SumTask prints out map keys and returns the summed map values.</p>
<pre><code class="lang-java">import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.HazelcastInstanceAware;
import com.hazelcast.core.IMap;

import java.io.Serializable;
import java.util.concurrent.Callable;

public class SumTask
    implements Callable&lt;Integer&gt;, Serializable, HazelcastInstanceAware {

  private transient HazelcastInstance hazelcastInstance;

  public void setHazelcastInstance( HazelcastInstance hazelcastInstance ) {
    this.hazelcastInstance = hazelcastInstance;
  }

  public Integer call() throws Exception {
    IMap&lt;String, Integer&gt; map = hazelcastInstance.getMap( &quot;map&quot; );
    int result = 0;
    for ( String key : map.localKeySet() ) {
      System.out.println( &quot;Calculating for key: &quot; + key );
      result += map.get( key );
    }
    System.out.println( &quot;Local Result: &quot; + result );
    return result;
  }
}
</code></pre>
<p>Another example is the Echo callable below. In its call() method, it returns the local member and the input passed in. Remember that <code>instance.getCluster().getLocalMember()</code> returns the local member and <code>toString()</code> returns the member&#39;s address (IP + port) in String form, just to see which member actually executed the code for our example. Of course, the <code>call()</code> method can do and return anything you like. </p>
<pre><code class="lang-java">import java.util.concurrent.Callable;
import java.io.Serializable;

public class Echo implements Callable&lt;String&gt;, Serializable {
    String input = null;

    public Echo() {
    }

    public Echo(String input) {
        this.input = input;
    }

    public String call() {
        Config cfg = new Config();
        HazelcastInstance instance = Hazelcast.newHazelcastInstance(cfg);
        return instance.getCluster().getLocalMember().toString() + &quot;:&quot; + input;
    }
}
</code></pre>
<a name="executing-a-callable-task"></a><h4 id="executing-a-callable-task">Executing a Callable Task</h4>
<p>To execute a callable task with the executor framework:</p>
<ul>
<li>Obtain an <code>ExecutorService</code> instance (generally via <code>Executors</code>).</li>
<li>Submit a task which returns a <code>Future</code>. </li>
<li>After executing the task, you do not have to wait for the execution to complete, you can process other things. </li>
<li>When ready, use the <code>Future</code> object to retrieve the result as shown in the code example below.</li>
</ul>
<p>Below, the Echo task is executed.</p>
<pre><code class="lang-java">ExecutorService executorService = Executors.newSingleThreadExecutor();
Future&lt;String&gt; future = executorService.submit( new Echo( &quot;myinput&quot;) );
//while it is executing, do some useful stuff
//when ready, get the result of your execution
String result = future.get();
</code></pre>
<p>Please note that the Echo callable in the above code sample also implements a Serializable interface, since it may be sent to another member to be processed.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>When a task is deserialized, HazelcastInstance needs to be accessed. To do this, the task should implement <code>HazelcastInstanceAware</code> interface. Please see the <a href="#implementing-hazelcastinstanceaware">HazelcastInstanceAware Interface section</a> for more information.</em>
<br></br></p>
<a name="implementing-a-runnable-task"></a><h3 id="implementing-a-runnable-task">Implementing a Runnable Task</h3>
<p>In Hazelcast, when you implement a task as <code>java.util.concurrent.runnable</code> (a task that does not return a value), you implement Runnable and Serializable.</p>
<p>Below is Runnable example code. It is a task that waits for some time and echoes a message.</p>
<pre><code class="lang-java">public class EchoTask implements Runnable, Serializable {
  private final String msg;

  public EchoTask( String msg ) {
    this.msg = msg;
  }

  @Override
  public void run() {
    try {
      Thread.sleep( 5000 );
    } catch ( InterruptedException e ) {
    }
    System.out.println( &quot;echo:&quot; + msg );
  }
}
</code></pre>
<a name="executing-a-runnable-task"></a><h4 id="executing-a-runnable-task">Executing a Runnable Task</h4>
<p>To execute the runnable task:</p>
<ul>
<li>Retrieve the Executor from <code>HazelcastInstance</code>.</li>
<li>Submit the tasks to the Executor.</li>
</ul>
<p>Now let&#39;s write a class that submits and executes these echo messages. Executor is retrieved from <code>HazelcastInstance</code> and 1000 echo tasks are submitted.</p>
<pre><code class="lang-java">public class MasterMember {
  public static void main( String[] args ) throws Exception {
    HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
    IExecutorService executor = hazelcastInstance.getExecutorService( &quot;exec&quot; );
    for ( int k = 1; k &lt;= 1000; k++ ) {
      Thread.sleep( 1000 );
      System.out.println( &quot;Producing echo task: &quot; + k );
      executor.execute( new EchoTask( String.valueOf( k ) ) );
    }
    System.out.println( &quot;EchoTaskMain finished!&quot; );
  }
}
</code></pre>
<a name="scaling-the-executor-service"></a><h3 id="scaling-the-executor-service">Scaling The Executor Service</h3>
<p>You can scale the Executor service both vertically (scale up) and horizontally (scale out).</p>
<p>To scale up, you should improve the processing capacity of the cluster member (JVM). You can do this by increasing the <code>pool-size</code> property mentioned in <a href="#configuring-executor-service">Configuring Executor Service</a> (i.e., increasing the thread count). However, please be aware of your member&#39;s capacity. If you think it cannot handle such an additional load caused by increasing the thread count, you may want to consider improving the member&#39;s resources (CPU, memory, etc.). As an example, set the <code>pool-size</code> to 5 and run the above <code>MasterMember</code>. You will see that <code>EchoTask</code> is run as soon as it is produced.</p>
<p>To scale out, add more members instead of increasing only one member&#39;s capacity. In reality, you may want to expand your cluster by adding more physical or virtual machines. For example, in the EchoTask example in the <a href="#implementing-a-runnable-task">Runnable section</a>, you can create another Hazelcast instance. That instance will automatically get involved in the executions started in <code>MasterMember</code> and start processing.</p>

<a name="executing-code-in-the-cluster"></a><h3 id="executing-code-in-the-cluster">Executing Code in the Cluster</h3>
<p>The distributed executor service is a distributed implementation of <code>java.util.concurrent.ExecutorService</code>. It allows you to execute your code in the cluster. In this section, the code examples are based on the <a href="#implementing-a-callable-task">Echo class above</a> (please note that the Echo class is <code>Serializable</code>). The code examples show how Hazelcast can execute your code (<code>Runnable, Callable</code>):</p>
<ul>
<li><code>echoOnTheMember</code>: On a specific cluster member you choose with the <code>IExecutorService</code> <code>submitToMember</code> method.</li>
<li><code>echoOnTheMemberOwningTheKey</code>: On the member owning the key you choose with the <code>IExecutorService</code> <code>submitToKeyOwner</code> method.</li>
<li><code>echoOnSomewhere</code>: On the member Hazelcast picks with the <code>IExecutorService</code> <code>submit</code> method.</li>
<li><code>echoOnMembers</code>: On all or a subset of the cluster members with the <code>IExecutorService</code> <code>submitToMembers</code> method.</li>
</ul>
<pre><code class="lang-java">import com.hazelcast.core.Member;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.IExecutorService;
import java.util.concurrent.Callable;
import java.util.concurrent.Future;   
import java.util.Set;

public void echoOnTheMember( String input, Member member ) throws Exception {
  Callable&lt;String&gt; task = new Echo( input );
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IExecutorService executorService = 
      hazelcastInstance.getExecutorService( &quot;default&quot; );

  Future&lt;String&gt; future = executorService.submitToMember( task, member );
  String echoResult = future.get();
}

public void echoOnTheMemberOwningTheKey( String input, Object key ) throws Exception {
  Callable&lt;String&gt; task = new Echo( input );
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IExecutorService executorService =
      hazelcastInstance.getExecutorService( &quot;default&quot; );

  Future&lt;String&gt; future = executorService.submitToKeyOwner( task, key );
  String echoResult = future.get();
}

public void echoOnSomewhere( String input ) throws Exception { 
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IExecutorService executorService =
      hazelcastInstance.getExecutorService( &quot;default&quot; );

  Future&lt;String&gt; future = executorService.submit( new Echo( input ) );
  String echoResult = future.get();
}

public void echoOnMembers( String input, Set&lt;Member&gt; members ) throws Exception {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IExecutorService executorService = 
      hazelcastInstance.getExecutorService( &quot;default&quot; );

  Map&lt;Member, Future&lt;String&gt;&gt; futures = executorService
      .submitToMembers( new Echo( input ), members );

  for ( Future&lt;String&gt; future : futures.values() ) {
    String echoResult = future.get();
    // ...
  }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can obtain the set of cluster members via <code>HazelcastInstance#getCluster().getMembers()</code> call.</em></p>

<a name="canceling-an-executing-task"></a><h3 id="canceling-an-executing-task">Canceling an Executing Task</h3>
<p>A task in the code that you execute in a cluster might take longer than expected. If you cannot stop/cancel that task, it will keep eating your resources. </p>
<p>To cancel a task, you can use the standard Java executor framework&#39;s <code>cancel()</code> API. This framework encourages us to code and design for cancellations, a highly ignored part of software development.</p>
<a name="example-task-to-cancel"></a><h4 id="example-task-to-cancel">Example Task to Cancel</h4>
<p>The Fibonacci callable class below calculates the Fibonacci number for a given number. In the <code>calculate</code> method, we check if the current thread is interrupted so that the code can respond to cancellations once the execution is started. </p>
<pre><code class="lang-java">public class Fibonacci&lt;Long&gt; implements Callable&lt;Long&gt;, Serializable {
  int input = 0; 

  public Fibonacci() { 
  } 

  public Fibonacci( int input ) { 
    this.input = input;
  } 

  public Long call() {
    return calculate( input );
  }

  private long calculate( int n ) {
    if ( Thread.currentThread().isInterrupted() ) {
      return 0;
    }
    if ( n &lt;= 1 ) {
      return n;
    } else {
      return calculate( n - 1 ) + calculate( n - 2 );
    }
  }
}
</code></pre>
<a name="example-method-to-execute-and-cancel-the-task"></a><h4 id="example-method-to-execute-and-cancel-the-task">Example Method to Execute and Cancel the Task</h4>
<p>The <code>fib()</code> method below submits the Fibonacci calculation task above for number &#39;n&#39; and waits a maximum of 3 seconds for the result. If the execution does not completed in three seconds, <code>future.get()</code> will throw a <code>TimeoutException</code> and upon catching it, we cancel the execution, saving some CPU cycles.</p>
<pre><code class="lang-java">long fib( int n ) throws Exception {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IExecutorService es = hazelcastInstance.getExecutorService();
  Future future = es.submit( new Fibonacci( n ) );  
  try {
    return future.get( 3, TimeUnit.SECONDS );
  } catch ( TimeoutException e ) {
    future.cancel( true );            
  }
  return -1;
}
</code></pre>
<p><code>fib(20)</code> will probably take less than 3 seconds. However, <code>fib(50)</code> will take much longer. (This is not an example for writing better Fibonacci calculation code, but for showing how to cancel a running execution that takes too long.) The method <code>future.cancel(false)</code> can only cancel execution before it is running (executing), but <code>future.cancel(true)</code> can interrupt running executions provided that your code is able to handle the interruption. If you are willing to cancel an already running task, then your task should be designed to handle interruptions. If the <code>calculate (int n)</code> method did not have the <code>(Thread.currentThread().isInterrupted())</code> line, then you would not be able to cancel the execution after it is started.</p>

<a name="callback-when-task-completes"></a><h3 id="callback-when-task-completes">Callback When Task Completes</h3>
<p>You can use the <code>ExecutionCallback</code> offered by Hazelcast to asynchronously be notified when the execution is done.</p>
<ul>
<li>To be notified when your task completes without an error, implement the <code>onResponse</code> method.</li>
<li>To be notified when your task completes with an error, implement the <code>onFailure</code> method.</li>
</ul>
<a name="example-task-to-callback"></a><h4 id="example-task-to-callback">Example Task to Callback</h4>
<p>Let&#39;s use the Fibonacci series to explain this. The example code below is the calculation that will be executed. Note that it is Callable and Serializable.</p>
<pre><code class="lang-java">public class Fibonacci&lt;Long&gt; implements Callable&lt;Long&gt;, Serializable {
  int input = 0;

  public Fibonacci() {
  }

  public Fibonacci( int input ) {
    this.input = input;
  }

  public Long call() {
    return calculate( input );
  }

  private long calculate( int n ) {
    if (n &lt;= 1) {
      return n;
    } else {
      return calculate( n - 1 ) + calculate( n - 2 );
    }
  }
}
</code></pre>
<a name="example-method-to-callback-the-task"></a><h4 id="example-method-to-callback-the-task">Example Method to Callback the Task</h4>
<p>The example code below submits the Fibonacci calculation to <code>ExecutionCallback</code> and prints the result asynchronously. <code>ExecutionCallback</code> has the methods <code>onResponse</code> and <code>onFailure</code>. In this example code, <code>onResponse</code> is called upon a valid response and prints the calculation result, whereas <code>onFailure</code> is called upon a failure and prints the stacktrace.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.ExecutionCallback;
import com.hazelcast.core.IExecutorService;
import java.util.concurrent.Future;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
IExecutorService es = hazelcastInstance.getExecutorService();
Callable&lt;Long&gt; task = new Fibonacci( 10 );

es.submit(task, new ExecutionCallback&lt;Long&gt; () {

  @Override
  public void onResponse( Long response ) {
    System.out.println( &quot;Fibonacci calculation result = &quot; + response );
  }

  @Override
  public void onFailure( Throwable t ) {
    t.printStackTrace();
  }
};
</code></pre>

<a name="selecting-members-for-task-execution"></a><h3 id="selecting-members-for-task-execution">Selecting Members for Task Execution</h3>
<p>As previously mentioned, it is possible to indicate where in the Hazelcast cluster the <code>Runnable</code> or <code>Callable</code> is executed. Usually you execute these in the cluster based on the location of a key or a set of keys, or you allow Hazelcast to select a member.</p>
<p>If you want more control over where your code runs, use the <code>MemberSelector</code> interface. For example, you may want certain tasks to run only on certain members, or you may wish to implement some form of custom load balancing regime.  The <code>MemberSelector</code> is an interface that you can implement and then provide to the <code>IExecutorService</code> when you submit or execute.</p>
<p>The <code>select(Member)</code> method is called for every available member in the cluster. Implement this method to decide if the member is going to be used or not.</p>
<p>In a simple example shown below, we select the cluster members based on the presence of an attribute.</p>
<pre><code class="lang-java">public class MyMemberSelector implements MemberSelector {
     public boolean select(Member member) {
         return Boolean.TRUE.equals(member.getAttribute(&quot;my.special.executor&quot;));
     }
 }
</code></pre>
<p>You can use <code>MemberSelector</code> instances provided by the <code>com.hazelcast.cluster.memberselector.MemberSelectors</code> class. For example, you can select a lite member for running a task using <code>com.hazelcast.cluster.memberselector.MemberSelectors#LITE_MEMBER_SELECTOR</code>.</p>
<a name="configuring-executor-service"></a><h3 id="configuring-executor-service">Configuring Executor Service</h3>
<p>The following are example configurations for executor service.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;executor-service name=&quot;exec&quot;&gt;
   &lt;pool-size&gt;1&lt;/pool-size&gt;
   &lt;queue-capacity&gt;10&lt;/queue-capacity&gt;
   &lt;statistics-enabled&gt;true&lt;/statistics-enabled&gt;
&lt;/executor-service&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
ExecutorConfig executorConfig = config.getExecutorConfig(&quot;exec&quot;);
executorConfig.setPoolSize( &quot;1&quot; ).setQueueCapacity( &quot;10&quot; )
          .setStatisticsEnabled( true );
</code></pre>
<p>Executor service configuration has the following elements.</p>
<ul>
<li><code>pool-size</code>: The number of executor threads per Member for the Executor. By default, Executor is configured to have 16 threads in the pool. You can change that with this element.</li>
<li><code>queue-capacity</code>: Executor&#39;s task queue capacity; the number of tasks this queue can hold.</li>
<li><code>statistics-enabled</code>: You can retrieve some statistics (such as pending operations count, started operations count, completed operations count, and cancelled operations count) by setting this parameter&#39;s value to <code>true</code>. The method for retrieving the statistics is <code>getLocalExecutorStats()</code>.</li>
</ul>

<a name="durable-executor-service"></a><h2 id="durable-executor-service">Durable Executor Service</h2>
<p>Hazelcast&#39;s durable executor service is a data structure which is able to store an execution task both on the executing Hazelcast member and its backup member(s), if configured. By this way, you do not lose any tasks if a member goes down or any results if the submitter (member or client) goes down while executing the task. When using the durable executor service you can either submit or execute a task randomly or on the owner of a provided key. Note that in <a href="#executor-service">executor service</a>, you can submit or execute tasks to/on the selected member(s).</p>
<p>Processing of the tasks when using durable executor service involves two invocations:</p>
<ol>
<li>Sending the task to primary Hazelcast member (primary partition) and to its backups, if configured, and executing the task.</li>
<li>Retrieving the result of the task.</li>
</ol>
<p>As you may already know, Hazelcast&#39;s executor service returns a <code>future</code> representing the task to the user. With the above two-invocations approach, it is guaranteed that the task is executed before the <code>future</code> returns and you can track the response of a submitted task with a unique ID. Hazelcast stores the task on both primary and backup members, and starts the execution also. </p>
<p>With the first invocation, a <a href="#ringbuffer">Ringbuffer</a> stores the task and a generated sequence for the task is returned to the caller as a result. In addition to the storing, the task is executed on the local execution service for the primary member. By this way, the task is now resilient to member failures and you are able to track the task with its ID.</p>
<p>After the first invocation has completed and the sequence of task is returned, second invocation starts to retrieve the result of task with that sequence. This retrieval waits in the waiting operations queue until notified, or it runs immediately if the result is already available.</p>
<p>When task execution is completed, Ringbuffer replaces the task with the result for the given task sequence. This replacement notifies the waiting operations queue.</p>
<a name="configuring-durable-executor-service"></a><h3 id="configuring-durable-executor-service">Configuring Durable Executor Service</h3>
<p>This section presents example configurations for durable executor service along with the descriptions of its configuration elements and attributes.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;durable-executor-service name=&quot;myDurableExecSvc&quot;&gt;
    &lt;pool-size&gt;8&lt;/pool-size&gt;
    &lt;durability&gt;1&lt;/durability&gt;
    &lt;capacity&gt;1&lt;/capacity&gt;
&lt;/durable-executor-service&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">HazelcastInstance hazelcast = Hazelcast.newHazelcastInstance();
DurableExecutorService durableExecSvc = hazelcast.getDurableExecutorService(&quot;myDurableExecSvc&quot;);

Config config = new Config();
config.getDurableExecutorConfig( &quot;myDurableExecSvc&quot; ).
      .setPoolSize ( &quot;8&quot; )
      .setDurability( &quot;1&quot; )
      .setCapacity( &quot;1&quot; );
</code></pre>
<p>Following are the descriptions of each configuration element and attribute:</p>
<ul>
<li><code>name</code>: Name of the executor task.</li>
<li><code>pool-size</code>: Number of executor threads per member for the executor.</li>
<li><code>durability</code>: Durability of the executor.</li>
<li><code>capacity</code>: Capacity of the executor task. 0 means Integer.MAX_VALUE.</li>
</ul>

<a name="entry-processor"></a><h2 id="entry-processor">Entry Processor</h2>
<p>Hazelcast supports entry processing. An entry processor is a function that executes your code on a map entry in an atomic way. </p>
<p>An entry processor is a good option if you perform bulk processing on an <code>IMap</code>. Usually you perform a loop of keys-- executing <code>IMap.get(key)</code>, mutating the value, and finally putting the entry back in the map using <code>IMap.put(key,value)</code>.  If you perform this process from a client or from a member where the keys do not exist, you effectively perform two network hops for each update: the first to retrieve the data and the second to update the mutated value.</p>
<p>If you are doing the process described above, you should consider using entry processors. An entry processor executes a read and updates upon the member where the data resides.  This eliminates the costly network hops described above.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>Entry processor is meant to process a single entry per call. Processing multiple entries and data structures in an entry processor is not supported as it may result in deadlocks.</em></p>
<a name="performing-fast-in-memory-map-operations"></a><h3 id="performing-fast-in-memory-map-operations">Performing Fast In-Memory Map Operations</h3>
<p>An entry processor enables fast in-memory operations on your map without you having to worry about locks or concurrency issues. You can apply it to a single map entry or to all map entries. Entry processors support choosing target entries using predicates. You do not need any explicit lock on entry thanks to the isolated threading model: Hazelcast runs the EntryProcessor for all entries on a <code>partitionThread</code> so there will NOT be any interleaving of the EntryProcessor and other mutations.</p>
<p>Hazelcast sends the entry processor to each cluster member and these members apply it to map entries. Therefore, if you add more members, your processing completes faster.</p>
<a name="using-indexes"></a><h3 id="using-indexes">Using Indexes</h3>
<p>Entry processors can be used with predicates. Predicates help to process a subset of data by selecting eligible entries. This selection can happen either by doing a full-table scan or by using indexes. To accelerate entry selection step, you can consider to add indexes. If indexes are there, entry processor will automatically use them.</p>
<a name="using-object-in-memory-format"></a><h3 id="using-object-in-memory-format">Using OBJECT In-Memory Format</h3>
<p>If entry processing is the major operation for a map and if the map consists of complex objects, you should use <code>OBJECT</code> as the <code>in-memory-format</code> to minimize serialization cost. By default, the entry value is stored as a byte array (<code>BINARY</code> format). When it is stored as an object (<code>OBJECT</code> format), then the entry processor is applied directly on the object. In that case, no serialization or deserialization is performed. However, if there is a defined event listener, a new entry value will be serialized when passing to the event publisher service.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>When <code>in-memory-format</code> is <code>OBJECT</code>, the old value of the updated entry will be null.</em></p>
<a name="entry-processing-with-imap-methods"></a><h4 id="entry-processing-with-imap-methods">Entry Processing with IMap Methods</h4>
<p>The methods below are in the IMap interface for entry processing.</p>
<ul>
<li><code>executeOnKey</code> processes an entry mapped by a key.</li>
<li><code>executeOnKeys</code> processes entries mapped by a collection of keys.</li>
<li><code>submitToKey</code> processes an entry mapped by a key while listening to event status.</li>
<li><code>executeOnEntries</code> processes all entries in a map.</li>
<li><code>executeOnEntries</code> can also process all entries in a map with a defined predicate.</li>
</ul>
<pre><code class="lang-java">/**
 * Applies the user defined EntryProcessor to the entry mapped by the key.
 * Returns the object which is the result of the process() method of EntryProcessor.
 */
Object executeOnKey( K key, EntryProcessor entryProcessor );

/**
 * Applies the user defined EntryProcessor to the entries mapped by the collection of keys.
 * Returns the results mapped by each key in the collection.
 */
Map&lt;K, Object&gt; executeOnKeys( Set&lt;K&gt; keys, EntryProcessor entryProcessor );

/**
 * Applies the user defined EntryProcessor to the entry mapped by the key with
 * specified ExecutionCallback to listen to event status and return immediately.
 */
void submitToKey( K key, EntryProcessor entryProcessor, ExecutionCallback callback );


/**
 * Applies the user defined EntryProcessor to all entries in the map.
 * Returns the results mapped by each key in the map.
 */
Map&lt;K, Object&gt; executeOnEntries( EntryProcessor entryProcessor );

/**
 * Applies the user defined EntryProcessor to the entries in the map which satisfies 
 provided predicate.
 * Returns the results mapped by each key in the map.
 */
Map&lt;K, Object&gt; executeOnEntries( EntryProcessor entryProcessor, Predicate predicate );
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>Entry Processors run via Operation Threads that are dedicated to specific partitions.  Therefore, with long running Entry Processor executions, other partition operations such as <code>map.put(key)</code> cannot be processed. With this in mind, it is good practice to make your Entry Processor executions as quick as possible.</em></p>
<a name="entryprocessor-interface"></a><h3 id="-entryprocessor-interface"><code>EntryProcessor</code> Interface</h3>
<p>The following is the <code>EntryProcessor</code> interface:</p>
<pre><code class="lang-java">public interface EntryProcessor&lt;K, V&gt; extends Serializable {
  Object process( Map.Entry&lt;K, V&gt; entry );

  EntryBackupProcessor&lt;K, V&gt; getBackupProcessor();
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>If you want to execute a task on a single key, you can also use <code>executeOnKeyOwner</code> provided by Executor Service. However, in this case you need to perform a lock and serialization.</em></p>
<p>When using the <code>executeOnEntries</code> method, if the number of entries is high and you need the results, then returning null with the <code>process()</code> method is a good practice. By returning null, results of the processing is not stored in the map and thus out of memory errors are eliminated.</p>
<a name="processing-backup-entries"></a><h3 id="processing-backup-entries">Processing Backup Entries</h3>
<p>If your code modifies the data, then you should also provide a processor for backup entries. This is required to prevent the primary map entries from having different values than the backups because it causes the entry processor to be applied both on the primary and backup entries.</p>
<pre><code class="lang-java">public interface EntryBackupProcessor&lt;K, V&gt; extends Serializable {
    void processBackup( Map.Entry&lt;K, V&gt; entry );
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>It is possible that an Entry Processor could see that a key exists though its backup processor may not find it at the run time due to an unsent backup of a previous operation (e.g., a previous put operation). In those situations, Hazelcast internally/eventually will synchronize those owner and backup partitions so you will not lose any data. When coding an <code>EntryBackupProcessor</code>, you should take that case into account, otherwise <code>NullPointerException</code> can be seen since <code>Map.Entry.getValue()</code> may return <code>null</code>.</em></p>

<a name="creating-an-entry-processor"></a><h3 id="creating-an-entry-processor">Creating an Entry Processor</h3>
<p>The EntryProcessorTest class has the following methods.</p>
<ul>
<li><code>testMapEntryProcessor</code> puts one map entry and calls <code>executeOnKey</code> to process that map entry.</li>
<li><code>testMapEntryProcessor</code> puts all the entries in a map and calls <code>executeOnEntries</code> to process 
 all the entries.</li>
</ul>
<p>The static class <code>IncrementingEntryProcessor</code> creates an entry processor to process the map 
entries in the EntryProcessorTest class. It creates the entry processor class by:</p>
<ul>
<li>implementing the map interfaces <code>EntryProcessor</code> and <code>EntryBackupProcessor</code>.</li>
<li>implementing the <code>java.io.Serializable</code> interface.</li>
<li>implementing the <code>EntryProcessor</code> methods <code>process</code> and <code>getBackupProcessor</code>.</li>
<li>implementing the <code>EntryBackupProcessor</code> method <code>processBackup</code>.</li>
</ul>
<pre><code class="lang-java">public class EntryProcessorTest {

  @Test
  public void testMapEntryProcessor() throws InterruptedException {
    Config config = new Config().getMapConfig( &quot;default&quot; )
        .setInMemoryFormat( MapConfig.InMemoryFormat.OBJECT );

    HazelcastInstance hazelcastInstance1 = Hazelcast.newHazelcastInstance( config );
    HazelcastInstance hazelcastInstance2 = Hazelcast.newHazelcastInstance( config );
    IMap&lt;Integer, Integer&gt; map = hazelcastInstance1.getMap( &quot;mapEntryProcessor&quot; );
    map.put( 1, 1 );
    EntryProcessor entryProcessor = new IncrementingEntryProcessor();
    map.executeOnKey( 1, entryProcessor );
    assertEquals( map.get( 1 ), (Object) 2 );
    hazelcastInstance1.getLifecycleService().shutdown();
    hazelcastInstance2.getLifecycleService().shutdown();
  }

  @Test
  public void testMapEntryProcessorAllKeys() throws InterruptedException {
    StaticNodeFactory factory = new StaticNodeFactory( 2 );
    Config config = new Config().getMapConfig( &quot;default&quot; )
        .setInMemoryFormat( MapConfig.InMemoryFormat.OBJECT );

    HazelcastInstance hazelcastInstance1 = factory.newHazelcastInstance( config );
    HazelcastInstance hazelcastInstance2 = factory.newHazelcastInstance( config );
    IMap&lt;Integer, Integer&gt; map = hazelcastInstance1
        .getMap( &quot;mapEntryProcessorAllKeys&quot; );

    int size = 100;
    for ( int i = 0; i &lt; size; i++ ) {
      map.put( i, i );
    }
    EntryProcessor entryProcessor = new IncrementingEntryProcessor();
    Map&lt;Integer, Object&gt; res = map.executeOnEntries( entryProcessor );
    for ( int i = 0; i &lt; size; i++ ) {
      assertEquals( map.get( i ), (Object) (i + 1) );
    }
    for ( int i = 0; i &lt; size; i++ ) {
      assertEquals( map.get( i ) + 1, res.get( i ) );
    }
    hazelcastInstance1.getLifecycleService().shutdown();
    hazelcastInstance2.getLifecycleService().shutdown();
  }

  static class IncrementingEntryProcessor
      implements EntryProcessor, EntryBackupProcessor, Serializable {

    public Object process( Map.Entry entry ) {
      Integer value = (Integer) entry.getValue();
      entry.setValue( value + 1 );
      return value + 1;
    }

    public EntryBackupProcessor getBackupProcessor() {
      return IncrementingEntryProcessor.this;
    }

    public void processBackup( Map.Entry entry ) {
      entry.setValue( (Integer) entry.getValue() + 1 );
    }
  }
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>You should explicitly call the <code>setValue</code> method of <code>Map.Entry</code> when modifying data in Entry Processor. Otherwise, Entry Processor will be accepted as read-only.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>An Entry Processor instance is not thread safe. If you are storing a partition specific state between invocations, be sure to register this in a thread-local.  An Entry Processor instance can be used by multiple partition threads.</em></p>

<a name="abstract-entry-processor"></a><h3 id="abstract-entry-processor">Abstract Entry Processor</h3>
<p>You can use the <code>AbstractEntryProcessor</code> class when the same processing will be performed both on the primary and backup map entries (i.e. the same logic applies to them). If you use Entry Processor, you need to apply the same logic to the backup entries separately. The <code>AbstractEntryProcessor</code> class makes this primary/backup processing easier.</p>
<p>The code below shows the Hazelcast <code>AbstractEntryProcessor</code> class. You can use it to create your own Abstract Entry Processor.</p>
<pre><code class="lang-java">public abstract class AbstractEntryProcessor &lt;K, V&gt;
    implements EntryProcessor &lt;K, V&gt; {

  private final EntryBackupProcessor &lt;K,V&gt; entryBackupProcessor;
  public AbstractEntryProcessor() {
    this(true);
  }

  public AbstractEntryProcessor(boolean applyOnBackup) {
    if ( applyOnBackup ) {
      entryBackupProcessor = new EntryBackupProcessorImpl();
    } else {
      entryBackupProcessor = null;
    }
  } 

  @Override
  public abstract Object process(Map.Entry&lt;K, V&gt; entry);

  @Override
  public final EntryBackupProcessor &lt;K, V&gt; getBackupProcessor() {
    return entryBackupProcessor;
  }

  private class EntryBackupProcessorImpl implements EntryBackupProcessor &lt;K,V&gt;{
    @Override
    public void processBackup(Map.Entry&lt;K, V&gt; entry) {
      process(entry); 
    }
  }    
}
</code></pre>
<p>In the above code, the method <code>getBackupProcessor</code> returns an <code>EntryBackupProcessor</code> instance. This means the same processing will be applied to both the primary and backup entries. If you want to apply the processing only upon the primary entries, make the <code>getBackupProcessor</code> method return null. </p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>Beware of the null issue described in the note in the <a href="#processing-backup-entries">Processing Backup Entries section</a>. Due to a yet unsent backup from a previous operation, an <code>EntryBackupProcessor</code> may temporarily receive <code>null</code> from <code>Map.Entry.getValue()</code> even though the value actually exists in the map. If you decide to use <code>AbstractEntryProcessor</code>, make sure your code logic is not sensitive to null values, or you may encounter <code>NullPointerException</code> during runtime.</em></p>

<a name="distributed-query"></a><h1 id="distributed-query">Distributed Query</h1>
<p>Distributed queries access data from multiple data sources stored on either the same or different members.</p>
<p>Hazelcast partitions your data and spreads it across cluster of members. You can iterate over the map entries and look for certain entries (specified by predicates) you are interested in. However, this is not very efficient because you will have to bring the entire entry set and iterate locally. Instead, Hazelcast allows you to run distributed queries on your distributed map.</p>
<a name="how-distributed-query-works"></a><h2 id="how-distributed-query-works">How Distributed Query Works</h2>
<ol>
<li>The requested predicate is sent to each member in the cluster.</li>
<li>Each member looks at its own local entries and filters them according to the predicate. At this stage, key/value pairs of the entries are deserialized and then passed to the predicate.</li>
<li>The predicate requester merges all the results coming from each member into a single set.</li>
</ol>
<p>Distributed query is highly scalable. If you add new members to the cluster, the partition count for each member is reduced and thus the time spent by each member on iterating its entries is reduced. In addition, the pool of partition threads evaluates the entries concurrently in each member, and the network traffic is also reduced since only filtered data is sent to the requester.</p>
<p>Hazelcast offers the following APIs for distributed query purposes:</p>
<ul>
<li>Criteria API</li>
<li>Distributed SQL Query
<br></br></li>
</ul>
<a name="employee-map-query-example"></a><h3 id="employee-map-query-example">Employee Map Query Example</h3>
<p>Assume that you have an &quot;employee&quot; map containing values of <code>Employee</code> objects, as coded below.</p>
<pre><code class="lang-java">import java.io.Serializable;

public class Employee implements Serializable {
private String name;
private int age;
private boolean active;
private double salary;

public Employee(String name, int age, boolean live, double price) {
    this.name = name;
    this.age = age;
    this.active = live;
    this.salary = price;
}

public Employee() {
}

public String getName() {
    return name;
}

public int getAge() {
    return age;
}

public double getSalary() {
    return salary;
}

public boolean isActive() {
    return active;
}
}
</code></pre>
<p>Now let&#39;s look for the employees who are active and have an age less than 30 using the aforementioned APIs (Criteria API and Distributed SQL Query). The following subsections describe each query mechanism for this example.</p>
<p><img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em>When using Portable objects, if one field of an object exists on one member but does not exist on another one, Hazelcast does not throw an unknown field exception.
Instead, Hazelcast treats that predicate, which tries to perform a query on an unknown field, as an always false predicate.</em></p>

<a name="querying-with-criteria-api"></a><h3 id="querying-with-criteria-api">Querying with Criteria API</h3>
<p>Criteria API is a programming interface offered by Hazelcast that is similar to the Java Persistence Query Language (JPQL). Below is the code
for the <a href="#employee-map-query-example">above example query</a>.</p>
<pre><code class="lang-java">import com.hazelcast.core.IMap;
import com.hazelcast.query.Predicate;
import com.hazelcast.query.PredicateBuilder;
import com.hazelcast.query.EntryObject;
import com.hazelcast.config.Config;

IMap&lt;String, Employee&gt; map = hazelcastInstance.getMap( &quot;employee&quot; );

EntryObject e = new PredicateBuilder().getEntryObject();
Predicate predicate = e.is( &quot;active&quot; ).and( e.get( &quot;age&quot; ).lessThan( 30 ) );

Set&lt;Employee&gt; employees = map.values( predicate );
</code></pre>
<p>In the above example code, <code>predicate</code> verifies whether the entry is active and its <code>age</code> value is less than 30. This <code>predicate</code> is
applied to the <code>employee</code> map using the <code>map.values(predicate)</code> method. This method sends the predicate to all cluster members
and merges the results coming from them. Since the predicate is communicated between the members, it needs to
be serializable.</p>
<p><img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em>Predicates can also be applied to <code>keySet</code>, <code>entrySet</code> and <code>localKeySet</code> of the Hazelcast distributed 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;map.</em></p>
<a name="predicates-class-operators"></a><h4 id="predicates-class-operators">Predicates Class Operators</h4>
<p>The <code>Predicates</code> class offered by Hazelcast includes many operators for your query requirements. Some of them are
explained below.</p>
<ul>
<li><code>equal</code>: Checks if the result of an expression is equal to a given value.</li>
<li><code>notEqual</code>: Checks if the result of an expression is not equal to a given value.</li>
<li><code>instanceOf</code>: Checks if the result of an expression has a certain type.</li>
<li><code>like</code>: Checks if the result of an expression matches some string pattern. % (percentage sign) is the placeholder for many
characters,  (underscore) is placeholder for only one character.</li>
<li><code>greaterThan</code>: Checks if the result of an expression is greater than a certain value.</li>
<li><code>greaterEqual</code>: Checks if the result of an expression is greater than or equal to a certain value.</li>
<li><code>lessThan</code>: Checks if the result of an expression is less than a certain value.</li>
<li><code>lessEqual</code>: Checks if the result of an expression is less than or equal to a certain value.</li>
<li><code>between</code>: Checks if the result of an expression is between two values (this is inclusive).</li>
<li><code>in</code>: Checks if the result of an expression is an element of a certain collection.</li>
<li><code>isNot</code>: Checks if the result of an expression is false.</li>
<li><code>regex</code>: Checks if the result of an expression matches some regular expression.
<br></br></li>
</ul>
<p><strong><em>RELATED INFORMATION</em></strong> </p>
<p><em>Please see the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/query/Predicates.java" target="_blank">
Predicates class</a> for all predicates provided.</em></p>
<a name="combining-predicates-with-and-or-not"></a><h4 id="combining-predicates-with-and-or-not">Combining Predicates with AND, OR, NOT</h4>
<p>You can combine predicates using the <code>and</code>, <code>or</code>, and <code>not</code> operators, as shown in the below examples.</p>
<pre><code class="lang-java">public Set&lt;Person&gt; getWithNameAndAge( String name, int age ) {
  Predicate namePredicate = Predicates.equal( &quot;name&quot;, name );
  Predicate agePredicate = Predicates.equal( &quot;age&quot;, age );
  Predicate predicate = Predicates.and( namePredicate, agePredicate );
  return personMap.values( predicate );
}
</code></pre>
<pre><code class="lang-java">public Set&lt;Person&gt; getWithNameOrAge( String name, int age ) {
  Predicate namePredicate = Predicates.equal( &quot;name&quot;, name );
  Predicate agePredicate = Predicates.equal( &quot;age&quot;, age );
  Predicate predicate = Predicates.or( namePredicate, agePredicate );
  return personMap.values( predicate );
}
</code></pre>
<pre><code class="lang-java">public Set&lt;Person&gt; getNotWithName( String name ) {
  Predicate namePredicate = Predicates.equal( &quot;name&quot;, name );
  Predicate predicate = Predicates.not( namePredicate );
  return personMap.values( predicate );
}
</code></pre>
<a name="simplifying-with-predicatebuilder"></a><h4 id="simplifying-with-predicatebuilder">Simplifying with PredicateBuilder</h4>
<p>You can simplify predicate usage with the <code>PredicateBuilder</code> class, which offers simpler predicate building. Please see the
below example code which selects all people with a certain name and age.</p>
<pre><code class="lang-java">public Set&lt;Person&gt; getWithNameAndAgeSimplified( String name, int age ) {
  EntryObject e = new PredicateBuilder().getEntryObject();
  Predicate agePredicate = e.get( &quot;age&quot; ).equal( age );
  Predicate predicate = e.get( &quot;name&quot; ).equal( name ).and( agePredicate );
  return personMap.values( predicate );
}
</code></pre>

<a name="querying-with-sql"></a><h3 id="querying-with-sql">Querying with SQL</h3>
<p><code>com.hazelcast.query.SqlPredicate</code> takes the regular SQL <code>where</code> clause. Here is an example:</p>
<pre><code class="lang-java">IMap&lt;Employee&gt; map = hazelcastInstance.getMap( &quot;employee&quot; );
Set&lt;Employee&gt; employees = map.values( new SqlPredicate( &quot;active AND age &lt; 30&quot; ) );
</code></pre>
<a name="supported-sql-syntax"></a><h4 id="supported-sql-syntax">Supported SQL Syntax</h4>
<p><strong>AND/OR:</strong> <code>&lt;expression&gt; AND &lt;expression&gt; AND &lt;expression&gt;...</code></p>
<ul>
<li><code>active AND age&gt;30</code></li>
<li><code>active=false OR age = 45 OR name = Joe</code></li>
<li><code>active AND ( age &gt; 20 OR salary &lt; 60000 )</code>
<br><br></li>
</ul>
<p><strong>Equality:</strong> <code>=, !=, &lt;, &lt;=, &gt;, &gt;=</code></p>
<ul>
<li><code>&lt;expression&gt; = value</code></li>
<li><code>age &lt;= 30</code></li>
<li><code>name = Joe</code></li>
<li><code>salary != 50000</code>
<br><br></li>
</ul>
<p><strong>BETWEEN:</strong> <code>&lt;attribute&gt; [NOT] BETWEEN &lt;value1&gt; AND &lt;value2&gt;</code></p>
<ul>
<li><code>age BETWEEN 20 AND 33 ( same as age &gt;= 20  AND age &lt;= 33 )</code></li>
<li><code>age NOT BETWEEN 30 AND 40 ( same as age &lt; 30 OR age &gt; 40 )</code>
<br><br></li>
</ul>
<p><strong>IN:</strong> <code>&lt;attribute&gt; [NOT] IN (val1, val2,...)</code></p>
<ul>
<li><code>age IN ( 20, 30, 40 )</code></li>
<li><code>age NOT IN ( 60, 70 )</code></li>
<li><code>active AND ( salary &gt;= 50000 OR ( age NOT BETWEEN 20 AND 30 ) )</code></li>
<li><code>age IN ( 20, 30, 40 ) AND salary BETWEEN ( 50000, 80000 )</code>
<br><br></li>
</ul>
<p><strong>LIKE:</strong> <code>&lt;attribute&gt; [NOT] LIKE &quot;expression&quot;</code></p>
<p>The <code>%</code> (percentage sign) is placeholder for multiple characters, an <code>_</code> (underscore) is placeholder for only one character.</p>
<ul>
<li><code>name LIKE Jo%</code> (true for &#39;Joe&#39;, &#39;Josh&#39;, &#39;Joseph&#39; etc.)</li>
<li><code>name LIKE Jo_</code> (true for &#39;Joe&#39;; false for &#39;Josh&#39;)</li>
<li><code>name NOT LIKE Jo_</code> (true for &#39;Josh&#39;; false for &#39;Joe&#39;)</li>
<li><code>name LIKE J_s%</code> (true for &#39;Josh&#39;, &#39;Joseph&#39;; false &#39;John&#39;, &#39;Joe&#39;)
<br><br></li>
</ul>
<p><strong>ILIKE:</strong> <code>&lt;attribute&gt; [NOT] ILIKE expression</code></p>
<p>Similar to LIKE predicate but in a case-insensitive manner.</p>
<ul>
<li><code>name ILIKE Jo%</code> (true for &#39;Joe&#39;, &#39;joe&#39;, &#39;jOe&#39;,&#39;Josh&#39;,&#39;joSH&#39;, etc.)</li>
<li><code>name ILIKE Jo_</code> (true for &#39;Joe&#39; or &#39;jOE&#39;; false for &#39;Josh&#39;)
<br><br></li>
</ul>
<p><strong>REGEX</strong>: <code>&lt;attribute&gt; [NOT] REGEX expression</code></p>
<ul>
<li><code>name REGEX abc-.*</code> (true for &#39;abc-123&#39;; false for &#39;abx-123&#39;)</li>
</ul>
<a name="querying-entry-keys-with-predicates"></a><h4 id="querying-entry-keys-with-predicates">Querying Entry Keys with Predicates</h4>
<p>You can use <code>__key</code> attribute to perform a predicated search for entry keys. Please see the following example:</p>
<pre><code class="lang-java">IMap&lt;String, Person&gt; personMap = hazelcastInstance.getMap(persons);
personMap.put(&quot;Alice&quot;, new Person(&quot;Alice&quot;, 35, Gender.FEMALE));
personMap.put(&quot;Andy&quot;,  new Person(&quot;Andy&quot;,  37, Gender.MALE));
personMap.put(&quot;Bob&quot;,   new Person(&quot;Bob&quot;,   22, Gender.MALE));
[...]
Predicate predicate = new SqlPredicate(&quot;__key like A%&quot;);
Collection&lt;Person&gt; startingWithA = personMap.values(predicate);
</code></pre>
<p>In this example, the code creates a collection with the entries whose keys start with the letter &quot;A&quot;.</p>

<a name="filtering-with-paging-predicates"></a><h3 id="filtering-with-paging-predicates">Filtering with Paging Predicates</h3>
<p>Hazelcast provides paging for defined predicates. With its <code>PagingPredicate</code> class, you can
get a collection of keys, values, or entries page by page by filtering them with predicates and giving the size of the pages. Also, you
can sort the entries by specifying comparators.</p>
<p>In the example code below:</p>
<ul>
<li>The <code>greaterEqual</code> predicate gets values from the &quot;students&quot; map. This predicate has a filter
to retrieve the objects with an &quot;age&quot; greater than or equal to 18. </li>
<li>Then a <code>PagingPredicate</code> is constructed in which the page size is 5, so there will be five objects in each page. 
The first time the values are called creates the first page. </li>
<li>It gets subsequent pages with the <code>nextPage()</code>
method of <code>PagingPredicate</code> and querying the map again with the updated <code>PagingPredicate</code>.</li>
</ul>
<pre><code class="lang-java">IMap&lt;Integer, Student&gt; map = hazelcastInstance.getMap( &quot;students&quot; );
Predicate greaterEqual = Predicates.greaterEqual( &quot;age&quot;, 18 );
PagingPredicate pagingPredicate = new PagingPredicate( greaterEqual, 5 );
// Retrieve the first page
Collection&lt;Student&gt; values = map.values( pagingPredicate );
...
// Set up next page
pagingPredicate.nextPage();
// Retrieve next page
values = map.values( pagingPredicate );
...
</code></pre>
<p>If a comparator is not specified for <code>PagingPredicate</code>, but you want to get a collection of keys or values page by page, this collection must be an instance of <code>Comparable</code> (i.e., it must implement <code>java.lang.Comparable</code>). Otherwise, the <code>java.lang.IllegalArgument</code> exception is thrown.</p>
<p>Starting with Hazelcast 3.6, you can also access a specific page more easily with the help of the method <code>setPage()</code>. This way, if you make a query for the hundredth page, for example, it will get all 100 pages at once instead of reaching the hundredth page one by one using the method <code>nextPage()</code>. Please note that this feature tires the memory and refer to the <a href="https://github.com/hazelcast/hazelcast/blob/66263987a7bf4bec20217f3c555381a51712d017/hazelcast/src/main/java/com/hazelcast/query/PagingPredicate.java">PagingPredicate class</a>.</p>
<p>Paging Predicate, also known as Order &amp; Limit, is not supported in Transactional Context.
<br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please see the
<a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/query/Predicates.java" target="_blank">
Predicates class</a> for all predicates provided.</em></p>

<a name="indexing-queries"></a><h3 id="indexing-queries">Indexing Queries</h3>
<p>Hazelcast distributed queries will run on each member in parallel and will return only the results to the caller.
Then, on the caller side, the results will be merged.</p>
<p>When a query runs on a
member, Hazelcast will iterate through all the owned entries and find the matching ones. This can be made faster by indexing
the mostly queried fields, just like you would do for your database. Indexing will add overhead for each <code>write</code>
operation but queries will be a lot faster. If you query your map a lot, make sure to add indexes for the most frequently
queried fields. For example, if you do an <code>active and age &lt; 30</code> query, make sure you add an index for the <code>active</code> and
<code>age</code> fields. The following example code does that by:</p>
<ul>
<li>getting the map from the Hazelcast instance, and</li>
<li>adding indexes to the map with the IMap <code>addIndex</code> method.</li>
</ul>
<pre><code class="lang-java">IMap map = hazelcastInstance.getMap( &quot;employees&quot; );
// ordered, since we have ranged queries for this field
map.addIndex( &quot;age&quot;, true );
// not ordered, because boolean field cannot have range
map.addIndex( &quot;active&quot;, false );
</code></pre>
<a name="indexing-ranged-queries"></a><h4 id="indexing-ranged-queries">Indexing Ranged Queries</h4>
<p><code>IMap.addIndex(fieldName, ordered)</code> is used for adding index. For each indexed field, if you have ranged queries such as <code>age&gt;30</code>,
<code>age BETWEEN 40 AND 60</code>, then you should set the <code>ordered</code> parameter to <code>true</code>. Otherwise, set it to <code>false</code>.</p>
<a name="configuring-imap-indexes"></a><h4 id="configuring-imap-indexes">Configuring IMap Indexes</h4>
<p>Also, you can define <code>IMap</code> indexes in configuration. An example is shown below.</p>
<pre><code class="lang-xml">&lt;map name=&quot;default&quot;&gt;
  ...
  &lt;indexes&gt;
    &lt;index ordered=&quot;false&quot;&gt;name&lt;/index&gt;
    &lt;index ordered=&quot;true&quot;&gt;age&lt;/index&gt;
  &lt;/indexes&gt;
&lt;/map&gt;
</code></pre>
<p>You can also define <code>IMap</code> indexes using programmatic configuration, as in the example below.</p>
<pre><code class="lang-java">mapConfig.addMapIndexConfig( new MapIndexConfig( &quot;name&quot;, false ) );
mapConfig.addMapIndexConfig( new MapIndexConfig( &quot;age&quot;, true ) );
</code></pre>
<p>The following is the Spring declarative configuration for the same sample.</p>
<pre><code class="lang-xml">&lt;hz:map name=&quot;default&quot;&gt;
  &lt;hz:indexes&gt;
    &lt;hz:index attribute=&quot;name&quot;/&gt;
    &lt;hz:index attribute=&quot;age&quot; ordered=&quot;true&quot;/&gt;
  &lt;/hz:indexes&gt;
&lt;/hz:map&gt;
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Non-primitive types to be indexed should implement </em><code>Comparable</code><em>.</em></p>

<a name="configuring-query-thread-pool"></a><h3 id="configuring-query-thread-pool">Configuring Query Thread Pool</h3>
<p>You can change the size of thread pool dedicated to query operations using the <code>pool-size</code> property. Each query consumes a single thread from a Generic Operations ThreadPool on each Hazelcast member - let&#39;s call it the query-orchestrating thread.  That thread is blocked throughout the whole execution-span of a query on the member.</p>
<p>The query-orchestrating thread will use the threads from the query-thread pool in two cases:</p>
<ul>
<li>if you run a <code>PagingPredicate</code> - since each page is run as a separate task,</li>
<li>if you set the system property <code>hazelcast.query.predicate.parallel.evaluation</code> to true - since the predicates are evaluated in parallel.</li>
</ul>
<p>Please see <a href="#filtering-with-paging-predicates">Filtering with Paging Predicates</a> and <a href="#system-properties">System Properties</a> sections for information on paging predicates and for description of the above system property.</p>
<p>Below is an example of that declarative configuration.</p>
<pre><code class="lang-xml">&lt;executor-service name=&quot;hz:query&quot;&gt;
  &lt;pool-size&gt;100&lt;/pool-size&gt;
&lt;/executor-service&gt;
</code></pre>
<p>Below is the equivalent programmatic configuration.</p>
<pre><code class="lang-java">Config cfg = new Config();
cfg.getExecutorConfig(&quot;hz:query&quot;).setPoolSize(100);
</code></pre>

<a name="querying-in-collections-and-arrays"></a><h2 id="querying-in-collections-and-arrays">Querying in Collections and Arrays</h2>
<p>Hazelcast allows querying in collections and arrays.
Querying in collections and arrays is compatible with all Hazelcast serialization methods, including the Portable serialization.</p>
<p>Let&#39;s have a look at the following data structure expressed in pseudo-code:</p>
<pre><code class="lang-java">class Motorbike {
    Wheel wheels[2];
}

class Wheel {
   String name;

}
</code></pre>
<p>In order to query a single element of a collection/array, you can execute the following query:</p>
<pre><code class="lang-java">// it matches all motorbikes where the zero wheel&#39;s name is &#39;front-wheel&#39;
Predicate p = Predicates.equals(&#39;wheels[0].name&#39;, &#39;front-wheel&#39;);
Collection&lt;Motorbike&gt; result = map.values(p);
</code></pre>
<p>It is also possible to query a collection/array using the <code>any</code> semantic as shown below:</p>
<pre><code class="lang-java">// it matches all motorbikes where any wheel&#39;s name is &#39;front-wheel&#39;
Predicate p = Predicates.equals(&#39;wheels[any].name&#39;, &#39;front&#39;);
Collection&lt;Motorbike&gt; result = map.values(p);
</code></pre>
<p>The exact same query may be executed using the <code>SQLPredicate</code> as shown below:</p>
<pre><code>Predicate p = new SQLPredicate(&#39;wheels[any].name&#39;, &#39;front&#39;);
Collection&lt;Motorbike&gt; result = map.values(p);
</code></pre><p><code>[]</code> notation applies to both collections and arrays.</p>
<a name="indexing-in-collections-and-arrays"></a><h3 id="indexing-in-collections-and-arrays">Indexing in Collections and Arrays</h3>
<p>You can also create an index using a query in collections and arrays.</p>
<p>Please note that in order to leverage the index, the attribute name used in the query has to be the same as the one used
in the index definition.</p>
<p>Let&#39;s assume you have the following index definition:</p>
<pre><code class="lang-xml">&lt;indexes&gt;
  &lt;index ordered=&quot;false&quot;&gt;wheels[any].name&lt;/index&gt;
&lt;/indexes&gt;
</code></pre>
<p>The following query will use the index:</p>
<pre><code class="lang-java">Predicate p = Predicates.equals(&#39;wheels[any].name&#39;, &#39;front-wheel&#39;);
</code></pre>
<p>The following query, however, will NOT leverage the index, since it does not use exactly the same attribute name that
was used in the index:</p>
<pre><code class="lang-java">Predicates.equals(&#39;wheels[0].name&#39;, &#39;front-wheel&#39;)
</code></pre>
<p>In order to use the index in the case mentioned above, you have to create another index, as shown below:</p>
<pre><code class="lang-xml">&lt;indexes&gt;
  &lt;index ordered=&quot;false&quot;&gt;wheels[0].name&lt;/index&gt;
&lt;/indexes&gt;
</code></pre>
<a name="corner-cases"></a><h3 id="corner-cases">Corner cases</h3>
<p>Handling of corner cases may be a bit different than in a programming language like <code>Java</code>.</p>
<p>Let&#39;s have a look at the following examples in order to understand the differences.
To make the analysis simpler, let&#39;s assume that there is only one <code>Motorbike</code> object stored in a Hazelcast Map.</p>
<table>
<thead>
<tr>
<th>Id</th>
<th>Query</th>
<th>Data state</th>
<th>Extraction Result</th>
<th>Match</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Predicates.equals(&#39;wheels[7].name&#39;, &#39;front-wheel&#39;)</td>
<td>wheels.size() == 1</td>
<td>null</td>
<td>No</td>
</tr>
<tr>
<td>2</td>
<td>Predicates.equals(&#39;wheels[7].name&#39;, null)</td>
<td>wheels.size() == 1</td>
<td>null</td>
<td>Yes</td>
</tr>
<tr>
<td>3</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, &#39;front-wheel&#39;)</td>
<td>wheels[0].name == null</td>
<td>null</td>
<td>No</td>
</tr>
<tr>
<td>4</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, null)</td>
<td>wheels[0].name == null</td>
<td>null</td>
<td>Yes</td>
</tr>
<tr>
<td>5</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, &#39;front-wheel&#39;)</td>
<td>wheels[0] == null</td>
<td>null</td>
<td>No</td>
</tr>
<tr>
<td>6</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, null)</td>
<td>wheels[0] == null</td>
<td>null</td>
<td>Yes</td>
</tr>
<tr>
<td>7</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, &#39;front-wheel&#39;)</td>
<td>wheels == null</td>
<td>null</td>
<td>No</td>
</tr>
<tr>
<td>8</td>
<td>Predicates.equals(&#39;wheels[0].name&#39;, null)</td>
<td>wheels == null</td>
<td>null</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>As you can see, <strong>no</strong> <code>NullPointerException</code>s or <code>IndexOutOfBoundException</code>s are thrown in the extraction process, even
though parts of the expression are <code>null</code>.</p>
<p>Looking at examples 4, 6 and 8, we can also easily notice that it is impossible to distinguish which part of the
expression was null.
If we execute the following query <code>wheels[1].name = null</code>, it may be evaluated to true because:</p>
<ul>
<li><code>wheels</code> collection/array is null.</li>
<li><code>index == 1</code> is out of bound.</li>
<li><code>name</code> attribute of the wheels[1] object is <code>null</code>.</li>
</ul>
<p>In order to make the query unambiguous, extra conditions would have to be added, e.g.,
<code>wheels != null AND wheels[1].name = null</code>.</p>

<a name="custom-attributes"></a><h2 id="custom-attributes">Custom Attributes</h2>
<p>It is possible to define a custom attribute that may be referenced in predicates, queries and indexes.</p>
<p>A custom attribute is a &quot;synthetic&quot; attribute that does not exist as a <code>field</code> or a <code>getter</code> in the object that it is extracted from.
Thus, it is necessary to define the policy on how the attribute is supposed to be extracted.
Currently the only way to extract a custom attribute is to implement a <code>com.hazelcast.query.extractor.ValueExtractor</code>
that encompasses the extraction logic.</p>
<p>Custom Attributes are compatible with all Hazelcast serialization methods, including the Portable serialization.</p>
<a name="implementing-a-valueextractor"></a><h3 id="implementing-a-valueextractor">Implementing a ValueExtractor</h3>
<p>In order to implement a <code>ValueExtractor</code>, extend the abstract <code>com.hazelcast.query.extractor.ValueExtractor</code> class
and implement the <code>extract()</code> method.</p>
<p>The <code>ValueExtractor</code> interface looks as follows:</p>
<pre><code class="lang-java">/***
 * Common superclass for all extractors.
 *
 * @param &lt;T&gt; type of the target object to extract the value from
 * @param &lt;A&gt; type of the extraction argument object passed to the extract() method
 *
 */
public abstract class ValueExtractor&lt;T, A&gt; {

    /**
     * Extracts custom attribute&#39;s value from the given target object.
     *
     * @param target    object to extract the value from
     * @param argument  extraction argument
     * @param collector collector of the extracted value(s)
     *
     */
    public abstract void extract(T target, A argument, ValueCollector collector);

}
</code></pre>
<p>The <code>extract()</code> method does not return any value since the extracted value is collected by the <code>ValueCollector</code>.
In order to return multiple results from a single extraction, invoke the <code>ValueCollector.collect()</code> method
multiple times, so that the collector collects all results.</p>
<p>Here is the <code>ValueCollector</code> contract:</p>
<pre><code class="lang-java">/**
 * Enables collecting values extracted by a {@see com.hazelcast.query.extractor.ValueExtractor}
 */
public abstract class ValueCollector {

    /**
     * Collects a value extracted by a ValueExtractor.
     * &lt;p/&gt;
     * More than one value may be collected in a single extraction
     *
     * @param value value to be collected
     */
    public abstract void addObject(Object value);

}
</code></pre>
<a name="valueextractor-with-portable-serialization"></a><h4 id="valueextractor-with-portable-serialization">ValueExtractor with Portable Serialization</h4>
<p>Portable serialization is a special kind of serialization where there is no need to have the class of the serialized object on the
classpath in order to read its attributes. That is the reason why the target object passed to the <code>ValueExtractor.extract()</code>
method will not be of the exact type that has been stored. Instead, an instance of a <code>com.hazelcast.query.extractor.ValueReader</code> will be passed.
<code>ValueReader</code> enables reading the attributes of a Portable object in a generic and type-agnostic way.
It contains two methods:</p>
<ul>
<li><code>read(String path, ValueCollector&lt;T&gt; collector)</code> - enables passing all results directly to the <code>ValueCollector</code>.</li>
<li><code>read(String path, ValueCallback&lt;T&gt; callback)</code> - enables filtering, transforming and grouping the result of the read operation and manually passing it to the <code>ValueCollector</code>.</li>
</ul>
<p>Here is the <code>ValueReader</code> contract:</p>
<pre><code class="lang-java">/**
 * Enables reading the value of the attribute specified by the path
 * &lt;p&gt;
 * The path may be:
 * - simple -&gt; it includes a single attribute only, like &quot;name&quot;
 * - nested -&gt; it includes more then a single attribute separated with a dot (.), e.g. person.address.city
 * &lt;p&gt;
 * The path may also includes array cells:
 * - specific quantifier, like person.leg[1] -&gt; returns the leg with index 1
 * - wildcard quantifier, like person.leg[any] -&gt; returns all legs
 * &lt;p&gt;
 * The wildcard quantifier may be used a couple of times, like person.leg[any].finger[any] which returns all fingers
 * from all legs.
 */
public abstract class ValueReader {

    /**
     * Read the value of the attribute specified by the path and returns the result via the callback.
     *
     */
    public abstract &lt;T&gt; void read(String path, ValueCallback&lt;T&gt; callback) throws ValueReadingException;

    /**
     * Read the value of the attribute specified by the path and returns the result directly to the collector.
     *
     */
    public abstract &lt;T&gt; void read(String path, ValueCollector&lt;T&gt; collector) throws ValueReadingException;

}
</code></pre>
<a name="returning-multiple-values-from-a-single-extraction"></a><h4 id="returning-multiple-values-from-a-single-extraction">Returning Multiple Values from a Single Extraction</h4>
<p>It sounds counter-intuitive, but a single extraction may return multiple values when arrays or collections are
involved.
Let&#39;s have a look at the following data structure in pseudo-code:</p>
<pre><code class="lang-java">class Motorbike {
    Wheel wheel[2];
}

class Wheel {
    String name;
}
</code></pre>
<p>Let&#39;s assume that we want to extract the names of all wheels from a single motorbike object. Each motorbike has two
wheels so there are two names for each bike. In order to return both values from the extraction operation, collect them
separately using the <code>ValueCollector</code>. Collecting multiple values in this way allows you to operate on these multiple
values as if they were single values during the evaluation of the predicates.</p>
<p>Let&#39;s assume that we registered a custom extractor with the name <code>wheelName</code> and executed the following query:
<code>wheelName = front-wheel</code>.</p>
<p>The extraction may return up to two wheel names for each <code>Motorbike</code> since each <code>Motorbike</code> has up to two wheels.
In such a case, it is enough if a single value evaluates the predicate&#39;s condition to true to return a match, so
it will return a <code>Motorbike</code> if &quot;any&quot; of the wheels matches the expression.</p>
<a name="extraction-arguments"></a><h3 id="extraction-arguments">Extraction Arguments</h3>
<p>A <code>ValueExtractor</code> may use a custom argument if it is specified in the query.
The custom argument may be passed within the square brackets located after the name of the custom attribute,
e.g., <code>customAttribute[argument]</code>.</p>
<p>Let&#39;s have a look at the following query: <code>currency[incoming] == EUR</code>
The <code>currency</code> is a custom attribute that uses a <code>com.test.CurrencyExtractor</code> for extraction.</p>
<p>The string <code>incoming</code> is an argument that will be passed to the <code>ArgumentParser</code> during the extraction.
The parser will parse the string according to the parser&#39;s custom logic and it will return a parsed object.
The parsed object may be a single object, array, collection, or any arbitrary object.
It is up to the <code>ValueExtractor</code>&#39;s implementor to understand the semantics of the parsed argument object.</p>
<p>For now it is <strong>not</strong> possible to register a custom <code>ArgumentParser</code>, thus a default parser is used.
It follows a <code>pass-through</code> semantic, which means that the string located in the square brackets is passed &quot;as is&quot; to
the <code>ValueExtractor.extract()</code> method.</p>
<p>Please note that using square brackets within the argument string is not allowed.</p>
<a name="configuring-a-custom-attribute-programmatically"></a><h3 id="configuring-a-custom-attribute-programmatically">Configuring a Custom Attribute Programmatically</h3>
<p>The following snippet demonstrates how to define a custom attribute using a <code>ValueExtractor</code>.</p>
<pre><code class="lang-java">MapAttributeConfig attributeConfig = new MapAttributeConfig();
attributeConfig.setName(&quot;currency&quot;);
attributeConfig.setExtractor(&quot;com.bank.CurrencyExtractor&quot;);

MapConfig mapConfig = new MapConfig();
mapConfig.addMapAttributeConfig(attributeConfig);
</code></pre>
<p><code>currency</code> is the name of the custom attribute that will be extracted using the <code>CurrencyExtractor</code> class.</p>
<p>Keep in mind that an extractor may not be added after the map has been instantiated.
All extractors have to be defined upfront in the map&#39;s initial configuration.</p>
<a name="configuring-a-custom-attribute-declaratively"></a><h3 id="configuring-a-custom-attribute-declaratively">Configuring a Custom Attribute Declaratively</h3>
<p>The following snippet demonstrates how to define a custom attribute in the Hazelcast XML Configuration.</p>
<pre><code class="lang-xml">&lt;map name=&quot;trades&quot;&gt;
   &lt;attributes&gt;
       &lt;attribute extractor=&quot;com.bank.CurrencyExtractor&quot;&gt;currency&lt;/attribute&gt;
   &lt;/attributes&gt;
&lt;/map&gt;
</code></pre>
<p>Analogous to the example above, <code>currency</code> is the name of the custom attribute that will be extracted using the
<code>CurrencyExtractor</code> class.</p>
<p>Please note that an attribute name may begin with an ASCII letter [A-Za-z] or digit [0-9] and may contain
ASCII letters [A-Za-z], digits [0-9] or underscores later on.</p>
<a name="indexing-custom-attributes"></a><h3 id="indexing-custom-attributes">Indexing Custom Attributes</h3>
<p>You can create an index using a custom attribute.</p>
<p>The name of the attribute used in the index definition has to match the one used in the attributes configuration.</p>
<p>Defining indexes with extraction arguments is allowed, as shown in the example below:</p>
<pre><code class="lang-xml">&lt;indexes&gt;
    &lt;!-- custom attribute without an extraction argument --&gt;
    &lt;index ordered=&quot;true&quot;&gt;currency&lt;/index&gt;

    &lt;!-- custom attribute using an extraction argument --&gt;
    &lt;index ordered=&quot;true&quot;&gt;currency[EUR]&lt;/index&gt;
&lt;/indexes&gt;
</code></pre>

<a name="mapreduce"></a><h2 id="mapreduce">MapReduce</h2>
<p>You have likely heard about MapReduce ever since Google released its <a href="http://research.google.com/archive/mapreduce.html" target="_blank">research white paper</a>  on this concept. With Hadoop as the most common and well known implementation, MapReduce gained a broad audience and made it into all kinds of business applications dominated by data warehouses.</p>
<p>MapReduce is a software framework for processing large amounts of data in a distributed way. Therefore, the processing is normally spread over several machines. The basic idea behind MapReduce is that source data is mapped into a collection of key-value pairs and reducing those pairs, grouped by key, in a second
step towards the final result.</p>
<p>The main idea can be summarized with the following steps.</p>
<ol>
<li>Read the source data.</li>
<li>Map the data to one or multiple key-value pairs.</li>
<li>Reduce all pairs with the same key.</li>
</ol>
<p><strong>Use Cases</strong></p>
<p>The best known examples for MapReduce algorithms are text processing tools, such as counting the word frequency in large texts or websites. Apart from that, there are more interesting examples of use cases listed below.</p>
<ul>
<li>Log Analysis</li>
<li>Data Querying</li>
<li>Aggregation and summing</li>
<li>Distributed Sort</li>
<li>ETL (Extract Transform Load)</li>
<li>Credit and Risk management</li>
<li>Fraud detection</li>
<li>and more.</li>
</ul>

<a name="understanding-mapreduce"></a><h3 id="understanding-mapreduce">Understanding MapReduce</h3>
<p>This section will give deeper insight into the MapReduce pattern and will help you understand the semantics behind the different MapReduce phases and how they are implemented in Hazelcast.</p>
<p>In addition to this, the following sections compare Hadoop and Hazelcast MapReduce implementations to help adopters with Hadoop backgrounds quickly get familiar with Hazelcast MapReduce.</p>
<a name="mapreduce-workflow-example"></a><h4 id="mapreduce-workflow-example">MapReduce Workflow Example</h4>
<p>The flowchart below demonstrates the basic workflow of the word count example (distributed occurrences analysis) mentioned in the <a href="#mapreduce">MapReduce section</a> introduction. From left to right, it iterates over all the entries of a data structure (in this case an IMap). In the mapping phase, it splits the sentence into single words and emits a key-value pair per word: the word is the key, 1 is the value. In the next phase, values are collected (grouped) and transported to their
corresponding reducers, where they are eventually reduced to a single key-value pair, the value being the number of occurrences of the word. At the last step, the different reducer results are grouped up to the final result and returned to the requester.</p>
<p><img src="images/Workflow.png" alt="MapReduce Workflow Example"></p>
<p>In pseudo code, the corresponding map and reduce function would look like the following. A Hazelcast code example will be shown in the next section.</p>
<pre><code class="lang-plain">map( key:String, document:String ):Void -&gt;
  for each w:word in document:
    emit( w, 1 )

reduce( word:String, counts:List[Int] ):Int -&gt;
  return sum( counts )
</code></pre>
<a name="mapreduce-phases"></a><h4 id="mapreduce-phases">MapReduce Phases</h4>
<p>As seen in the workflow example, a MapReduce process consists of multiple phases. The original MapReduce pattern describes two phases (map, reduce) and one optional phase (combine). In Hazelcast, these phases either only exist virtually to explain the data flow, or are executed in parallel during the real operation while the general idea is still persisting.</p>
<p>(K x V)* -&gt; (L x W)*</p>
<p>[(k<em>1</em>, v<em>1</em>), ..., (k<em>n</em>, v<em>n</em>)] -&gt; [(l<em>1</em>, w<em>1</em>), ..., (l<em>m</em>, w<em>m</em>)]</p>
<p><strong>Mapping Phase</strong></p>
<p>The mapping phase iterates all key-value pairs of any kind of legal input source. The mapper then analyzes the input pairs and emits zero or more new key-value pairs.</p>
<p>K x V -&gt; (L x W)*</p>
<p>(k, v) -&gt; [(l<em>1</em>, w<em>1</em>), ..., (l<em>n</em>, w<em>n</em>)]</p>
<p><strong>Combine Phase</strong></p>
<p>In the combine phase, multiple key-value pairs with the same key are collected and combined to an intermediate result before being sent to the reducers. <strong>Combine phase is also optional in Hazelcast, but is highly recommended to lower the traffic.</strong></p>
<p>In terms of the word count example, this can be explained using the sentences &quot;Saturn is a planet but the Earth is a planet, too&quot;. As shown above, we would send two key-value pairs (planet, 1). The registered combiner now collects those two pairs and combines them into an intermediate result of (planet, 2). Instead of two key-value
pairs sent through the wire, there is now only one for the key &quot;planet&quot;.</p>
<p>The pseudo code for a combiner is similar to the reducer.</p>
<pre><code class="lang-text">combine( word:String, counts:List[Int] ):Void -&gt;
  emit( word, sum( counts ) )
</code></pre>
<p><strong>Grouping / Shuffling Phase</strong></p>
<p>The grouping or shuffling phase only exists virtually in Hazelcast since it is not a real phase; emitted key-value pairs with the same key are always transferred to the same reducer in the same job. They are grouped together, which is equivalent to the shuffling phase.</p>
<p><strong>Reducing Phase</strong></p>
<p>In the reducing phase, the collected intermediate key-value pairs are reduced by their keys to build the final by-key result. This value can be a sum of all the emitted values of the same key, an average value, or something completely different, depending on the use case.</p>
<p>Here is a reduced representation of this phase.</p>
<p>L x W* -&gt; X*</p>
<p>(l, [w<em>1</em>, ..., w<em>n</em>]) -&gt; [x<em>1</em>, ..., x<em>n</em>]</p>
<p><strong>Producing the Final Result</strong></p>
<p>This is not a real MapReduce phase, but it is the final step in Hazelcast after all reducers are notified that reducing has finished. The original job initiator then requests all reduced results and builds the final result.</p>
<a name="additional-mapreduce-resources"></a><h4 id="additional-mapreduce-resources">Additional MapReduce Resources</h4>
<p>The Internet is full of useful resources for finding deeper information on MapReduce. Below is a short collection of more introduction material. In addition, there are books written about all kinds of MapReduce patterns and how to write a MapReduce function for your use case. To name them all is out of the scope of this documentation, but here are some resources:</p>
<ul>
<li><a href="http://research.google.com/archive/mapreduce.html" target="_blank">http://research.google.com/archive/mapreduce.html</a></li>
<li><a href="http://en.wikipedia.org/wiki/MapReduce" target="_blank">http://en.wikipedia.org/wiki/MapReduce</a></li>
<li><a href="http://hci.stanford.edu/courses/cs448g/a2/files/map_reduce_tutorial.pdf" target="_blank">http://hci.stanford.edu/courses/cs448g/a2/files/map_reduce_tutorial.pdf</a></li>
<li><a href="http://ksat.me/map-reduce-a-really-simple-introduction-kloudo/" target="_blank">http://ksat.me/map-reduce-a-really-simple-introduction-kloudo/</a></li>
<li><a href="http://www.slideshare.net/franebandov/an-introduction-to-mapreduce-6789635" target="_blank">http://www.slideshare.net/franebandov/an-introduction-to-mapreduce-6789635</a></li>
</ul>

<a name="using-the-mapreduce-api"></a><h3 id="using-the-mapreduce-api">Using the MapReduce API</h3>
<p>This section explains the basics of the Hazelcast MapReduce framework. While walking through the different API classes, we will build the <a href="#understanding-mapreduce">word count example that was discussed earlier</a> and create it step by step.</p>
<p>The Hazelcast API for MapReduce operations consists of a fluent DSL-like configuration syntax to build and submit jobs. <code>JobTracker</code> is the basic entry point to all MapReduce operations and is retrieved from <code>com.hazelcast.core.HazelcastInstance</code> by calling <code>getJobTracker</code> and supplying the name of the required <code>JobTracker</code> configuration. The configuration for <code>JobTracker</code>s will be discussed later; for now we focus on the API itself.
In addition, the complete submission part of the API is built to support a fully reactive way of programming.</p>
<p>To give an easy introduction to people used to Hadoop, we created the class names to be as familiar as possible to their counterparts on Hadoop. That means while most users will recognize a lot of similar sounding classes, the way to configure the jobs is more fluent due to the DSL-like styled API.</p>
<p>While building the example, we will go through as many options as possible, e.g., we will create a specialized <code>JobTracker</code> configuration (at the end). Special <code>JobTracker</code> configuration is not required, because for all other Hazelcast features you can use &quot;default&quot; as the configuration name. However, special configurations offer better options to predict behavior of the framework execution.</p>
<p>The full example is available <a href="http://github.com/noctarius/hz-map-reduce" target="_blank">here</a> as a ready to run Maven project.</p>
<a name="retrieving-a-jobtracker-instance"></a><h4 id="retrieving-a-jobtracker-instance">Retrieving a JobTracker Instance</h4>
<p><code>JobTracker</code> creates Job instances, whereas every instance of <code>com.hazelcast.mapreduce.Job</code> defines a single MapReduce configuration. The same Job can be submitted multiple times regardless of whether it is executed in parallel or after the previous execution is finished.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>After retrieving the <code>JobTracker</code>, be aware that it should only be used with data structures derived from the same HazelcastInstance. Otherwise, you can get unexpected behavior.</em></p>
<p>To retrieve a <code>JobTracker</code> from Hazelcast, we will start by using the &quot;default&quot; configuration for convenience reasons to show the basic way.</p>
<pre><code class="lang-java">import com.hazelcast.mapreduce.*;

JobTracker jobTracker = hazelcastInstance.getJobTracker( &quot;default&quot; );
</code></pre>
<p><code>JobTracker</code> is retrieved using the same kind of entry point as most other Hazelcast features. After building the cluster connection, you use the created HazelcastInstance to request the configured (or default) <code>JobTracker</code> from Hazelcast.</p>
<p>The next step will be to create a new <code>Job</code> and configure it to execute our first MapReduce request against cluster data.</p>
<a name="creating-a-job"></a><h4 id="creating-a-job">Creating a Job</h4>
<p>As mentioned in <a href="#retrieving-a-jobtracker-instance">Retrieving a JobTracker Instance</a>, you create a Job using the retrieved <code>JobTracker</code> instance. A Job defines exactly one configuration of a MapReduce task. Mapper, combiner and reducers will be defined per job. However, since the Job instance is only a configuration, it can be submitted multiple times, regardless of whether executions happen in parallel or one after the other.</p>
<p>A submitted job is always identified using a unique combination of the <code>JobTracker</code>&#39;s name and a jobId generated on submit-time. The way to retrieve the jobId will be shown in one of the later sections.</p>
<p>To create a Job, a second class <code>com.hazelcast.mapreduce.KeyValueSource</code> is necessary. We will have a deeper look at the <code>KeyValueSource</code> class in the next section. <code>KeyValueSource</code> is used to wrap any kind of data or data structure into a well defined set of key-value pairs.</p>
<p>The example code below is a direct follow up to the example in <a href="#retrieving-a-jobTracker-instance">Retrieving a JobTracker Instance</a>, and it reuses the already created HazelcastInstance and <code>JobTracker</code> instances.</p>
<p>The example starts by retrieving an instance of our data map, and then it creates the Job instance. Implementations used to configure the Job will be discussed while walking further through the API documentation.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Since the Job class is highly dependent upon generics to support type safety, the generics change over time and may not be assignment compatible to old variable types. To make use of the full potential of the fluent API, we recommend you use fluent method chaining as shown in this example to prevent the need for too many variables.</em></p>
<pre><code class="lang-java">IMap&lt;String, String&gt; map = hazelcastInstance.getMap( &quot;articles&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromMap( map );
Job&lt;String, String&gt; job = jobTracker.newJob( source );

ICompletableFuture&lt;Map&lt;String, Long&gt;&gt; future = job
    .mapper( new TokenizerMapper() )
    .combiner( new WordCountCombinerFactory() )
    .reducer( new WordCountReducerFactory() )
    .submit();

// Attach a callback listener
future.andThen( buildCallback() );

// Wait and retrieve the result
Map&lt;String, Long&gt; result = future.get();
</code></pre>
<p>As seen above, we create the Job instance and define a mapper, combiner, and reducer. Then we submit the request to the cluster. The <code>submit</code> method returns an ICompletableFuture that can be used to attach our callbacks or to wait for the result to be processed in a blocking fashion.</p>
<p>There are more options available for job configurations, such as defining a general chunk size or on what keys the operation will operate. For more information, please refer to the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/mapreduce/Job.java">Hazelcast source code for Job.java</a>.</p>
<a name="creating-key-value-input-sources-with-keyvaluesource"></a><h4 id="creating-key-value-input-sources-with-keyvaluesource">Creating Key-Value Input Sources with KeyValueSource</h4>
<p><code>KeyValueSource</code> can either wrap Hazelcast data structures (like IMap, MultiMap, IList, ISet) into key-value pair input sources, or build your own custom key-value input source. The latter option makes it possible to feed Hazelcast MapReduce with all kinds of data, such as just-in-time downloaded web page contents or data files. People familiar with Hadoop will recognize similarities with the Input class.</p>
<p>You can imagine a <code>KeyValueSource</code> as a bigger <code>java.util.Iterator</code> implementation. Whereas most methods must be implemented, implementing the <code>getAllKeys</code> method is optional. If implementation is able to gather all keys upfront, it should be implemented and <code>isAllKeysSupported</code> must return <code>true</code>. That way, Job configured KeyPredicates are able to evaluate keys upfront before sending them to the cluster. Otherwise they are serialized and transferred as well, to be evaluated at execution time.</p>
<p>As shown in the example above, the abstract <code>KeyValueSource</code> class provides a number of static methods to easily wrap Hazelcast data structures into <code>KeyValueSource</code> implementations already provided by Hazelcast. The data structures&#39; generics are inherited by the resulting <code>KeyValueSource</code> instance. For data structures like IList or ISet, the key type is always String. While mapping, the key is the data structure&#39;s name, whereas
the value type and value itself are inherited from the IList or ISet itself.</p>
<pre><code class="lang-java">// KeyValueSource from com.hazelcast.core.IMap
IMap&lt;String, String&gt; map = hazelcastInstance.getMap( &quot;my-map&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromMap( map );
</code></pre>
<pre><code class="lang-java">// KeyValueSource from com.hazelcast.core.MultiMap
MultiMap&lt;String, String&gt; multiMap = hazelcastInstance.getMultiMap( &quot;my-multimap&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromMultiMap( multiMap );
</code></pre>
<pre><code class="lang-java">// KeyValueSource from com.hazelcast.core.IList
IList&lt;String&gt; list = hazelcastInstance.getList( &quot;my-list&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromList( list );
</code></pre>
<pre><code class="lang-java">// KeyValueSource from com.hazelcast.core.ISet
ISet&lt;String&gt; set = hazelcastInstance.getSet( &quot;my-set&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromSet( set );
</code></pre>
<p><strong>PartitionIdAware</strong></p>
<p>The <code>com.hazelcast.mapreduce.PartitionIdAware</code> interface can be implemented by the <code>KeyValueSource</code> implementation if the underlying data set is aware of the Hazelcast partitioning schema (as it is for all internal data structures). If this interface is implemented, the same <code>KeyValueSource</code> instance is reused multiple times for all partitions on the cluster member. As a consequence, the <code>close</code> and <code>open</code> methods are also executed
multiple times but once per partitionId.</p>
<a name="implementing-mapping-logic-with-mapper"></a><h4 id="implementing-mapping-logic-with-mapper">Implementing Mapping Logic with Mapper</h4>
<p>Using the <code>Mapper</code> interface, you will implement the mapping logic. Mappers can transform, split, calculate, and aggregate data from data sources. In Hazelcast you can also integrate data from more than the KeyValueSource data source by implementing <code>com.hazelcast.core.HazelcastInstanceAware</code> and requesting additional maps, multimaps, list, and/or sets.</p>
<p>The mappers <code>map</code> function is called once per available entry in the data structure. If you work on distributed data structures that operate in a partition-based fashion, multiple mappers work in parallel on the different cluster members on the members&#39; assigned partitions. Mappers then prepare and maybe transform the input key-value pair and emit zero or more key-value pairs for the reducing phase.</p>
<p>For our word count example, we retrieve an input document (a text document) and we transform it by splitting the text into the available words. After that, as discussed in the <a href="#mapreduce-workflow-example">pseudo code</a>, we emit every single word with a key-value pair with the word as the key and 1 as the value.</p>
<p>A common implementation of that <code>Mapper</code> might look like the following example:</p>
<pre><code class="lang-java">public class TokenizerMapper implements Mapper&lt;String, String, String, Long&gt; {
  private static final Long ONE = Long.valueOf( 1L );

  @Override
  public void map(String key, String document, Context&lt;String, Long&gt; context) {
    StringTokenizer tokenizer = new StringTokenizer( document.toLowerCase() );
    while ( tokenizer.hasMoreTokens() ) {
      context.emit( tokenizer.nextToken(), ONE );
    }
  }
}
</code></pre>
<p>This code splits the mapped texts into their tokens, iterates over the tokenizer as long as there are more tokens, and emits a pair per word. Note that we&#39;re not yet collecting multiple occurrences of the same word, we just fire every word on its own.</p>
<p><strong>LifecycleMapper / LifecycleMapperAdapter</strong></p>
<p>The LifecycleMapper interface or its adapter class LifecycleMapperAdapter can be used to make the Mapper implementation lifecycle aware. That means it will be notified when mapping of a partition or set of data begins and when the last entry was mapped.</p>
<p>Only special algorithms might need those additional lifecycle events to prepare, clean up, or emit additional values.</p>
<a name="minimizing-cluster-traffic-with-combiner"></a><h4 id="minimizing-cluster-traffic-with-combiner">Minimizing Cluster Traffic with Combiner</h4>
<p>As stated in the introduction, a Combiner is used to minimize traffic between the different cluster members when transmitting mapped values from mappers to the reducers. It does this by aggregating multiple values for the same emitted key. This is a fully optional operation, but using it is highly recommended.</p>
<p>Combiners can be seen as an intermediate reducer. The calculated value is always assigned back to the key for which the combiner initially was created. Since combiners are created per emitted key, the Combiner implementation itself is not defined in the jobs configuration; instead, a CombinerFactory that is able to create the expected Combiner instance is created.</p>
<p>Because Hazelcast MapReduce is executing the mapping and reducing phases in parallel, the Combiner implementation must be able to deal with chunked data. Therefore, you must reset its internal state whenever you call <code>finalizeChunk</code>. Calling the <code>finalizeChunk</code> method creates a chunk of intermediate data to be grouped (shuffled) and sent to the reducers.</p>
<p>Combiners can override <code>beginCombine</code> and <code>finalizeCombine</code> to perform preparation or cleanup work.</p>
<p>For our word count example, we are going to have a simple CombinerFactory and Combiner implementation similar to the following example.</p>
<pre><code class="lang-java">public class WordCountCombinerFactory
    implements CombinerFactory&lt;String, Long, Long&gt; {

  @Override
  public Combiner&lt;Long, Long&gt; newCombiner( String key ) {
    return new WordCountCombiner();
  }

  private class WordCountCombiner extends Combiner&lt;Long, Long&gt; {
    private long sum = 0;

    @Override
    public void combine( Long value ) {
      sum++;
    }

    @Override
    public Long finalizeChunk() {
      return sum;
    }

    @Override
    public void reset() {
      sum = 0;
    }
  }
}
</code></pre>
<p>The Combiner must be able to return its current value as a chunk and reset the internal state by setting <code>sum</code> back to 0. Since combiners are always called from a single thread, no synchronization or volatility of the variables is necessary.</p>
<a name="doing-algorithm-work-with-reducer"></a><h4 id="doing-algorithm-work-with-reducer">Doing Algorithm Work with Reducer</h4>
<p>Reducers do the last bit of algorithm work. This can be aggregating values, calculating averages, or any other work that is expected from the algorithm.</p>
<p>Since values arrive in chunks, the <code>reduce</code> method is called multiple times for every emitted value of the creation key. This also can happen multiple times per chunk if no Combiner implementation was configured for a job configuration.</p>
<p>Unlike combiners, a reducer&#39;s <code>finalizeReduce</code> method is only called once per reducer (which means once per key). Therefore, a reducer does not need to reset its internal state at any time.</p>
<p>Reducers can override <code>beginReduce</code> to perform preparation work.</p>
<p>For our word count example, the implementation will look similar to the following code example.</p>
<pre><code class="lang-java">public class WordCountReducerFactory implements ReducerFactory&lt;String, Long, Long&gt; {

  @Override
  public Reducer&lt;Long, Long&gt; newReducer( String key ) {
    return new WordCountReducer();
  }

  private class WordCountReducer extends Reducer&lt;Long, Long&gt; {
    private volatile long sum = 0;

    @Override
    public void reduce( Long value ) {
      sum += value.longValue();
    }

    @Override
    public Long finalizeReduce() {
      return sum;
    }
  }
}
</code></pre>
<a name="reducers-switching-threads"></a><h5 id="reducers-switching-threads">Reducers Switching Threads</h5>
<p>Unlike combiners, reducers tend to switch threads if running out of data to prevent blocking threads from the <code>JobTracker</code> configuration. They are rescheduled at a later point when new data to be processed arrives, but are unlikely to be executed on the same thread as before. As of Hazelcast version 3.3.3 the guarantee for memory visibility on the new thread is ensured by the framework. This means the previous requirement for making fields volatile is dropped.</p>
<a name="modifying-the-result-with-collator"></a><h4 id="modifying-the-result-with-collator">Modifying the Result with Collator</h4>
<p>A Collator is an optional operation that is executed on the job emitting member and is able to modify the finally reduced result before returned to the user&#39;s codebase. Only special use cases are likely to use collators.</p>
<p>For an imaginary use case, we might want to know how many words were all over in the documents we analyzed. For this case, a Collator implementation can be given to the <code>submit</code> method of the Job instance.</p>
<p>A collator would look like the following snippet:</p>
<pre><code class="lang-java">public class WordCountCollator implements Collator&lt;Map.Entry&lt;String, Long&gt;, Long&gt; {

  @Override
  public Long collate( Iterable&lt;Map.Entry&lt;String, Long&gt;&gt; values ) {
    long sum = 0;

    for ( Map.Entry&lt;String, Long&gt; entry : values ) {
      sum += entry.getValue().longValue();
    }
    return sum;
  }
}
</code></pre>
<p>The definition of the input type is a bit strange, but because Combiner and Reducer implementations are optional, the input type heavily depends on the state of the data. As stated above, collators are non-typical use cases and the generics of the framework always help in finding the correct signature.</p>
<a name="preselecting-keys-with-keypredicate"></a><h4 id="preselecting-keys-with-keypredicate">Preselecting Keys with KeyPredicate</h4>
<p>You can use <code>KeyPredicate</code> to pre-select whether or not a key should be selected for mapping in the mapping phase. If the <code>KeyValueSource</code> implementation is able to know all keys prior to execution, the keys are filtered before the operations are divided among the different cluster members.</p>
<p>A <code>KeyPredicate</code> can also be used to select only a special range of data (e.g., a time frame), or in similar use cases.</p>
<p>A basic <code>KeyPredicate</code> implementation that only maps keys containing the word &quot;hazelcast&quot; might look like the following code example:</p>
<pre><code class="lang-java">public class WordCountKeyPredicate implements KeyPredicate&lt;String&gt; {

  @Override
  public boolean evaluate( String s ) {
    return s != null &amp;&amp; s.toLowerCase().contains( &quot;hazelcast&quot; );
  }
}
</code></pre>
<a name="job-monitoring-with-trackablejob"></a><h4 id="job-monitoring-with-trackablejob">Job Monitoring with TrackableJob</h4>
<p>You can retrieve a <code>TrackableJob</code> instance after submitting a job. It is requested from the <code>JobTracker</code> using the unique jobId (per <code>JobTracker</code>). You can use it get runtime statistics of the job. The information available is limited to the number of processed (mapped) records and the processing state of the different partitions or members (if <code>KeyValueSource</code> is not PartitionIdAware).</p>
<p>To retrieve the jobId after submission of the job, use <code>com.hazelcast.mapreduce.JobCompletableFuture</code> instead of the <code>com.hazelcast.core.ICompletableFuture</code> as the variable type for the returned future.</p>
<p>The example code below gives a quick introduction on how to retrieve the instance and the runtime data. For more information, please have a look at the Javadoc corresponding your running Hazelcast version.</p>
<p>The example performs the following steps to get the job instance.</p>
<ul>
<li>It gets the map with the hazelcastInstance <code>getMap</code> method.</li>
<li>From the map, it gets the source with the KeyValueSource <code>fromMap</code> method.</li>
<li>From the source, it gets a job with the JobTracker <code>newJob</code> method.</li>
</ul>
<pre><code class="lang-java">IMap&lt;String, String&gt; map = hazelcastInstance.getMap( &quot;articles&quot; );
KeyValueSource&lt;String, String&gt; source = KeyValueSource.fromMap( map );
Job&lt;String, String&gt; job = jobTracker.newJob( source );

JobCompletableFuture&lt;Map&lt;String, Long&gt;&gt; future = job
    .mapper( new TokenizerMapper() )
    .combiner( new WordCountCombinerFactory() )
    .reducer( new WordCountReducerFactory() )
    .submit();

String jobId = future.getJobId();
TrackableJob trackableJob = jobTracker.getTrackableJob(jobId);

JobProcessInformation stats = trackableJob.getJobProcessInformation();
int processedRecords = stats.getProcessedRecords();
log( &quot;ProcessedRecords: &quot; + processedRecords );

JobPartitionState[] partitionStates = stats.getPartitionStates();
for ( JobPartitionState partitionState : partitionStates ) {
  log( &quot;PartitionOwner: &quot; + partitionState.getOwner()
          + &quot;, Processing state: &quot; + partitionState.getState().name() );
}
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Caching of the JobProcessInformation does not work on Java native clients since current values are retrieved while retrieving the instance to minimize traffic between executing member and client.</em></p>
<a name="configuring-jobtracker"></a><h4 id="configuring-jobtracker">Configuring JobTracker</h4>
<p>You configure <code>JobTracker</code> configuration to set up behavior of the Hazelcast MapReduce framework.</p>
<p>Every <code>JobTracker</code> is capable of running multiple MapReduce jobs at once; one configuration is meant as a shared resource for all jobs created by the same <code>JobTracker</code>. The configuration gives full control over the expected load behavior and thread counts to be used.</p>
<p>The following snippet shows a typical <code>JobTracker</code> configuration. The configuration properties are discussed below the example.</p>
<pre><code class="lang-xml">&lt;jobtracker name=&quot;default&quot;&gt;
  &lt;max-thread-size&gt;0&lt;/max-thread-size&gt;
  &lt;!-- Queue size 0 means number of partitions * 2 --&gt;
  &lt;queue-size&gt;0&lt;/queue-size&gt;
  &lt;retry-count&gt;0&lt;/retry-count&gt;
  &lt;chunk-size&gt;1000&lt;/chunk-size&gt;
  &lt;communicate-stats&gt;true&lt;/communicate-stats&gt;
  &lt;topology-changed-strategy&gt;CANCEL_RUNNING_OPERATION&lt;/topology-changed-strategy&gt;
&lt;/jobtracker&gt;
</code></pre>
<ul>
<li><strong>max-thread-size:</strong> Maximum thread pool size of the JobTracker.</li>
<li><strong>queue-size:</strong> Maximum number of tasks that are able to wait to be processed. A value of 0 means an unbounded queue. Very low numbers can prevent successful execution since the job might not be correctly scheduled or intermediate chunks might be lost.</li>
<li><strong>retry-count:</strong> Currently not used. Reserved for later use where the framework will automatically try to restart/retry operations from an available save point.</li>
<li><strong>chunk-size:</strong> Number of emitted values before a chunk is sent to the reducers. If your emitted values are big or you want to better balance your work, you might want to change this to a lower or higher value. A value of 0 means immediate transmission, but remember that low values mean higher traffic costs. A very high value might cause an OutOfMemoryError to occur if the emitted values do not fit into heap memory before
being sent to the reducers. To prevent this, you might want to use a combiner to pre-reduce values on mapping members.</li>
<li><strong>communicate-stats:</strong> Specifies whether the statistics (for example, statistics about processed entries) are transmitted to the job emitter. This can show progress to a user inside of an UI system, but it produces additional traffic. If not needed, you might want to deactivate this.</li>
<li><strong>topology-changed-strategy:</strong> Specifies how the MapReduce framework reacts on topology changes while executing a job. Currently, only CANCEL_RUNNING_OPERATION is fully supported, which throws an exception to the job emitter (will throw a <code>com.hazelcast.mapreduce.TopologyChangedException</code>).</li>
</ul>

<a name="hazelcast-mapreduce-architecture"></a><h3 id="hazelcast-mapreduce-architecture">Hazelcast MapReduce Architecture</h3>
<p>This section explains some of the internals of the MapReduce framework. This is more advanced information. If you&#39;re not interested in how it works internally, you might want to skip this section.</p>
<a name="member-interoperation-example"></a><h4 id="member-interoperation-example">Member Interoperation Example</h4>
<p>To understand the following technical internals, we first have a short look at what happens in terms of an example workflow.</p>
<p>As a simple example, think of an <code>IMap&lt;String, Integer&gt;</code> and emitted keys having the same types. Imagine you have a cluster with three members, and you initiate the MapReduce job on the first member. After you requested the JobTracker from your running/connected Hazelcast, we submit the task and retrieve the ICompletableFuture, which gives us a chance to wait for the result to be calculated or to add a callback (and being more reactive).</p>
<p>The example expects that the chunk size is 0 or 1, so an emitted value is directly sent to the reducers. Internally, the job is prepared, started, and executed on all members as shown below. The first member acts as the job owner (job emitter).</p>
<pre><code class="lang-plain">Member1 starts MapReduce job
Member1 emits key=Foo, value=1
Member1 does PartitionService::getKeyOwner(Foo) =&gt; results in Member3

Member2 emits key=Foo, value=14
Member2 asks jobOwner (Member1) for keyOwner of Foo =&gt; results in Member3

Member1 sends chunk for key=Foo to Member3

Member3 receives chunk for key=Foo and looks if there is already a Reducer,
      if not creates one for key=Foo
Member3 processes chunk for key=Foo

Member2 sends chunk for key=Foo to Member3

Member3 receives chunk for key=Foo and looks if there is already a Reducer and uses
      the previous one
Member3 processes chunk for key=Foo

Member1 send LastChunk information to Member3 because processing local values finished

Member2 emits key=Foo, value=27
Member2 has cached keyOwner of Foo =&gt; results in Member3
Member2 sends chunk for key=Foo to Member3

Member3 receives chunk for key=Foo and looks if there is already a Reducer and uses
      the previous one
Member3 processes chunk for key=Foo

Member2 send LastChunk information to Member3 because processing local values finished

Member3 finishes reducing for key=Foo

Member1 registers its local partitions are processed
Member2 registers its local partitions are processed

Member1 sees all partitions processed and requests reducing from all members

Member1 merges all reduced results together in a final structure and returns it
</code></pre>
<p>The flow is quite complex but extremely powerful since everything is executed in parallel. Reducers do not wait until all values are emitted, but they immediately begin to reduce (when the first chunk for an emitted key arrives).</p>
<a name="internal-mapreduce-packages"></a><h4 id="internal-mapreduce-packages">Internal MapReduce Packages</h4>
<p>Beginning with the package level, there is one basic package: <code>com.hazelcast.mapreduce</code>. This includes the external API and the <strong>impl</strong> package, which itself contains the internal implementation.</p>
<ul>
<li>The <strong>impl</strong> package contains all the default <code>KeyValueSource</code> implementations and abstract base and support classes for the exposed API.</li>
<li>The <strong>client</strong> package contains all classes that are needed on the client and member sides when a client offers a MapReduce job.</li>
<li>The <strong>notification</strong> package contains all &quot;notification&quot; or event classes that notify other members about progress on operations.</li>
<li>The <strong>operation</strong> package contains all operations that are used by the workers or job owner to coordinate work and sync partition or reducer processing.</li>
<li>The <strong>task</strong> package contains all classes that execute the actual MapReduce operation. It features the supervisor, mapping phase implementation, and mapping and reducing tasks.</li>
</ul>
<a name="mapreduce-job-walk-through"></a><h4 id="mapreduce-job-walk-through">MapReduce Job Walk-Through</h4>
<p>Now to the technical walk-through: A MapReduce Job is always retrieved from a named <code>JobTracker</code>, which is implemented in <code>NodeJobTracker</code> (extends <code>AbstractJobTracker</code>) and is configured using the configuration DSL. All of the internal implementation is completely ICompletableFuture-driven and mostly non-blocking in design.</p>
<p>On submit, the Job creates a unique UUID which afterwards acts as a jobId and is combined with the JobTracker&#39;s name to be uniquely identifiable inside the cluster. Then, the preparation is sent around the cluster and every member prepares its execution by creating a JobSupervisor, MapCombineTask, and ReducerTask. The job-emitting JobSupervisor gains special capabilities to synchronize and control JobSupervisors on other members for the same job.</p>
<p>If preparation is finished on all members, the job itself is started by executing a StartProcessingJobOperation on every member. This initiates a MappingPhase implementation (defaults to KeyValueSourceMappingPhase) and starts the actual mapping on the members.</p>
<p>The mapping process is currently a single threaded operation per member, but will be extended to run in parallel on multiple partitions (configurable per Job) in future versions. The Mapper is now called on every available value on the partition and eventually emits values. For every emitted value, either a configured CombinerFactory is called to create a Combiner or a cached one is used (or the default CollectingCombinerFactory is used to create Combiners). When the chunk limit is reached on a member, a IntermediateChunkNotification is prepared by collecting emitted keys to their corresponding members. This is either done by asking the job owner to assign members or by an already cached assignment. In later versions, a PartitionStrategy might also be configurable.</p>
<p>The IntermediateChunkNotification is then sent to the reducers (containing only values for this member) and is offered to the ReducerTask. On every offer, the ReducerTask checks if it is already running and if not, it submits itself to the configured ExecutorService (from the JobTracker configuration).</p>
<p>If reducer queue runs out of work, the ReducerTask is removed from the ExecutorService to not block threads but eventually will be resubmitted on next chunk of work.</p>
<p>On every phase, the partition state is changed to keep track of the currently running operations. A JobPartitionState can be in one of the following states with self-explanatory titles: <code>[WAITING, MAPPING, REDUCING, PROCESSED, CANCELLED]</code>. If you have a deeper interest of these states, look at the Javadoc.</p>
<ul>
<li>Member asks for new partition to process: WAITING =&gt; MAPPING</li>
<li>Member emits first chunk to a reducer: MAPPING =&gt; REDUCING</li>
<li>All members signal that they finished mapping phase and reducing is finished, too: REDUCING =&gt; PROCESSED</li>
</ul>
<p>Eventually, all JobPartitionStates reach the state of PROCESSED. Then, the job emitter&#39;s JobSupervisor asks all members for their reduced results and executes a potentially offered Collator. With this Collator, the overall result is calculated before it removes itself from the JobTracker, doing some final cleanup and returning the result to the requester (using the internal TrackableJobFuture).</p>
<p>If a job is cancelled while execution, all partitions are immediately set to the CANCELLED state and a CancelJobSupervisorOperation is executed on all members to kill the running processes.</p>
<p>While the operation is running in addition to the default operations, some more operations like
ProcessStatsUpdateOperation (updates processed records statistics) or NotifyRemoteExceptionOperation (notifies the members that the sending member encountered an unrecoverable situation and the Job needs to
be cancelled - e.g. NullPointerException inside of a Mapper) are executed against the job owner to keep track of the process.</p>

<a name="aggregators"></a><h2 id="aggregators">Aggregators</h2>
<p>Based on the Hazelcast MapReduce framework, Aggregators are ready-to-use data aggregations. These are typical operations like
sum up values, finding minimum or maximum values, calculating averages, and other operations that you would expect 
in the relational database world.  </p>
<p>Aggregation operations are implemented, as mentioned above, on top of the MapReduce framework, and all operations can be
achieved using pure MapReduce calls. However, using the Aggregation feature is more convenient for a big set of standard operations.</p>
<a name="aggregations-basics"></a><h3 id="aggregations-basics">Aggregations Basics</h3>
<p>This section will quickly guide you through the basics of the Aggregations framework and some of its available classes.
We also will implement a first base example.</p>
<a name="aggregations-and-map-interfaces"></a><h4 id="aggregations-and-map-interfaces">Aggregations and Map Interfaces</h4>
<p>Aggregations are available on both types of map interfaces, <code>com.hazelcast.core.IMap</code> and <code>com.hazelcast
.core.MultiMap</code>, using
the <code>aggregate</code> methods. Two overloaded methods are available that customize resource management of the
underlying MapReduce framework by supplying a custom configured 
<code>com.hazelcast.mapreduce.JobTracker</code> instance. To find out how to
configure the MapReduce framework, please see <a href="#configuring-jobtracker">Configuring JobTracker</a>. We will
later see another way to configure the automatically used MapReduce framework if no special <code>JobTracker</code> is supplied.</p>
<a name="aggregations-and-java"></a><h4 id="aggregations-and-java">Aggregations and Java</h4>
<p>To make Aggregations more convenient to use and future proof, the API is heavily optimized for Java 8 and future versions.
The API is still fully compatible with any Java version Hazelcast supports (Java 6 and Java 7). The biggest difference is how you
work with the Java generics: on Java 6 and 7, the process to resolve generics is not as strong as on Java 8 and
future Java versions. In addition, the whole Aggregations API has full Java 8 Project Lambda (or Closure, 
<a href="https://jcp.org/en/jsr/detail?id=335" target="_blank">JSR 335</a>) support.</p>
<p>For illustration of the differences in Java 6 and 7 in comparison to Java 8, we will have a quick look at code
examples for both. After that, we will focus on using Java 8 syntax to keep examples short and easy to understand, and we will see some hints about what the code looks like in Java 6 or 7.</p>
<p>The first example will produce the sum of some <code>int</code> values stored in a Hazelcast IMap. This example does not use much of the functionality of the Aggregations framework, but it will show the main difference.</p>
<pre><code class="lang-java">IMap&lt;String, Integer&gt; personAgeMapping = hazelcastInstance.getMap( &quot;person-age&quot; );
for ( int i = 0; i &lt; 1000; i++ ) {
  String lastName = RandomUtil.randomLastName();
  int age = RandomUtil.randomAgeBetween( 20, 80 );
  personAgeMapping.put( lastName, Integer.valueOf( age ) );
}
</code></pre>
<p>With our demo data prepared, we can see how to produce the sums in different Java versions.</p>
<a name="aggregations-and-java-6-or-java-7"></a><h4 id="aggregations-and-java-6-or-java-7">Aggregations and Java 6 or Java 7</h4>
<p>Since Java 6 and 7 are not as strong on resolving generics as Java 8, you need to be a bit more verbose
with the code you write. You might also consider using raw types but breaking the type safety to ease this process.</p>
<p>For a short introduction on what the following code example means, look at the source code comments. We will later dig deeper into
the different options. </p>
<pre><code class="lang-java">// No filter applied, select all entries
Supplier&lt;String, Integer, Integer&gt; supplier = Supplier.all();
// Choose the sum aggregation
Aggregation&lt;String, Integer, Integer&gt; aggregation = Aggregations.integerSum();
// Execute the aggregation
int sum = personAgeMapping.aggregate( supplier, aggregation );
</code></pre>
<a name="aggregations-and-java-8"></a><h4 id="aggregations-and-java-8">Aggregations and Java 8</h4>
<p>With Java 8, the Aggregations API looks simpler because Java 8 can resolve the generic parameters for us. That means
the above lines of Java 6/7 example code will end up in just one easy line on Java 8.</p>
<pre><code>int sum = personAgeMapping.aggregate( Supplier.all(), Aggregations.integerSum() );
</code></pre><a name="aggregations-and-the-mapreduce-framework"></a><h4 id="aggregations-and-the-mapreduce-framework">Aggregations and the MapReduce Framework</h4>
<p>As mentioned before, the Aggregations implementation is based on the Hazelcast MapReduce framework and therefore you might find
overlaps in their APIs. One overload of the <code>aggregate</code> method can be supplied with
a <code>JobTracker</code>, which is part of the MapReduce framework.</p>
<p>If you implement your own aggregations, you will use a mixture of the Aggregations and
the MapReduce API. If you do so, e.g., to make the life of colleagues easier,
please read the <a href="#implementing-aggregations">Implementing Aggregations section</a>.</p>
<p>For the full MapReduce documentation please see the <a href="#mapreduce">MapReduce section</a>.</p>

<a name="using-the-aggregations-api"></a><h3 id="using-the-aggregations-api">Using the Aggregations API</h3>
<p>We now look into what can be achieved using the
Aggregations API. To work on some deeper examples, let&#39;s quickly have a look at the available classes and interfaces and
discuss their usage.</p>
<a name="supplier"></a><h4 id="supplier">Supplier</h4>
<p>The <code>com.hazelcast.mapreduce.aggregation.Supplier</code> provides filtering and data extraction to the aggregation operation.
This class already provides a few different static methods to achieve the most common cases. <code>Supplier.all()</code>
accepts all incoming values and does not apply any data extraction or transformation upon them before supplying them to
the aggregation function itself.</p>
<p>For filtering data sets, you have two different options by default:</p>
<ul>
<li>You can either supply a <code>com.hazelcast.query.Predicate</code> if you want to filter on values and/or keys, or</li>
<li>You can supply a <code>com.hazelcast.mapreduce.KeyPredicate</code> if you can decide directly on the data
key without the need to deserialize the value.</li>
</ul>
<p>As mentioned above, all APIs are fully Java 8 and Lambda compatible. Let&#39;s have a look on how we can do basic filtering using
those two options.</p>
<a name="basic-filtering-with-keypredicate"></a><h5 id="basic-filtering-with-keypredicate">Basic Filtering with KeyPredicate</h5>
<p>First, we have a look at a <code>KeyPredicate</code> and we only accept people whose last name is &quot;Jones&quot;.</p>
<pre><code class="lang-java">Supplier&lt;...&gt; supplier = Supplier.fromKeyPredicate(
    lastName -&gt; &quot;Jones&quot;.equalsIgnoreCase( lastName )
);
</code></pre>
<pre><code class="lang-java">class JonesKeyPredicate implements KeyPredicate&lt;String&gt; {
  public boolean evaluate( String key ) {
    return &quot;Jones&quot;.equalsIgnoreCase( key );
  }
}
</code></pre>
<a name="filtering-on-values-with-predicate"></a><h5 id="filtering-on-values-with-predicate">Filtering on Values with Predicate</h5>
<p>Using the standard Hazelcast <code>Predicate</code> interface, we can also filter based on the value of a data entry. In the following example, you can
only select values that are divisible by 4 without a remainder. </p>
<pre><code class="lang-java">Supplier&lt;...&gt; supplier = Supplier.fromPredicate(
    entry -&gt; entry.getValue() % 4 == 0
);
</code></pre>
<pre><code class="lang-java">class DivisiblePredicate implements Predicate&lt;String, Integer&gt; {
  public boolean apply( Map.Entry&lt;String, Integer&gt; entry ) {
    return entry.getValue() % 4 == 0;
  }
}
</code></pre>
<a name="extracting-and-transforming-data"></a><h5 id="extracting-and-transforming-data">Extracting and Transforming Data</h5>
<p>As well as filtering, <code>Supplier</code> can also extract or transform data before providing it
to the aggregation operation itself. The following example shows how to transform an input value to a string.</p>
<pre><code class="lang-java">Supplier&lt;String, Integer, String&gt; supplier = Supplier.all(
    value -&gt; Integer.toString(value)
);
</code></pre>
<p>You can see a Java 6/7 example in the <a href="#aggregations-examples">Aggregations Examples section</a>.</p>
<p>Apart from the fact we transformed the input value of type <code>int</code> (or Integer) to a string, we can see that the generic information
of the resulting <code>Supplier</code> has changed as well. This indicates that we now have an aggregation working on string values.</p>
<a name="chaining-multiple-filtering-rules"></a><h5 id="chaining-multiple-filtering-rules">Chaining Multiple Filtering Rules</h5>
<p>Another feature of <code>Supplier</code> is its ability to chain multiple filtering rules. Let&#39;s combine all of the
above examples into one rule set:</p>
<pre><code class="lang-java">Supplier&lt;String, Integer, String&gt; supplier =
    Supplier.fromKeyPredicate(
        lastName -&gt; &quot;Jones&quot;.equalsIgnoreCase( lastName ),
        Supplier.fromPredicate(
            entry -&gt; entry.getValue() % 4 == 0,  
            Supplier.all( value -&gt; Integer.toString(value) )
        )
    );
</code></pre>
<a name="implementing-supplier-with-special-requirements"></a><h5 id="implementing-supplier-with-special-requirements">Implementing Supplier with Special Requirements</h5>
<p>You might prefer or need to implement your <code>Supplier</code> based on special
requirements. This is a very basic task. The <code>Supplier</code> abstract class has just one method: the <code>apply</code> method.
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Due to a limitation of the Java Lambda API, you cannot implement abstract classes using Lambdas. Instead it is
recommended that you create a standard named class.</em></p>
<pre><code class="lang-java">class MyCustomSupplier extends Supplier&lt;String, Integer, String&gt; {
  public String apply( Map.Entry&lt;String, Integer&gt; entry ) {
    Integer value = entry.getValue();
    if (value == null) {
      return null;
    }
    return value % 4 == 0 ? String.valueOf( value ) : null;
  }
}
</code></pre>
<p>The <code>Supplier</code> <code>apply</code> methods are expected to return null whenever the input value should not be mapped to the aggregation
process. This can be used, as in the example above, to implement filter rules directly. Implementing filters using the
<code>KeyPredicate</code> and <code>Predicate</code> interfaces might be more convenient.</p>
<p>To use your own <code>Supplier</code>, just pass it to the aggregate method or use it in combination with other <code>Supplier</code>s.</p>
<pre><code class="lang-java">int sum = personAgeMapping.aggregate( new MyCustomSupplier(), Aggregations.count() );
</code></pre>
<pre><code class="lang-java">Supplier&lt;String, Integer, String&gt; supplier =
    Supplier.fromKeyPredicate(
        lastName -&gt; &quot;Jones&quot;.equalsIgnoreCase( lastName ),
        new MyCustomSupplier()
     );
int sum = personAgeMapping.aggregate( supplier, Aggregations.count() );
</code></pre>
<a name="defining-the-aggregation-operation"></a><h4 id="defining-the-aggregation-operation">Defining the Aggregation Operation</h4>
<p>The <code>com.hazelcast.mapreduce.aggregation.Aggregation</code> interface defines the aggregation operation itself. It contains a set of
MapReduce API implementations like <code>Mapper</code>, <code>Combiner</code>, <code>Reducer</code>, and <code>Collator</code>. These implementations are normally unique to
the chosen <code>Aggregation</code>. This interface can also be implemented with your aggregation operations based on MapReduce calls. For
more information, refer to <a href="#implementing-aggregations">Implementing Aggregations section</a>.</p>
<p>The <code>com.hazelcast.mapreduce.aggregation.Aggregations</code> class provides a common predefined set of aggregations. This class
contains type safe aggregations of the following types:</p>
<ul>
<li>Average (Integer, Long, Double, BigInteger, BigDecimal)</li>
<li>Sum (Integer, Long, Double, BigInteger, BigDecimal)</li>
<li>Min (Integer, Long, Double, BigInteger, BigDecimal, Comparable)</li>
<li>Max (Integer, Long, Double, BigInteger, BigDecimal, Comparable)</li>
<li>DistinctValues</li>
<li>Count</li>
</ul>
<p>Those aggregations are similar to their counterparts on relational databases and can be equated to SQL statements as set out
below.</p>
<a name="average"></a><h5 id="average">Average</h5>
<p>Calculates an average value based on all selected values.</p>
<pre><code class="lang-java">map.aggregate( Supplier.all( person -&gt; person.getAge() ),
               Aggregations.integerAvg() );
</code></pre>
<pre><code class="lang-sql">SELECT AVG(person.age) FROM person;
</code></pre>
<a name="sum"></a><h5 id="sum">Sum</h5>
<p>Calculates a sum based on all selected values.</p>
<pre><code class="lang-java">map.aggregate( Supplier.all( person -&gt; person.getAge() ),
               Aggregations.integerSum() );
</code></pre>
<pre><code class="lang-sql">SELECT SUM(person.age) FROM person;
</code></pre>
<a name="minimum-min"></a><h5 id="minimum-min-">Minimum (Min)</h5>
<p>Finds the minimal value over all selected values.</p>
<pre><code class="lang-java">map.aggregate( Supplier.all( person -&gt; person.getAge() ),
               Aggregations.integerMin() );
</code></pre>
<pre><code class="lang-sql">SELECT MIN(person.age) FROM person;
</code></pre>
<a name="maximum-max"></a><h5 id="maximum-max-">Maximum (Max)</h5>
<p>Finds the maximal value over all selected values.</p>
<pre><code class="lang-java">map.aggregate( Supplier.all( person -&gt; person.getAge() ),
               Aggregations.integerMax() );
</code></pre>
<pre><code class="lang-sql">SELECT MAX(person.age) FROM person;
</code></pre>
<a name="distinct-values"></a><h5 id="distinct-values">Distinct Values</h5>
<p>Returns a collection of distinct values over the selected values</p>
<pre><code class="lang-java">map.aggregate( Supplier.all( person -&gt; person.getAge() ),
               Aggregations.distinctValues() );
</code></pre>
<pre><code class="lang-sql">SELECT DISTINCT person.age FROM person;
</code></pre>
<a name="count"></a><h5 id="count">Count</h5>
<p>Returns the element count over all selected values </p>
<pre><code class="lang-java">map.aggregate( Supplier.all(), Aggregations.count() );
</code></pre>
<pre><code class="lang-sql">SELECT COUNT(*) FROM person;
</code></pre>
<a name="extracting-attribute-values-with-propertyextractor"></a><h4 id="extracting-attribute-values-with-propertyextractor">Extracting Attribute Values with PropertyExtractor</h4>
<p>We used the <code>com.hazelcast.mapreduce.aggregation.PropertyExtractor</code> interface before when we had a look at the example
on how to use a <code>Supplier</code> to <a href="#extracting-and-transforming-data">transform a value to another type</a>. It can also be used to extract attributes from values.</p>
<pre><code class="lang-java">class Person {
  private String firstName;
  private String lastName;
  private int age;

  // getters and setters
}

PropertyExtractor&lt;Person, Integer&gt; propertyExtractor = (person) -&gt; person.getAge();
</code></pre>
<pre><code class="lang-java">class AgeExtractor implements PropertyExtractor&lt;Person, Integer&gt; {
  public Integer extract( Person value ) {
    return value.getAge();
  }
}
</code></pre>
<p>In this example, we extract the value from the person&#39;s age attribute. The value type changes from Person to <code>Integer</code> which is reflected in the generics information to stay type safe.</p>
<p>You can use <code>PropertyExtractor</code>s for any kind of transformation of data. You might even want to have multiple
transformation steps chained one after another.</p>
<a name="configuring-aggregations"></a><h4 id="configuring-aggregations">Configuring Aggregations</h4>
<p>As stated before, the easiest way to configure the resources used by the underlying MapReduce framework is to supply a <code>JobTracker</code>
to the aggregation call itself by passing it to either <code>IMap::aggregate</code> or <code>MultiMap::aggregate</code>.</p>
<p>There is another way to implicitly configure the underlying used <code>JobTracker</code>. If no specific <code>JobTracker</code> was
passed for the aggregation call, internally one will be created using the following naming specifications:</p>
<p>For <code>IMap</code> aggregation calls the naming specification is created as:</p>
<ul>
<li><code>hz::aggregation-map-</code> and the concatenated name of the map.</li>
</ul>
<p>For <code>MultiMap</code> it is very similar:</p>
<ul>
<li><code>hz::aggregation-multimap-</code> and the concatenated name of the MultiMap.</li>
</ul>
<p>Knowing the specification of the name, we can configure the <code>JobTracker</code> as expected 
(as described in <a href="#retrieving-a-jobtracker-instance">Retrieving a JobTracker Instance</a>) using the naming spec we just learned. For more information on configuration of the 
<code>JobTracker</code>, please see <a href="#configuring-jobtracker">Configuring Jobtracker</a>. </p>
<p>To finish this section, let&#39;s have a quick example for the above naming specs:</p>
<pre><code class="lang-java">IMap&lt;String, Integer&gt; map = hazelcastInstance.getMap( &quot;mymap&quot; );

// The internal JobTracker name resolves to &#39;hz::aggregation-map-mymap&#39; 
map.aggregate( ... );
</code></pre>
<pre><code class="lang-java">MultiMap&lt;String, Integer&gt; multimap = hazelcastInstance.getMultiMap( &quot;mymultimap&quot; );

// The internal JobTracker name resolves to &#39;hz::aggregation-multimap-mymultimap&#39; 
multimap.aggregate( ... );
</code></pre>

<a name="aggregations-examples"></a><h3 id="aggregations-examples">Aggregations Examples</h3>
<p>For the final example, imagine you are working for an international company and you have an employee database stored in Hazelcast
<code>IMap</code> with all employees worldwide and a <code>MultiMap</code> for assigning employees to their certain locations or offices. In addition,
there is another <code>IMap</code> that holds the salary per employee.</p>
<a name="setting-up-the-data-model"></a><h4 id="setting-up-the-data-model">Setting up the Data Model</h4>
<p>Let&#39;s have a look at our data model.</p>
<pre><code class="lang-java">class Employee implements Serializable {
  private String firstName;
  private String lastName;
  private String companyName;
  private String address;
  private String city;
  private String county;
  private String state;
  private int zip;
  private String phone1;
  private String phone2;
  private String email;
  private String web;

  // getters and setters
}

class SalaryMonth implements Serializable {
  private Month month;
  private int salary;

  // getters and setters
}

class SalaryYear implements Serializable {
  private String email;
  private int year;
  private List&lt;SalaryMonth&gt; months;

  // getters and setters

  public int getAnnualSalary() {
    int sum = 0;
    for ( SalaryMonth salaryMonth : getMonths() ) {
      sum += salaryMonth.getSalary();
    }
    return sum;
  }
}
</code></pre>
<p>The two <code>IMap</code>s and the <code>MultiMap</code> are keyed by the string of email. They are defined as follows:</p>
<pre><code class="lang-java">IMap&lt;String, Employee&gt; employees = hz.getMap( &quot;employees&quot; );
IMap&lt;String, SalaryYear&gt; salaries = hz.getMap( &quot;salaries&quot; );
MultiMap&lt;String, String&gt; officeAssignment = hz.getMultiMap( &quot;office-employee&quot; );
</code></pre>
<p>So far, we know all the important information to work out some example aggregations. We will look into some deeper implementation
details and how we can work around some current limitations that will be eliminated in future versions of the API.</p>
<a name="average-aggregation-example"></a><h4 id="average-aggregation-example">Average Aggregation Example</h4>
<p>Let&#39;s start with a very basic example. We want to know the average salary of all of our employees. To do this,
we need a <code>PropertyExtractor</code> and the average aggregation for type <code>Integer</code>.</p>
<pre><code class="lang-java">IMap&lt;String, SalaryYear&gt; salaries = hazelcastInstance.getMap( &quot;salaries&quot; );
PropertyExtractor&lt;SalaryYear, Integer&gt; extractor =
    (salaryYear) -&gt; salaryYear.getAnnualSalary();
int avgSalary = salaries.aggregate( Supplier.all( extractor ),
                                    Aggregations.integerAvg() );
</code></pre>
<p>That&#39;s it. Internally, we created a MapReduce task based on the predefined aggregation and fired it up immediately. Currently all
aggregation calls are blocking operations, so it is not yet possible to execute the aggregation in a reactive way (using
<code>com.hazelcast.core.ICompletableFuture</code>), but this will be part of an upcoming version.</p>
<a name="map-join-example"></a><h4 id="map-join-example">Map Join Example</h4>
<p>The following example is a little more complex. We only want to have our US-based employees selected into the average
salary calculation, so we need to execute a join operation between the employees and salaries maps.</p>
<pre><code class="lang-java">class USEmployeeFilter implements KeyPredicate&lt;String&gt;, HazelcastInstanceAware {
  private transient HazelcastInstance hazelcastInstance;

  public void setHazelcastInstance( HazelcastInstance hazelcastInstance ) {
    this.hazelcastInstance = hazelcastInstance;
  }

  public boolean evaluate( String email ) {
    IMap&lt;String, Employee&gt; employees = hazelcastInstance.getMap( &quot;employees&quot; );
    Employee employee = employees.get( email );
    return &quot;US&quot;.equals( employee.getCountry() );
  }
}
</code></pre>
<p>Using the <code>HazelcastInstanceAware</code> interface, we get the current instance of Hazelcast injected into our filter and we can perform data
joins on other data structures of the cluster. We now only select employees that work as part of our US offices into the
aggregation.</p>
<pre><code class="lang-java">IMap&lt;String, SalaryYear&gt; salaries = hazelcastInstance.getMap( &quot;salaries&quot; );
PropertyExtractor&lt;SalaryYear, Integer&gt; extractor =
    (salaryYear) -&gt; salaryYear.getAnnualSalary();
int avgSalary = salaries.aggregate( Supplier.fromKeyPredicate(
                                        new USEmployeeFilter(), extractor
                                    ), Aggregations.integerAvg() );
</code></pre>
<a name="grouping-example"></a><h4 id="grouping-example">Grouping Example</h4>
<p>For our next example, we will do some grouping based on the different worldwide offices. Currently, a group aggregator is not yet 
available, so we need a small workaround to achieve this goal. (In later versions of the Aggregations API this will not be 
required because it will be available out of the box in a much more convenient way.)</p>
<p>Again, let&#39;s start with our filter. This time, we want to filter based on an office name and we need to do some data joins
to achieve this kind of filtering. </p>
<p><strong>A short tip:</strong> to minimize the data transmission on the aggregation we can use
<a href="#data-affinity">Data Affinity</a> rules to influence the partitioning of data. Be aware that this is an expert feature of Hazelcast.</p>
<pre><code class="lang-java">class OfficeEmployeeFilter implements KeyPredicate&lt;String&gt;, HazelcastInstanceAware {
  private transient HazelcastInstance hazelcastInstance;
  private String office;

  // Deserialization Constructor
  public OfficeEmployeeFilter() {
  } 

  public OfficeEmployeeFilter( String office ) {
    this.office = office;
  }

  public void setHazelcastInstance( HazelcastInstance hazelcastInstance ) {
    this.hazelcastInstance = hazelcastInstance;
  }

  public boolean evaluate( String email ) {
    MultiMap&lt;String, String&gt; officeAssignment = hazelcastInstance
        .getMultiMap( &quot;office-employee&quot; );

    return officeAssignment.containsEntry( office, email );    
  }
}
</code></pre>
<p>Now we can execute our aggregations. As mentioned before, we currently need to do the grouping on our own by executing multiple
aggregations in a row.</p>
<pre><code class="lang-java">Map&lt;String, Integer&gt; avgSalariesPerOffice = new HashMap&lt;String, Integer&gt;();

IMap&lt;String, SalaryYear&gt; salaries = hazelcastInstance.getMap( &quot;salaries&quot; );
MultiMap&lt;String, String&gt; officeAssignment =
    hazelcastInstance.getMultiMap( &quot;office-employee&quot; );

PropertyExtractor&lt;SalaryYear, Integer&gt; extractor =
    (salaryYear) -&gt; salaryYear.getAnnualSalary();

for ( String office : officeAssignment.keySet() ) {
  OfficeEmployeeFilter filter = new OfficeEmployeeFilter( office );
  int avgSalary = salaries.aggregate( Supplier.fromKeyPredicate( filter, extractor ),
                                      Aggregations.integerAvg() );

  avgSalariesPerOffice.put( office, avgSalary );
}
</code></pre>
<a name="simple-count-example"></a><h4 id="simple-count-example">Simple Count Example</h4>
<p>We want to end this section by executing one final and easy aggregation. We
want to know how many employees we currently have on a worldwide basis. Before reading the next lines of example code, you
can try to do it on your own to see if you understood how to execute aggregations.</p>
<pre><code class="lang-java">IMap&lt;String, Employee&gt; employees = hazelcastInstance.getMap( &quot;employees&quot; );
int count = employees.size();
</code></pre>
<p>Ok, after the quick joke of the previous two code lines, we look at the real two code lines:</p>
<pre><code class="lang-java">IMap&lt;String, Employee&gt; employees = hazelcastInstance.getMap( &quot;employees&quot; );
int count = employees.aggregate( Supplier.all(), Aggregations.count() );
</code></pre>
<p>We now have an overview of how to use aggregations in real life situations. If you want to do your colleagues a favor, you
might want to write your own additional set of aggregations. If so, then read the next section, <a href="#implementing-aggregations">Implementing Aggregations</a>.</p>

<a name="implementing-aggregations"></a><h3 id="implementing-aggregations">Implementing Aggregations</h3>
<p>This section explains how to implement your own aggregations in your own application. It
is an advanced section, so if you do not intend to implement your own aggregation, you might want to
stop reading here and come back later when you need to know how to implement your own
aggregation.</p>
<p>An <code>Aggregation</code> implementation is defining a MapReduce task, but with a small difference: the <code>Mapper</code>
is always expected to work on a <code>Supplier</code> that filters and/or transforms the mapped input value to some output value.</p>
<a name="aggregation-methods"></a><h4 id="aggregation-methods">Aggregation Methods</h4>
<p>The main interface for making your own aggregation is <code>com.hazelcast.mapreduce.aggregation.Aggregation</code>. It consists of four
methods.</p>
<pre><code class="lang-java">interface Aggregation&lt;Key, Supplied, Result&gt; {
  Mapper getMapper(Supplier&lt;Key, ?, Supplied&gt; supplier);
  CombinerFactory getCombinerFactory();
  ReducerFactory getReducerFactory();
  Collator&lt;Map.Entry, Result&gt; getCollator();
}
</code></pre>
<p>The <code>getMapper</code> and <code>getReducerFactory</code> methods should return non-null values. <code>getCombinerFactory</code> and <code>getCollator</code> are
optional operations and you do not need to implement them. You can decide to implement them depending on the use case you want
to achieve.</p>
<a name="aggregation-tips"></a><h4 id="aggregation-tips">Aggregation Tips</h4>
<p>For more information on how you implement mappers, combiners, reducers, and collators, refer to the
<a href="#mapreduce">MapReduce section</a>.</p>
<p>For best speed and traffic usage, as mentioned in the <a href="#mapreduce">MapReduce section</a>, you should add a <code>Combiner</code> to your aggregation
whenever it is possible to do some kind of pre-reduction step.</p>
<p>Your implementation also should use <code>DataSerializable</code> or <code>IdentifiedDataSerializable</code> for best compatibility and speed/stream-size
reasons.</p>
<p><br></br></p>

<a name="continuous-query-cache"></a><h2 id="continuous-query-cache">Continuous Query Cache</h2>
<font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>

<p><br></br></p>
<p>A continuous query cache is used to cache the result of a continuous query. After the construction of a continuous query cache, all changes on the underlying <code>IMap</code> are immediately reflected to this cache as a stream of events. Therefore, this cache will be an always up-to-date view of the <code>IMap</code>. You can create a continuous query cache either on the client or member.</p>
<a name="keeping-query-results-local-and-ready"></a><h3 id="keeping-query-results-local-and-ready">Keeping Query Results Local and Ready</h3>
<p>A continuous query cache is beneficial when you need to query the distributed <code>IMap</code> data in a very frequent and fast way. By using a continuous query cache, the result of the query will always be ready and local to the application.</p>
<a name="accessing-continuous-query-cache-from-member"></a><h3 id="accessing-continuous-query-cache-from-member">Accessing Continuous Query Cache from Member</h3>
<p>The following code snippet shows how you can access a continuous query cache from a member.</p>
<pre><code class="lang-java">
QueryCacheConfig queryCacheConfig = new QueryCacheConfig(&quot;cache-name&quot;);
queryCacheConfig.getPredicateConfig().setImplementation(new OddKeysPredicate());

MapConfig mapConfig = new MapConfig(&quot;map-name&quot;);
mapConfig.addQueryCacheConfig(queryCacheConfig);

Config config = new Config();
config.addMapConfig(mapConfig);

HazelcastInstance node = Hazelcast.newHazelcastInstance(config);
IEnterpriseMap&lt;Integer, String&gt; map = (IEnterpriseMap) node.getMap(&quot;map-name&quot;);

QueryCache&lt;Integer, String&gt; cache = map.getQueryCache(&quot;cache-name&quot;);
</code></pre>
<a name="accessing-continuous-query-cache-from-client-side"></a><h3 id="accessing-continuous-query-cache-from-client-side">Accessing Continuous Query Cache from Client Side</h3>
<p>The following code snippet shows how you can access a continuous query cache from the client side.
The difference in this code from the member side code above is that you configure and instantiate
a client instance instead of a member instance.</p>
<pre><code class="lang-java">
QueryCacheConfig queryCacheConfig = new QueryCacheConfig(&quot;cache-name&quot;);
queryCacheConfig.getPredicateConfig().setImplementation(new OddKeysPredicate());

ClientConfig clientConfig = new ClientConfig();
clientConfig.addQueryCacheConfig(&quot;map-name&quot;, queryCacheConfig);

HazelcastInstance client = HazelcastClient.newHazelcastClient(clientConfig);
IEnterpriseMap&lt;Integer, Integer&gt; clientMap = (IEnterpriseMap) client.getMap(&quot;map-name&quot;);

QueryCache&lt;Integer, Integer&gt; cache = clientMap.getQueryCache(&quot;cache-name&quot;);
</code></pre>
<a name="features-of-continuous-query-cache"></a><h3 id="features-of-continuous-query-cache">Features of Continuous Query Cache</h3>
<p>The following features of continuous query cache are valid for both the member and client.</p>
<ul>
<li>The initial query that is run on the existing <code>IMap</code> data during the continuous query cache construction can be enabled/disabled according to the supplied predicate via <code>QueryCacheConfig#setPopulate</code>.</li>
<li>Continuous query cache allows you to run queries with indexes, and perform event batching and coalescing.</li>
<li>A continuous query cache is evictable. Note that a continuous query cache has a default maximum capacity of 10000. If you need a non-evictable cache, you should configure the eviction via <code>QueryCacheConfig#setEvictionConfig</code>.</li>
<li>A listener can be added to a continuous query cache using <code>QueryCache#addEntryListener</code>.</li>
<li><p><code>IMap</code> events are reflected in continuous query cache in the same order as they were generated on map entries. Since events are created on entries stored in partitions, ordering of events is maintained based on the ordering within the partition. You can add listeners to capture lost events using <code>EventLostListener</code> and you can recover lost events with the method <code>QueryCache#tryRecover</code>.
Recovery of lost events largely depends on the size of the buffer on Hazelcast members. Default buffer size is 16 per partition; i.e. 16 events per partition can be maintained in the buffer. If the event generation is high, setting the buffer size to a higher number will provide better chances of recovering lost events. You can set buffer size with <code>QueryCacheConfig#setBufferSize</code>.
You can use the following example code for a recovery case.</p>
<pre><code class="lang-java">
     QueryCache queryCache = map.getQueryCache(&quot;cache-name&quot;, new SqlPredicate(&quot;this &gt; 20&quot;), true);
     queryCache.addEntryListener(new EventLostListener() {
         @Override
         public void eventLost(EventLostEvent event) {
             queryCache.tryRecover();
         }
     }, false);
</code></pre>
</li>
<li><p>You can configure continuous query cache declaratively or programmatically.</p>
</li>
<li>You can populate a continuous query cache with only the keys of its entries and retrieve the subsequent values directly via <code>QueryCache#get</code> from the underlying <code>IMap</code>. This helps to decrease the initial population time when the values are very large.
<br></br></li>
</ul>

<a name="transactions"></a><h1 id="transactions">Transactions</h1>
<p>This chapter explains the usage of Hazelcast in a transactional context. It describes the Hazelcast transaction types and how they work, how to provide XA (e<strong>X</strong>tended <strong>A</strong>rchiteture) transactions, and how to integrate Hazelcast with J2EE containers.</p>
<a name="creating-a-transaction-interface"></a><h2 id="creating-a-transaction-interface">Creating a Transaction Interface</h2>
<p>You create a <code>TransactionContext</code> object to begin, commit, and rollback a transaction. You can obtain transaction-aware instances of queues, maps, sets, lists, multimaps via <code>TransactionContext</code>, work with them, and commit/rollback in one shot. You can see the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/transaction/TransactionContext.java">TransactionContext source code here</a>.</p>
<p>Hazelcast supports two types of transactions: ONE_PHASE and TWO_PHASE. The type of transaction controls what happens when a member crashes while a transaction is committing. The default behavior is TWO_PHASE.
<br><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Starting with Hazelcast 3.6, the transaction type <code>LOCAL</code> has been deprecated. Please use <code>ONE_PHASE</code> for the Hazelcast releases 3.6 and higher.</em>
<br><br></p>
<ul>
<li><p><strong>ONE_PHASE</strong>: By selecting this transaction type, you execute the transactions with a single phase that is committing the changes. Since a preparing phase does not exist, the conflicts are not detected. When a conflict happens while committing the changes (e.g., due to a member crash), not all the changes are written and this leaves the system in an inconsistent state.</p>
</li>
<li><p><strong>TWO_PHASE</strong>: When you select this transaction type, Hazelcast first tries to execute the prepare phase. This phase fails if there are any conflicts. Once the prepare phase is successful, Hazelcast executes the commit phase (writing the changes). Before TWO_PHASE commits, Hazelcast copies the commit log to other members, so in case of a member failure, another member can complete the commit.</p>
</li>
</ul>
<pre><code class="lang-java">import java.util.Queue;
import java.util.Map;
import java.util.Set;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.Transaction;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

TransactionOptions options = new TransactionOptions()
    .setTransactionType( TransactionType.ONE_PHASE );

TransactionContext context = hazelcastInstance.newTransactionContext( options );
context.beginTransaction();

TransactionalQueue queue = context.getQueue( &quot;myqueue&quot; );
TransactionalMap map = context.getMap( &quot;mymap&quot; );
TransactionalSet set = context.getSet( &quot;myset&quot; );

try {
  Object obj = queue.poll();
  //process obj
  map.put( &quot;1&quot;, &quot;value1&quot; );
  set.add( &quot;value&quot; );
  //do other things..
  context.commitTransaction();
} catch ( Throwable t ) {
  context.rollbackTransaction();
}
</code></pre>
<p>In a transaction, operations will not be executed immediately. Their changes will be local to the <code>TransactionContext</code> until committed. However, they will ensure the changes via locks. </p>
<p>For the above example, when <code>map.put</code> is executed, no data will be put in the map but the key will be locked against changes. While committing, operations will be executed, the value will be put to the map, and the key will be unlocked.</p>
<p>The isolation level in Hazelcast Transactions is <code>READ_COMMITTED</code>. If you are in a transaction, you can read the data in your transaction and the data that is already committed. If you are not in a transaction, you can only read the committed data.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The REPEATABLE_READ isolation level can also be exercised using the method <code>getForUpdate()</code> of <code>TransactionalMap</code>.</em></p>
<a name="queuesetlist-vs-mapmultimap"></a><h3 id="queue-set-list-vs-map-multimap">Queue/Set/List vs. Map/Multimap</h3>
<p>Hazelcast implements queue/set/list operations differently than map/multimap operations. For queue operations (offer, poll), offered and/or polled objects are copied to the owner member in order to safely commit/rollback. For map/multimap, Hazelcast first acquires the locks for the write operations (put, remove) and holds the differences (what is added/removed/updated) locally for each transaction. When the transaction is set to commit, Hazelcast will release the locks and apply the differences. When rolling back, Hazelcast will release the locks and discard the differences.</p>
<p><code>MapStore</code> and <code>QueueStore</code> do not participate in transactions. Hazelcast will suppress exceptions thrown by the store in a transaction. Please refer to the <a href="#providing-xa-transactions">XA Transactions section</a> for further information.</p>
<a name="one-phase-vs-two-phase"></a><h3 id="one_phase-vs-two_phase">ONE_PHASE vs. TWO_PHASE</h3>
<p>As discussed in <a href="#creating-a-transaction-interface">Creating a Transaction Interface</a>, when you choose ONE_PHASE as the transaction type, Hazelcast tracks all changes you make locally in a commit log, i.e., a list of changes. In this case, all the other members are asked to agree that the commit can succeed and once they agree, Hazelcast starts to write the changes. 
However, if the member that initiates the commit crashes after it has written to at least one member (but has not completed writing to all other members), your system may be left in an inconsistent state.</p>
<p>On the other hand, if you choose TWO_PHASE as the transaction type, the commit log is again tracked locally but it is copied to another cluster member. Therefore, when a failure happens (e.g. the member initiating the commit crashes), you still have the commit log in another member and that member can complete the commit. However, copying the commit log to another member makes the TWO_PHASE approach slow.</p>
<p>Consequently, it is recommended that you choose ONE_PHASE as the transaction type if you want better performance, and that you choose TWO_PHASE if reliability of your system is more important than the performance. </p>

<a name="providing-xa-transactions"></a><h2 id="providing-xa-transactions">Providing XA Transactions</h2>
<p>XA describes the interface between the global transaction manager and the local resource manager. XA allows multiple resources (such as databases, application servers, message queues, transactional caches, etc.) to be accessed within the same transaction, thereby preserving the ACID properties across applications. XA uses a two-phase commit to ensure that all resources either commit or rollback any particular transaction consistently (all do the same).</p>
<p>When you implement the <code>XAResource</code> interface, Hazelcast provides XA transactions. You can obtain the <code>HazelcastXAResource</code> instance via the <code>HazelcastInstance getXAResource</code> method. You can see the
<a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/transaction/HazelcastXAResource.java">HazelcastXAResource source code here</a>.</p>
<p>Below is example code that uses Atomikos for transaction management.</p>
<pre><code class="lang-java">UserTransactionManager tm = new UserTransactionManager();
tm.setTransactionTimeout(60);
tm.begin();

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
HazelcastXAResource xaResource = hazelcastInstance.getXAResource();

Transaction transaction = tm.getTransaction();
transaction.enlistResource(xaResource);
// other resources (database, app server etc...) can be enlisted

try {
  TransactionContext context = xaResource.getTransactionContext();
  TransactionalMap map = context.getMap(&quot;m&quot;);
  map.put(&quot;key&quot;, &quot;value&quot;);
  // other resource operations

  transaction.delistResource(xaResource, XAResource.TMSUCCESS);
  tm.commit();
} catch (Exception e) {
  tm.rollback();
}
</code></pre>

<a name="integrating-into-j2ee"></a><h2 id="integrating-into-j2ee">Integrating into J2EE</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>You can integrate Hazelcast into J2EE containers. This integration is offered as a Hazelcast plugin. Please see its own GitHub repository at <a href="https://github.com/hazelcast/hazelcast-ra" target="_blank">Hazelcast Resource Adapter</a> for information on configuring the resource adapter, glassfish applications, and JBoss web applications.</p>

<a name="hazelcast-jcache"></a><h1 id="hazelcast-jcache">Hazelcast JCache</h1>
<p>This chapter describes the basics of JCache, the standardized Java caching layer API. The JCache
caching API is specified by the Java Community Process (JCP) as Java Specification Request (JSR) 107.</p>
<p>Caching keeps data in memory that either are slow to calculate/process or originate from another underlying backend system.
Caching is used to prevent additional request round trips for frequently used data. In both cases, caching can be used to
gain performance or decrease application latencies.</p>
<a name="jcache-overview"></a><h2 id="jcache-overview">JCache Overview</h2>
<p>Starting with Hazelcast release 3.3.1, Hazelcast offers a specification-compliant JCache implementation. To show our commitment to this
important specification that the Java world was waiting for over a decade, we did not just provide a simple wrapper around our existing
APIs; we implemented a caching structure from the ground up to optimize the behavior to the needs of JCache.
The Hazelcast JCache implementation is 100% TCK (Technology Compatibility Kit) compliant and therefore passes all specification
requirements.</p>
<p>In addition to the given specification, we added some features like asynchronous versions of almost all
operations to give the user extra power.  </p>
<p>This chapter gives a basic understanding of how to configure your application and how to setup Hazelcast to be your JCache
provider. It also shows examples of basic JCache usage as well as the additionally offered features that are not part of JSR-107.
To gain a full understanding of the JCache functionality and provided guarantees of different operations, read
the specification document (which is also the main documentation for functionality) at the specification page of <a href="https://www.jcp.org/en/jsr/detail?id=107" target="_blank">JSR-107</a>.</p>

<a name="jcache-setup-and-configuration"></a><h2 id="jcache-setup-and-configuration">JCache Setup and Configuration</h2>
<p>This section shows what is necessary to provide the JCache API and the Hazelcast JCache implementation for your application. In
addition, it demonstrates the different configuration options and describes the configuration properties.</p>
<a name="setting-up-your-application"></a><h3 id="setting-up-your-application">Setting up Your Application</h3>
<p>To provide your application with this JCache functionality, your application needs the JCache API inside its classpath. This API is the bridge between the specified JCache standard and the implementation provided by Hazelcast.</p>
<p>The method of integrating the JCache API JAR into the application classpath depends on the build system used. For Maven, Gradle, SBT,
Ivy, and many other build systems, all using Maven-based dependency repositories, perform the integration by adding
the Maven coordinates to the build descriptor.</p>
<p>As already mentioned, you have to add JCache
coordinates next to the default Hazelcast coordinates that might be already part of the application.</p>
<p>For Maven users, the coordinates look like the following code:</p>
<pre><code class="lang-xml">&lt;dependency&gt;
  &lt;groupId&gt;javax.cache&lt;/groupId&gt;
  &lt;artifactId&gt;cache-api&lt;/artifactId&gt;
  &lt;version&gt;1.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>With other build systems, you might need to describe the coordinates in a different way.</p>
<a name="activating-hazelcast-as-jcache-provider"></a><h4 id="activating-hazelcast-as-jcache-provider">Activating Hazelcast as JCache Provider</h4>
<p>To activate Hazelcast as the JCache provider implementation, add either <code>hazelcast-all.jar</code> or
<code>hazelcast.jar</code> to the classpath (if not already available) by either one of the following Maven snippets.</p>
<p>If you use <code>hazelcast-all.jar</code>:</p>
<pre><code class="lang-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
  &lt;artifactId&gt;hazelcast-all&lt;/artifactId&gt;
  &lt;version&gt;&quot;your Hazelcast version, e.g. 3.7&quot;&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>If you use <code>hazelcast.jar</code>:</p>
<pre><code class="lang-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
  &lt;artifactId&gt;hazelcast&lt;/artifactId&gt;
  &lt;version&gt;&quot;your Hazelcast version, e.g. 3.7&quot;&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>The users of other build systems have to adjust the definition of the dependency to their needs.</p>
<a name="connecting-clients-to-remote-member"></a><h4 id="connecting-clients-to-remote-member">Connecting Clients to Remote Member</h4>
<p>When the users want to use Hazelcast clients to connect to a remote cluster, the <code>hazelcast-client.jar</code> dependency is also required
on the client side applications. This JAR is already included in <code>hazelcast-all.jar</code>. Or, you can add it to the classpath using the following
Maven snippet:</p>
<pre><code class="lang-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
  &lt;artifactId&gt;hazelcast-client&lt;/artifactId&gt;
  &lt;version&gt;&quot;your Hazelcast version, e.g. 3.7&quot;&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>For other build systems, for instance, ANT, the users have to download these dependencies from either the JSR-107 specification and
Hazelcast community website (<a href="http://www.hazelcast.org" target="_blank">http://www.hazelcast.org</a>) or from the Maven repository search page
(<a href="http://search.maven.org" target="_blank">http://search.maven.org</a>).</p>

<a name="example-jcache-application"></a><h3 id="example-jcache-application">Example JCache Application</h3>
<p>Before moving on to configuration, let&#39;s have a look at a basic introductory example. The following code shows how to use the Hazelcast JCache integration
inside an application in an easy but typesafe way.</p>
<pre><code class="lang-java">// Retrieve the CachingProvider which is automatically backed by
// the chosen Hazelcast member or client provider
CachingProvider cachingProvider = Caching.getCachingProvider();

// Create a CacheManager
CacheManager cacheManager = cachingProvider.getCacheManager();

// Create a simple but typesafe configuration for the cache
CompleteConfiguration&lt;String, String&gt; config =
    new MutableConfiguration&lt;String, String&gt;()
        .setTypes( String.class, String.class );

// Create and get the cache
Cache&lt;String, String&gt; cache = cacheManager.createCache( &quot;example&quot;, config );
// Alternatively to request an already existing cache
// Cache&lt;String, String&gt; cache = cacheManager
//     .getCache( name, String.class, String.class );

// Put a value into the cache
cache.put( &quot;world&quot;, &quot;Hello World&quot; );

// Retrieve the value again from the cache
String value = cache.get( &quot;world&quot; );

// Print the value &#39;Hello World&#39;
System.out.println( value );
</code></pre>
<p>Although the example is simple, let&#39;s go through the code lines one by one.</p>
<a name="getting-the-hazelcast-jcache-implementation"></a><h4 id="getting-the-hazelcast-jcache-implementation">Getting the Hazelcast JCache Implementation</h4>
<p>First of all, we retrieve the <code>javax.cache.spi.CachingProvider</code> using the static method from
<code>javax.cache.Caching::
getCachingManager</code>, which automatically picks up Hazelcast as the underlying JCache implementation, if
available in the classpath. This way, the Hazelcast implementation of a <code>CachingProvider</code> will automatically start a new Hazelcast
member or client (depending on the chosen provider type) and pick up the configuration from either the command line parameter
or from the classpath. We will show how to use an existing <code>HazelcastInstance</code> later in this chapter; for now, we keep it simple.</p>
<a name="setting-up-the-jcache-entry-point"></a><h4 id="setting-up-the-jcache-entry-point">Setting up the JCache Entry Point</h4>
<p>In the next line, we ask the <code>CachingProvider</code> to return a <code>javax.cache.CacheManager</code>. This is the general application&#39;s entry
point into JCache. The <code>CachingProvider</code> creates and manages named caches.</p>
<a name="configuring-the-cache-before-creating-it"></a><h4 id="configuring-the-cache-before-creating-it">Configuring the Cache Before Creating It</h4>
<p>The next few lines create a simple <code>javax.cache.configuration.MutableConfiguration</code> to configure the cache before actually
creating it. In this case, we only configure the key and value types to make the cache typesafe which is highly recommended and
checked on retrieval of the cache.</p>
<a name="creating-the-cache"></a><h4 id="creating-the-cache">Creating the Cache</h4>
<p>To create the cache, we call <code>javax.cache.CacheManager::createCache</code> with a name for the cache and the previously created
configuration; the call returns the created cache. If you need to retrieve a previously created cache, you can use the corresponding method overload <code>javax.cache.CacheManager::getCache</code>. If the cache was created using type parameters, you must retrieve the cache afterward using the type checking version of <code>getCache</code>.</p>
<a name="get-put-and-getandput"></a><h4 id="get-put-and-getandput">get, put, and getAndPut</h4>
<p>The following lines are simple <code>put</code> and <code>get</code> calls from the <code>java.util.Map</code> interface. The
<code>javax.cache.Cache::put</code> has a <code>void</code> return type and does not return the previously assigned value of the key. To imitate the
<code>java.util.Map::put</code> method, the JCache cache has a method called <code>getAndPut</code>.</p>

<a name="configuring-for-jcache"></a><h3 id="configuring-for-jcache">Configuring for JCache</h3>
<p>Hazelcast JCache provides for two different methods of cache configuration:</p>
<ul>
<li>programmatically: the typical Hazelcast way, using the Config API seen above, and</li>
<li>declaratively: using <code>hazelcast.xml</code> or <code>hazelcast-client.xml</code>.</li>
</ul>
<a name="jcache-declarative-configuration"></a><h4 id="jcache-declarative-configuration">JCache Declarative Configuration</h4>
<p>You can declare your JCache cache configuration using the <code>hazelcast.xml</code> or <code>hazelcast-client.xml</code> configuration files. Using this declarative configuration makes creating the <code>javax.cache.Cache</code> fully transparent and automatically ensures internal thread safety. You do not need a call to <code>javax.cache.Cache::createCache</code> in this case: you can retrieve the cache using
<code>javax.cache.Cache::getCache</code> overloads and by passing in the name defined in the configuration for the cache.</p>
<p>To retrieve the cache that you defined in the declaration files, you need only perform a simple call (example below) because the cache is created automatically by the implementation.</p>
<pre><code class="lang-java">CachingProvider cachingProvider = Caching.getCachingProvider();
CacheManager cacheManager = cachingProvider.getCacheManager();
Cache&lt;Object, Object&gt; cache = cacheManager
    .getCache( &quot;default&quot;, Object.class, Object.class );
</code></pre>
<p>Note that this section only describes the JCache provided standard properties. For the Hazelcast specific properties, please see the
<a href="#icache-configuration">ICache Configuration section</a>.</p>
<pre><code class="lang-xml">&lt;cache name=&quot;default&quot;&gt;
  &lt;key-type class-name=&quot;java.lang.Object&quot; /&gt;
  &lt;value-type class-name=&quot;java.lang.Object&quot; /&gt;
  &lt;statistics-enabled&gt;false&lt;/statistics-enabled&gt;
  &lt;management-enabled&gt;false&lt;/management-enabled&gt;

  &lt;read-through&gt;true&lt;/read-through&gt;
  &lt;write-through&gt;true&lt;/write-through&gt;
  &lt;cache-loader-factory
     class-name=&quot;com.example.cache.MyCacheLoaderFactory&quot; /&gt;
  &lt;cache-writer-factory
     class-name=&quot;com.example.cache.MyCacheWriterFactory&quot; /&gt;
  &lt;expiry-policy-factory
     class-name=&quot;com.example.cache.MyExpirePolicyFactory&quot; /&gt;

  &lt;cache-entry-listeners&gt;
    &lt;cache-entry-listener old-value-required=&quot;false&quot; synchronous=&quot;false&quot;&gt;
      &lt;cache-entry-listener-factory
         class-name=&quot;com.example.cache.MyEntryListenerFactory&quot; /&gt;
      &lt;cache-entry-event-filter-factory
         class-name=&quot;com.example.cache.MyEntryEventFilterFactory&quot; /&gt;
    &lt;/cache-entry-listener&gt;
    ...
  &lt;/cache-entry-listeners&gt;
&lt;/cache&gt;
</code></pre>
<ul>
<li><code>key-type#class-name</code>: Fully qualified class name of the cache key type. Its default value is <code>java.lang.Object</code>.</li>
<li><code>value-type#class-name</code>: Fully qualified class name of the cache value type. Its default value is <code>java.lang.Object</code>.</li>
<li><code>statistics-enabled</code>: If set to true, statistics like cache hits and misses are collected. Its default value is false.</li>
<li><code>management-enabled</code>: If set to true, JMX beans are enabled and collected statistics are provided. It doesn&#39;t automatically enable statistics collection. Defaults to false.</li>
<li><code>read-through</code>: If set to true, enables read-through behavior of the cache to an underlying configured <code>javax.cache.integration.CacheLoader</code> which is also known as lazy-loading. Its default value is false.</li>
<li><code>write-through</code>: If set to true, enables write-through behavior of the cache to an underlying configured <code>javax.cache.integration.CacheWriter</code> which passes any changed value to the external backend resource. Its default value is false.</li>
<li><code>cache-loader-factory#class-name</code>: Fully qualified class name of the <code>javax.cache.configuration.Factory</code> implementation providing a <code>javax.cache.integration.CacheLoader</code> instance to the cache.</li>
<li><code>cache-writer-factory#class-name</code>: Fully qualified class name of the <code>javax.cache.configuration.Factory</code> implementation providing a <code>javax.cache.integration.CacheWriter</code> instance to the cache.</li>
<li><code>expiry-policy-factory#-class-name</code>: Fully qualified class name of the <code>javax.cache.configuration.Factory</code> implementation providing a <code>javax.cache.expiry.ExpiryPolicy</code> instance to the cache.</li>
<li><code>cache-entry-listener</code>: A set of attributes and elements, explained below, to describe a <code>javax.cache.event.
CacheEntryListener</code>.<ul>
<li><code>cache-entry-listener#old-value-required</code>: If set to true, previously assigned values for the affected keys will be sent to the <code>javax.cache.event.CacheEntryListener</code> implementation. Setting this attribute to true creates additional traffic. Its default value is false.</li>
<li><code>cache-entry-listener#synchronous</code>: If set to true, the <code>javax.cache.event.CacheEntryListener</code> implementation will be called in a synchronous manner. Its default value is false.</li>
<li><code>cache-entry-listener/entry-listener-factory#class-name</code>: Fully qualified class name of the <code>javax.cache.configuration.Factory</code> implementation providing a <code>javax.cache.event.CacheEntryListener</code> instance.</li>
<li><code>cache-entry-listener/entry-event-filter-factory#class-name</code>: Fully qualified class name of the <code>javax.cache.configuration.Factory</code> implementation providing a <code>javax.cache.event.
CacheEntryEventFilter</code> instance.</li>
</ul>
</li>
</ul>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The JMX MBeans provided by Hazelcast JCache show statistics of the local member only.
To show the cluster-wide statistics, the user should collect statistic information from all members and accumulate them to
the overall statistics.</em>
<br></br></p>
<a name="jcache-programmatic-configuration"></a><h4 id="jcache-programmatic-configuration">JCache Programmatic Configuration</h4>
<p>To configure the JCache programmatically:</p>
<ul>
<li>either instantiate <code>javax.cache.configuration.MutableConfiguration</code> if you will use
only the JCache standard configuration,</li>
<li>or instantiate <code>com.hazelcast.config.CacheConfig</code> for a deeper Hazelcast integration.</li>
</ul>
<p><code>com.hazelcast.config.CacheConfig</code> offers additional options that are specific to Hazelcast, such as asynchronous and synchronous backup counts.
Both classes share the same supertype interface <code>javax.cache.configuration.
CompleteConfiguration</code> which is part of the JCache
standard.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>To stay vendor independent, try to keep your code as near as possible to the standard JCache API. We recommend that you use declarative configuration
and that you use the <code>javax.cache.configuration.Configuration</code> or <code>javax.cache.configuration.CompleteConfiguration</code> interfaces in
your code only when you need to pass the configuration instance throughout your code.</em>
<br></br></p>
<p>If you don&#39;t need to configure Hazelcast specific properties, we recommend that you instantiate
<code>javax.cache.configuration.MutableConfiguration</code> and that you use the setters to configure Hazelcast as shown in the example in the
<a href="#example-jcache-application">Example JCache Application section</a>. Since the configurable properties are the same as the ones explained in the
<a href="#jcache-declarative-configuration">JCache Declarative Configuration section</a>, they are not mentioned here. For Hazelcast specific
properties, please read the <a href="#icache-configuration">ICache Configuration section</a> section.</p>

<a name="jcache-providers"></a><h2 id="jcache-providers">JCache Providers</h2>
<p>Use JCache providers to create caches for a specification compliant implementation. Those providers abstract the platform
specific behavior and bindings, and provide the different JCache required features.</p>
<p>Hazelcast has two types of providers. Depending on your application setup and the cluster topology,
you can use the Client Provider (used by Hazelcast clients) or the Server Provider (used by cluster members).</p>
<a name="configuring-jcache-provider"></a><h3 id="configuring-jcache-provider">Configuring JCache Provider</h3>
<p>Configure the JCache <code>javax.cache.spi.CachingProvider</code> by either specifying the provider at the command line or by declaring the provider inside the Hazelcast configuration XML file. For more information on setting properties in this XML
configuration file, please see the <a href="#jcache-declarative-configuration">JCache Declarative Configuration section</a>.</p>
<p>Hazelcast implements a delegating <code>CachingProvider</code> that can automatically be configured for either client or member mode and that
delegates to the real underlying implementation based on the user&#39;s choice. Hazelcast recommends that you use this <code>CachingProvider</code>
implementation.</p>
<p>The delegating <code>CachingProvider</code>s fully qualified class name is</p>
<pre><code class="lang-plain">com.hazelcast.cache.HazelcastCachingProvider
</code></pre>
<p>To configure the delegating provider at the command line, add the following parameter to the Java startup call, depending on the chosen provider:</p>
<pre><code class="lang-plain">-Dhazelcast.jcache.provider.type=[client|server]
</code></pre>
<p>By default, the delegating <code>CachingProvider</code> is automatically picked up by the JCache SPI and provided as shown above. In cases where multiple <code>javax.cache.spi.CachingProvider</code> implementations reside on the classpath (like in some Application
Server scenarios), you can select a <code>CachingProvider</code> by explicitly calling <code>Caching::getCachingProvider</code>
overloads and providing them using the canonical class name of the provider to be used. The class names of member and client providers
provided by Hazelcast are mentioned in the following two subsections.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Hazelcast advises that you use the <code>Caching::getCachingProvider</code> overloads to select a
<code>CachingProvider</code> explicitly. This ensures that uploading to later environments or Application Server versions doesn&#39;t result in
unexpected behavior like choosing a wrong <code>CachingProvider</code>.</em>
<br></br></p>
<p>For more information on cluster topologies and Hazelcast clients, please see the <a href="#hazelcast-topology">Hazelcast Topology section</a>.</p>

<a name="configuring-jcache-with-client-provider"></a><h3 id="configuring-jcache-with-client-provider">Configuring JCache with Client Provider</h3>
<p>For cluster topologies where Hazelcast light clients are used to connect to a remote Hazelcast cluster, use the Client Provider to configure JCache.</p>
<p>The Client Provider provides the same features as the Server Provider. However, it does not hold data on its own but instead delegates requests and calls to the remotely connected cluster.</p>
<p>The Client Provider can connect to multiple clusters at the same time. This can be achieved by scoping the client side
<code>CacheManager</code> with different Hazelcast configuration files. For more information, please see
<a href="#scoping-to-join-clusters">Scoping to Join Clusters</a>.</p>
<p>To request this <code>CachingProvider</code> using <code>Caching#getCachingProvider( String )</code> or
<code>Caching#getCachingProvider( String, ClassLoader )</code>, use the following fully qualified class name:</p>
<pre><code class="lang-plain">com.hazelcast.client.cache.impl.HazelcastClientCachingProvider
</code></pre>

<a name="configuring-jcache-with-server-provider"></a><h3 id="configuring-jcache-with-server-provider">Configuring JCache with Server Provider</h3>
<p>If a Hazelcast member is embedded into an application directly and the Hazelcast client is not used, the Server Provider is
required. In this case, the member itself becomes a part of the distributed cache and requests and operations are distributed
directly across the cluster by its given key.</p>
<p>The Server Provider provides the same features as the Client provider, but it keeps data in the local Hazelcast member and also distributes
non-owned keys to other direct cluster members.</p>
<p>Like the Client Provider, the Server Provider can connect to multiple clusters at the same time. This can be achieved by scoping the client side <code>CacheManager</code> with different Hazelcast configuration files. For more
information please see <a href="#scoping-to-join-clusters">Scoping to Join Clusters</a>.</p>
<p>To request this <code>CachingProvider</code> using <code>Caching#getCachingProvider( String )</code> or
<code>Caching#getCachingProvider( String, ClassLoader )</code>, use the following fully qualified class name:</p>
<pre><code class="lang-plain">com.hazelcast.cache.impl.HazelcastServerCachingProvider
</code></pre>

<a name="jcache-api"></a><h2 id="jcache-api">JCache API</h2>
<p>This section explains the JCache API by providing simple examples and use cases. While walking through the examples, we will have
a look at a couple of the standard API classes and see how these classes are used.</p>
<a name="jcache-api-application-example"></a><h3 id="jcache-api-application-example">JCache API Application Example</h3>
<p>The code in this subsection creates a small account application by providing a caching layer over an imagined database abstraction. The
database layer will be simulated using single demo data in a simple DAO interface. To show the difference between the &quot;database&quot;
access and retrieving values from the cache, a small waiting time is used in the DAO implementation to simulate network and
database latency.</p>
<a name="creating-user-class-example"></a><h4 id="creating-user-class-example">Creating User Class Example</h4>
<p>Before we implement the JCache caching layer, let&#39;s have a quick look at some basic
classes we need for this example.</p>
<p>The <code>User</code> class is the representation of a user table in the database. To keep it simple, it has just two properties:
<code>userId</code> and <code>username</code>.</p>
<pre><code class="lang-java">public class User {
  private int userId;
  private String username;

  // Getters and setters
}
</code></pre>
<a name="creating-dao-interface-example"></a><h4 id="creating-dao-interface-example">Creating DAO Interface Example</h4>
<p>The DAO interface is also kept easy in this example. It provides a simple method to retrieve (find) a user by its <code>userId</code>.</p>
<pre><code class="lang-java">public interface UserDAO {
  User findUserById( int userId );
  boolean storeUser( int userId, User user );
  boolean removeUser( int userId );
  Collection&lt;Integer&gt; allUserIds();
}
</code></pre>
<a name="configuring-jcache-example"></a><h4 id="configuring-jcache-example">Configuring JCache Example</h4>
<p>To show most of the standard features, the configuration example is a little more complex.</p>
<pre><code class="lang-java">// Create javax.cache.configuration.CompleteConfiguration subclass
CompleteConfiguration&lt;Integer, User&gt; config =
    new MutableConfiguration&lt;Integer, User&gt;()
        // Configure the cache to be typesafe
        .setTypes( Integer.class, User.class )
        // Configure to expire entries 30 secs after creation in the cache
        .setExpiryPolicyFactory( FactoryBuilder.factoryOf(
            new AccessedExpiryPolicy( new Duration( TimeUnit.SECONDS, 30 ) )
        ) )
        // Configure read-through of the underlying store
        .setReadThrough( true )
        // Configure write-through to the underlying store
        .setWriteThrough( true )
        // Configure the javax.cache.integration.CacheLoader
        .setCacheLoaderFactory( FactoryBuilder.factoryOf(
            new UserCacheLoader( userDao )
        ) )
        // Configure the javax.cache.integration.CacheWriter
        .setCacheWriterFactory( FactoryBuilder.factoryOf(
            new UserCacheWriter( userDao )
        ) )
        // Configure the javax.cache.event.CacheEntryListener with no
        // javax.cache.event.CacheEntryEventFilter, to include old value
        // and to be executed synchronously
        .addCacheEntryListenerConfiguration(
            new MutableCacheEntryListenerConfiguration&lt;Integer, User&gt;(
                new UserCacheEntryListenerFactory(),
                null, true, true
            )
        );
</code></pre>
<p>Let&#39;s go through this configuration line by line.</p>
<a name="setting-the-cache-type-and-expire-policy"></a><h5 id="setting-the-cache-type-and-expire-policy">Setting the Cache Type and Expire Policy</h5>
<p>First, we set the expected types for the cache, which is already known from the previous example. On the next line, a
<code>javax.cache.expiry.ExpirePolicy</code> is configured. Almost all integration <code>ExpirePolicy</code> implementations are configured using
<code>javax.cache.configuration.Factory</code> instances. <code>Factory</code> and <code>FactoryBuilder</code> are explained later in this chapter.</p>
<a name="configuring-read-through-and-write-through"></a><h5 id="configuring-read-through-and-write-through">Configuring Read-Through and Write-Through</h5>
<p>The next two lines configure the thread that will be read-through and write-through to the underlying backend resource that is configured
over the next few lines. The JCache API offers <code>javax.cache.integration.CacheLoader</code> and <code>javax.cache.integration.CacheWriter</code> to
implement adapter classes to any kind of backend resource, e.g. JPA, JDBC, or any other backend technology implementable in Java.
The interface provides the typical CRUD operations like <code>create</code>, <code>get</code>, <code>update</code>, <code>delete</code>, and some bulk operation versions of those
common operations. We will look into the implementation of those implementations later.</p>
<a name="configuring-entry-listeners"></a><h5 id="configuring-entry-listeners">Configuring Entry Listeners</h5>
<p>The last configuration setting defines entry listeners based on sub-interfaces of <code>javax.cache.event.CacheEntryListener</code>. This
config does not use a <code>javax.cache.event.CacheEntryEventFilter</code> since the listener is meant to be fired on every change that
happens on the cache. Again we will look in the implementation of the listener in later in this chapter.</p>
<a name="full-example-code"></a><h5 id="full-example-code">Full Example Code</h5>
<p>A full running example that is presented in this
subsection is available in the
<a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/jcache/src/main/java/com/hazelcast/examples/application" target="_blank">code samples repository</a>.
The application is built to be a command line app. It offers a small shell to accept different commands. After startup, you can
enter <code>help</code> to see all available commands and their descriptions.</p>

<a name="jcache-base-classes"></a><h3 id="jcache-base-classes">JCache Base Classes</h3>
<p>In the <a href="#example-jcache-application">Example JCache Application section</a>, we have already seen a couple of the base classes and explained how those work. The following are quick descriptions of them:</p>
<p><strong><code>javax.cache.Caching</code></strong>:</p>
<p>The access point into the JCache API. It retrieves the general <code>CachingProvider</code> backed by any compliant JCache
implementation, such as Hazelcast JCache.</p>
<p><strong><code>javax.cache.spi.CachingProvider</code></strong>:</p>
<p>The SPI that is implemented to bridge between the JCache API and the implementation itself. Hazelcast members and clients use different
providers chosen as seen in the <a href="#configuring-jcache-provider">Configuring JCache Provider section</a> which enable the JCache API to
interact with Hazelcast clusters.</p>
<p>When a <code>javax.cache.spi.CachingProvider::getCacheManager</code> overload is used that takes a <code>java.lang.ClassLoader</code> argument, this
classloader will be part of the scope of the created <code>java.cache.Cache</code> and it is not possible to retrieve it on other members.
We advise not to use those overloads, as they are not meant to be used in distributed environments!</p>
<p><strong><code>javax.cache.CacheManager</code></strong>:</p>
<p>The <code>CacheManager</code> provides the capability to create new and manage existing JCache caches.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>A <code>javax.cache.Cache</code> instance created with key and value types in the configuration
provides a type checking of those types at retrieval of the cache. For that reason, all non-types retrieval methods like
<code>getCache</code> throw an exception because types cannot be checked.</em>
<br></br></p>
<p><strong><code>javax.cache.configuration.Configuration</code></strong>, <strong><code>javax.cache.configuration.MutableConfiguration</code></strong>:</p>
<p>These two classes are used to configure a cache prior to retrieving it from a <code>CacheManager</code>. The <code>Configuration</code> interface,
therefore, acts as a common super type for all compatible configuration classes such as <code>MutableConfiguration</code>.</p>
<p>Hazelcast itself offers a special implementation (<code>com.hazelcast.config.CacheConfig</code>) of the <code>Configuration</code> interface which
offers more options on the specific Hazelcast properties that can be set to configure features like synchronous and asynchronous
backups counts or selecting the underlying <a href="#setting-in-memory-format">In Memory Format</a> of the cache. For more information on this
configuration class, please see the reference in <a href="#jcache-programmatic-configuration">JCache Programmatic Configuration section</a>.</p>
<p><strong><code>javax.cache.Cache</code></strong>:</p>
<p>This interface represents the cache instance itself. It is comparable to <code>java.util.Map</code> but offers special operations dedicated
to the caching use case. Therefore, for example <code>javax.cache.Cache::put</code>, unlike <code>java.util.Map::put</code>, does not return the old
value previously assigned to the given key.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Bulk operations on the <code>Cache</code> interface guarantee atomicity per entry but not over
all given keys in the same bulk operations since no transactional behavior is applied over the whole batch process.</em>
<br></br></p>

<a name="implementing-factory-and-factorybuilder"></a><h3 id="implementing-factory-and-factorybuilder">Implementing Factory and FactoryBuilder</h3>
<p>The <code>javax.cache.configuration.Factory</code> implementations configure features like
<code>CacheEntryListener</code>, <code>ExpirePolicy</code>, and <code>CacheLoader</code>s or <code>CacheWriter</code>s. These factory implementations are required to distribute the
different features to members in a cluster environment like Hazelcast. Therefore, these factory implementations have to be serializable.</p>
<p><code>Factory</code> implementations are easy to do, as they follow the default Provider- or Factory-Pattern. The sample class
<code>UserCacheEntryListenerFactory</code> shown below implements a custom JCache <code>Factory</code>.</p>
<pre><code class="lang-java">public class UserCacheEntryListenerFactory
    implements Factory&lt;CacheEntryListener&lt;Integer, User&gt;&gt; {

  @Override
  public CacheEntryListener&lt;Integer, User&gt; create() {
    // Just create a new listener instance
    return new UserCacheEntryListener();
  }
}
</code></pre>
<p>To simplify the process for the users, JCache API offers a set of helper methods collected in
<code>javax.cache.
configuration.FactoryBuilder</code>. In the above configuration example, <code>FactoryBuilder::factoryOf</code> creates a
singleton factory for the given instance.</p>

<a name="implementing-cacheloader"></a><h3 id="implementing-cacheloader">Implementing CacheLoader</h3>
<p><code>javax.cache.integration.CacheLoader</code> loads cache entries from any external backend resource. </p>
<a name="cache-read-through"></a><h4 id="cache-read-through">Cache read-through</h4>
<p>If the cache is
configured to be <code>read-through</code>, then <code>CacheLoader::load</code> is called transparently from the cache when the key or the value is not
yet found in the cache. If no value is found for a given key, it returns null.</p>
<p>If the cache is not configured to be <code>read-through</code>, nothing is loaded automatically. The user code must call <code>javax.cache.Cache::loadAll</code> to load data for the given set of keys into the cache.</p>
<p>For the bulk load operation (<code>loadAll()</code>), some keys may not be found in the returned result set. In this case, a <code>javax.cache.integration.CompletionListener</code> parameter can be used as an asynchronous callback after all the key-value pairs are loaded because loading many key-value pairs can take lots of time.</p>
<a name="cacheloader-example"></a><h4 id="cacheloader-example">CacheLoader Example</h4>
<p>Let&#39;s look at the <code>UserCacheLoader</code> implementation. This implementation is quite straight forward.</p>
<ul>
<li>It implements <code>CacheLoader</code>.</li>
<li>It overrides the <code>load</code> method to compute or retrieve the value corresponding to <code>key</code>.</li>
<li>It overrides the <code>loadAll</code> method to compute or retrieve the values corresponding to <code>keys</code>.</li>
</ul>
<p>An important note is that
any kind of exception has to be wrapped into <code>javax.cache.integration.CacheLoaderException</code>.</p>
<pre><code class="lang-java">public class UserCacheLoader
    implements CacheLoader&lt;Integer, User&gt;, Serializable {

  private final UserDao userDao;

  public UserCacheLoader( UserDao userDao ) {
    // Store the dao instance created externally
    this.userDao = userDao;
  }

  @Override
  public User load( Integer key ) throws CacheLoaderException {
    // Just call through into the dao
    return userDao.findUserById( key );
  }

  @Override
  public Map&lt;Integer, User&gt; loadAll( Iterable&lt;? extends Integer&gt; keys )
      throws CacheLoaderException {

    // Create the resulting map  
    Map&lt;Integer, User&gt; loaded = new HashMap&lt;Integer, User&gt;();
    // For every key in the given set of keys
    for ( Integer key : keys ) {
      // Try to retrieve the user
      User user = userDao.findUserById( key );
      // If user is not found do not add the key to the result set
      if ( user != null ) {
        loaded.put( key, user );
      }
    }
    return loaded;
  }
}
</code></pre>

<a name="cachewriter"></a><h3 id="cachewriter">CacheWriter</h3>
<p>You use a <code>javax.cache.integration.CacheWriter</code> to update an external backend resource. If the cache is configured to be
<code>write-through</code>, this process is executed transparently to the user&#39;s code. Otherwise, there is currently no way to trigger
writing changed entries to the external resource to a user-defined point in time.</p>
<p>If bulk operations throw an exception, <code>java.util.Collection</code> has to be cleaned of all successfully written keys so
the cache implementation can determine what keys are written and can be applied to the cache state.</p>
<p>The following example performs the following tasks:</p>
<ul>
<li>It implements <code>CacheWriter</code>.</li>
<li>It overrides the <code>write</code> method to write the specified entry to the underlying store.</li>
<li>It overrides the <code>writeAll</code> method to write the specified entires to the underlying store.</li>
<li>It overrides the <code>delete</code> method to delete the key entry from the store.</li>
<li>It overrides the <code>deleteAll</code> method to delete the data and keys from the underlying store for the given collection of keys, if present.</li>
</ul>
<pre><code class="lang-java">public class UserCacheWriter
    implements CacheWriter&lt;Integer, User&gt;, Serializable {

  private final UserDao userDao;

  public UserCacheWriter( UserDao userDao ) {
    // Store the dao instance created externally
    this.userDao = userDao;
  }

  @Override
  public void write( Cache.Entry&lt;? extends Integer, ? extends User&gt; entry )
      throws CacheWriterException {

    // Store the user using the dao
    userDao.storeUser( entry.getKey(), entry.getValue() );
  }

  @Override
  public void writeAll( Collection&lt;Cache.Entry&lt;...&gt;&gt; entries )
      throws CacheWriterException {

    // Retrieve the iterator to clean up the collection from
    // written keys in case of an exception
    Iterator&lt;Cache.Entry&lt;...&gt;&gt; iterator = entries.iterator();
    while ( iterator.hasNext() ) {
      // Write entry using dao
      write( iterator.next() );
      // Remove from collection of keys
      iterator.remove();
    }
  }

  @Override
  public void delete( Object key ) throws CacheWriterException {
    // Test for key type
    if ( !( key instanceof Integer ) ) {
      throw new CacheWriterException( &quot;Illegal key type&quot; );
    }
    // Remove user using dao
    userDao.removeUser( ( Integer ) key );
  }

  @Override
  public void deleteAll( Collection&lt;?&gt; keys ) throws CacheWriterException {
    // Retrieve the iterator to clean up the collection from
    // written keys in case of an exception
    Iterator&lt;?&gt; iterator = keys.iterator();
    while ( iterator.hasNext() ) {
      // Write entry using dao
      delete( iterator.next() );
      // Remove from collection of keys
      iterator.remove();
    }
  }
}
</code></pre>
<p>Again, the implementation is pretty straightforward and also as above all exceptions thrown by the external resource, like
<code>java.sql.SQLException</code> has to be wrapped into a <code>javax.cache.integration.CacheWriterException</code>. Note this is a different
exception from the one thrown by <code>CacheLoader</code>.</p>

<a name="implementing-entryprocessor"></a><h3 id="implementing-entryprocessor">Implementing EntryProcessor</h3>
<p>With <code>javax.cache.processor.EntryProcessor</code>, you can apply an atomic function to a cache entry. In a distributed
environment like Hazelcast, you can move the mutating function to the member that owns the key. If the value
object is big, it might prevent traffic by sending the object to the mutator and sending it back to the owner to update it.</p>
<p>By default, Hazelcast JCache sends the complete changed value to the backup partition. Again, this can cause a lot of traffic if
the object is big. The Hazelcast ICache extension can also prevent this. Further information is available at
<a href="#implementing-backupawareentryprocessor">Implementing BackupAwareEntryProcessor</a>.</p>
<p>An arbitrary number of arguments can be passed to the <code>Cache::invoke</code> and <code>Cache::invokeAll</code> methods. All of those arguments need
to be fully serializable because in a distributed environment like Hazelcast, it is very likely that these arguments have to be passed around the cluster.</p>
<p>The following example performs the following tasks.</p>
<ul>
<li>It implements <code>EntryProcessor</code>.</li>
<li>It overrides the <code>process</code> method to process an entry.</li>
</ul>
<pre><code class="lang-java">public class UserUpdateEntryProcessor
    implements EntryProcessor&lt;Integer, User, User&gt; {

  @Override
  public User process( MutableEntry&lt;Integer, User&gt; entry, Object... arguments )
      throws EntryProcessorException {

    // Test arguments length
    if ( arguments.length &lt; 1 ) {
      throw new EntryProcessorException( &quot;One argument needed: username&quot; );
    }

    // Get first argument and test for String type
    Object argument = arguments[0];
    if ( !( argument instanceof String ) ) {
      throw new EntryProcessorException(
          &quot;First argument has wrong type, required java.lang.String&quot; );
    }

    // Retrieve the value from the MutableEntry
    User user = entry.getValue();

    // Retrieve the new username from the first argument
    String newUsername = ( String ) arguments[0];

    // Set the new username
    user.setUsername( newUsername );

    // Set the changed user to mark the entry as dirty
    entry.setValue( user );

    // Return the changed user to return it to the caller
    return user;
  }
}
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>By executing the bulk <code>Cache::invokeAll</code> operation, atomicity is only guaranteed for a
single cache entry. No transactional rules are applied to the bulk operation.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>JCache <code>EntryProcessor</code> implementations are not allowed to call
<code>javax.cache.Cache</code> methods. This prevents operations from deadlocking between different calls.</em>
<br></br></p>
<p>In addition, when using a <code>Cache::invokeAll</code> method, a <code>java.util.Map</code> is returned that maps the key to its
<code>javax.cache.processor.EntryProcessorResult</code>, which itself wraps the actual result or a thrown
<code>javax.cache.processor.EntryProcessorException</code>.</p>

<a name="cacheentrylistener"></a><h3 id="cacheentrylistener">CacheEntryListener</h3>
<p>The <code>javax.cache.event.CacheEntryListener</code> implementation is straight forward. <code>CacheEntryListener</code> is a super-interface that is used as a marker for listener classes in JCache. The specification brings a set of sub-interfaces.</p>
<ul>
<li><code>CacheEntryCreatedListener</code>: Fires after a cache entry is added (even on read-through by a <code>CacheLoader</code>) to the cache.</li>
<li><code>CacheEntryUpdatedListener</code>: Fires after an already existing cache entry updates.</li>
<li><code>CacheEntryRemovedListener</code>: Fires after a cache entry was removed (not expired) from the cache.</li>
<li><code>CacheEntryExpiredListener</code>: Fires after a cache entry has been expired. Expiry does not have to be a parallel process-- it is only required to be executed on the keys that are requested by <code>Cache::get</code> and some other operations. For a full table of expiry please see the <a href="https://www.jcp.org/en/jsr/detail?id=107" target="_blank">https://www.jcp.org/en/jsr/detail?id=107</a> point 6.  </li>
</ul>
<p>To configure <code>CacheEntryListener</code>, add a <code>javax.cache.configuration.CacheEntryListenerConfiguration</code> instance to
the JCache configuration class, as seen in the above example configuration. In addition, listeners can be configured to be
executed synchronously (blocking the calling thread) or asynchronously (fully running in parallel).</p>
<p>In this example application, the listener is implemented to print event information on the console. That visualizes what is going on in the cache. This application performs the following tasks:</p>
<ul>
<li>It implements CacheEntryCreatedListener.</li>
<li>It implements the <code>onCreated</code> method to call after an entry is created.</li>
<li>It implements the <code>onUpdated</code> method to call after an entry is updated.</li>
<li>It implements the <code>onRemoved</code> method to call after an entry is removed.</li>
<li>It implements the <code>onExpired</code> method to call after an entry expires.</li>
<li>It implements <code>printEvents</code> to print event information on the console.</li>
</ul>
<pre><code class="lang-java">public class UserCacheEntryListener
    implements CacheEntryCreatedListener&lt;Integer, User&gt;,
        CacheEntryUpdatedListener&lt;Integer, User&gt;,
        CacheEntryRemovedListener&lt;Integer, User&gt;,
        CacheEntryExpiredListener&lt;Integer, User&gt; {

  @Override
  public void onCreated( Iterable&lt;CacheEntryEvent&lt;...&gt;&gt; cacheEntryEvents )
      throws CacheEntryListenerException {

    printEvents( cacheEntryEvents );
  }

  @Override
  public void onUpdated( Iterable&lt;CacheEntryEvent&lt;...&gt;&gt; cacheEntryEvents )
      throws CacheEntryListenerException {

    printEvents( cacheEntryEvents );
  }

  @Override
  public void onRemoved( Iterable&lt;CacheEntryEvent&lt;...&gt;&gt; cacheEntryEvents )
      throws CacheEntryListenerException {

    printEvents( cacheEntryEvents );
  }

  @Override
  public void onExpired( Iterable&lt;CacheEntryEvent&lt;...&gt;&gt; cacheEntryEvents )
      throws CacheEntryListenerException {

    printEvents( cacheEntryEvents );
  }

  private void printEvents( Iterable&lt;CacheEntryEvent&lt;...&gt;&gt; cacheEntryEvents ) {
    Iterator&lt;CacheEntryEvent&lt;...&gt;&gt; iterator = cacheEntryEvents.iterator();
    while ( iterator.hasNext() ) {
      CacheEntryEvent&lt;...&gt; event = iterator.next();
      System.out.println( event.getEventType() );
    }
  }
}
</code></pre>

<a name="expirepolicy"></a><h3 id="expirepolicy">ExpirePolicy</h3>
<p>In JCache, <code>javax.cache.expiry.ExpirePolicy</code> implementations are used to automatically expire cache entries based on different rules.</p>
<p>Expiry timeouts are defined using <code>javax.cache.expiry.Duration</code>, which is a pair of <code>java.util.concurrent.TimeUnit</code>, that
describes a time unit and a long, defining the timeout value. The minimum allowed <code>TimeUnit</code> is <code>TimeUnit.MILLISECONDS</code>.
The long value <code>durationAmount</code> must be equal or greater than zero. A value of zero (or <code>Duration.ZERO</code>) indicates that the
cache entry expires immediately.</p>
<p>By default, JCache delivers a set of predefined expiry strategies in the standard API.</p>
<ul>
<li><code>AccessedExpiryPolicy</code>: Expires after a given set of time measured from creation of the cache entry. The expiry timeout is updated on accessing the key.</li>
<li><code>CreatedExpiryPolicy</code>: Expires after a given set of time measured from creation of the cache entry. The expiry timeout is never updated.</li>
<li><code>EternalExpiryPolicy</code>: Never expires. This is the default behavior, similar to <code>ExpiryPolicy</code> being set to null.</li>
<li><code>ModifiedExpiryPolicy</code>: Expires after a given set of time measured from creation of the cache entry. The expiry timeout is updated on updating the key.</li>
<li><code>TouchedExpiryPolicy</code>: Expires after a given set of time measured from creation of the cache entry. The expiry timeout is updated on accessing or updating the key.</li>
</ul>
<p>Because <code>EternalExpirePolicy</code> does not expire cache entries, it is still possible to evict values from memory if an underlying
<code>CacheLoader</code> is defined.</p>

<a name="jcache-hazelcast-instance-integration"></a><h2 id="jcache-hazelcast-instance-integration">JCache - Hazelcast Instance Integration</h2>
<p>You can retrieve <code>javax.cache.Cache</code> instances directly through <code>HazelcastInstance::getCache(String name)</code> method.
The parameter <code>name</code> in <code>HazelcastInstance::getCache(String name)</code> is the full cache name except the Hazelcast prefix, i.e., <code>/hz/</code>. </p>
<p>If you create a cache through a <code>CacheManager</code> which has its own specified URI scope (and/or specified classloader), 
it must be prepended to the pure cache name as a prefix while retrieving the cache through <code>HazelcastInstance::getCache(String name)</code>. 
Prefix generation for full cache name (except the Hazelcast prefix, which is <code>/hz/</code>) is exposed through 
<code>com.hazelcast.cache.CacheUtil#getPrefixedCacheName(String name, java.net.URI uri, ClassLoader classloader)</code>. 
If the URI scope and classloader is not specified, the pure cache name can be used directly while retrieving cache over <code>HazelcastInstance</code>.</p>
<p>If you have a cache which is not created, but is defined/exists (cache is specified in Hazelcast configuration but not created yet), you can retrieve this cache by its name.  This also triggers cache creation before retrieving it. This retrieval is supported through <code>HazelcastInstance</code>. However, <code>HazelcastInstance</code> <strong><em>does not</em></strong> support creating a cache by specifying configuration; this is supported   by Hazelcast&#39;s <code>CacheManager</code> as it is.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If a valid (rather than </em>1.0.0-PFD<em> or </em>0.x<em> versions) JCache library does not exist on the classpath, <code>IllegalStateException</code> is thrown.</em>
<br></br></p>
<a name="jcache-and-hazelcast-instance-awareness"></a><h3 id="jcache-and-hazelcast-instance-awareness">JCache and Hazelcast Instance Awareness</h3>
<p><code>HazelcastInstance</code> is injected into the following cache API interfaces (provided by <code>javax.cache.Cache</code> and <code>com.hazelcast.cache.ICache</code>)  if they implement <code>HazelcastInstanceAware</code> interface:</p>
<ul>
<li><code>ExpiryPolicyFactory</code> and <code>ExpiryPolicy</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CacheLoaderFactory</code> and <code>CacheLoader</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CacheWriteFactory</code> and <code>CacheWriter</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>EntryProcessor</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CacheEntryListener</code> (<code>CacheEntryCreatedListener</code>, <code>CacheEntryUpdatedListener</code>, <code>CacheEntryRemovedListener</code>, <code>CacheEntryExpiredListener</code>) [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CacheEntryEventFilter</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CompletionListener</code> [provided by <code>javax.cache.Cache</code>]</li>
<li><code>CachePartitionLostListener</code> [provided by <code>com.hazelcast.cache.ICache</code>]</li>
</ul>

<a name="hazelcast-jcache-extension-icache"></a><h2 id="hazelcast-jcache-extension-icache">Hazelcast JCache Extension - ICache</h2>
<p>Hazelcast provides extension methods to Cache API through the interface <code>com.hazelcast.cache.ICache</code>.</p>
<p>It has two sets of extensions:</p>
<ul>
<li>Asynchronous version of all cache operations. See <a href="#icache-async-methoods">Async Operations</a>.</li>
<li>Cache operations with custom <code>ExpiryPolicy</code> parameter to apply on that specific operation. See <a href="#defining-a-custom-expirypolicy">Custom ExpiryPolicy</a>.</li>
</ul>
<a name="scoping-to-join-clusters"></a><h3 id="scoping-to-join-clusters">Scoping to Join Clusters</h3>
<p>As mentioned before, you can scope a <code>CacheManager</code> in the case of a client to connect to multiple clusters. In the case of an embedded member, you can scope a <code>CacheManager</code> to join different clusters at the same time. This process is called scoping. To apply scoping, request
a <code>CacheManager</code> by passing a <code>java.net.URI</code> instance to <code>CachingProvider::getCacheManager</code>. The <code>java.net.URI</code> instance must point to either a Hazelcast configuration or to the name of a named
<code>com.hazelcast.core.HazelcastInstance</code> instance.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Multiple requests for the same <code>java.net.URI</code> result in returning a <code>CacheManager</code>
instance that shares the same <code>HazelcastInstance</code> as the <code>CacheManager</code> returned by the previous call.</em>
<br></br></p>
<a name="applying-configuration-scope"></a><h4 id="applying-configuration-scope">Applying Configuration Scope</h4>
<p>To connect or join different clusters, apply a configuration scope to the <code>CacheManager</code>. If the same <code>URI</code> is
used to request a <code>CacheManager</code> that was created previously, those <code>CacheManager</code>s share the same underlying <code>HazelcastInstance</code>.</p>
<p>To apply a configuration scope, pass in the path of the configuration file using the location property
<code>HazelcastCachingProvider#HAZELCAST_CONFIG_LOCATION</code> (which resolves to <code>hazelcast.config.location</code>) as a mapping inside a
<code>java.util.Properties</code> instance to the <code>CachingProvider#getCacheManager(uri, classLoader, properties)</code> call.</p>
<p>Here is an example of using Configuration Scope.</p>
<pre><code class="lang-java">CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a Hazelcast config file
Properties properties = new Properties();
properties.setProperty( HazelcastCachingProvider.HAZELCAST_CONFIG_LOCATION,
    &quot;classpath://my-configs/scoped-hazelcast.xml&quot; );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p>Here is an example using <code>HazelcastCachingProvider::propertiesByLocation</code> helper method.</p>
<pre><code class="lang-java">CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a Hazelcast config file
String configFile = &quot;classpath://my-configs/scoped-hazelcast.xml&quot;;
Properties properties = HazelcastCachingProvider
    .propertiesByLocation( configFile );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p>The retrieved <code>CacheManager</code> is scoped to use the <code>HazelcastInstance</code> that was just created and was configured using the given XML
configuration file.</p>
<p>Available protocols for config file URL include <code>classpath://</code> to point to a classpath location, <code>file://</code> to point to a filesystem
location, <code>http://</code> an <code>https://</code> for remote web locations. In addition, everything that does not specify a protocol is recognized
as a placeholder that can be configured using a system property.</p>
<pre><code class="lang-java">String configFile = &quot;my-placeholder&quot;;
Properties properties = HazelcastCachingProvider
    .propertiesByLocation( configFile );
</code></pre>
<p>You can set this on the command line.</p>
<pre><code class="lang-plain">-Dmy-placeholder=classpath://my-configs/scoped-hazelcast.xml
</code></pre>
<p>You should consider the following rules about the Hazelcast instance name when you specify the configuration file location using <code>HazelcastCachingProvider#HAZELCAST_CONFIG_LOCATION</code> (which resolves to <code>hazelcast.config.location</code>):</p>
<ul>
<li>If you also specified the <code>HazelcastCachingProvider#HAZELCAST_INSTANCE_NAME</code> (which resolves to <code>hazelcast.instance.name</code>) property, this property is used as the instance name even though you configured the instance name in the configuration file.</li>
<li>If you do not specify <code>HazelcastCachingProvider#HAZELCAST_INSTANCE_NAME</code> but you configure the instance name in the configuration file using the element <code>&lt;instance-name&gt;</code>, this element&#39;s value will be used as the instance name.</li>
<li>If you do not specify an instance name via property or in the configuration file, the URL of the configuration file location is used as the instance name.</li>
</ul>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>No check is performed to prevent creating multiple <code>CacheManager</code>s with the same cluster
configuration on different configuration files. If the same cluster is referred from different configuration files, multiple
cluster members or clients are created.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The configuration file location will not be a part of the resulting identity of the
<code>CacheManager</code>. An attempt to create a <code>CacheManager</code> with a different set of properties but an already used name will result in
undefined behavior.</em>
<br></br></p>
<a name="binding-to-a-named-instance"></a><h4 id="binding-to-a-named-instance">Binding to a Named Instance</h4>
<p>You can bind <code>CacheManager</code> to an existing and named <code>HazelcastInstance</code> instance. If the <code>instanceName</code> is specified in <code>com.hazelcast.config.Config</code>, it can be used directly by passing it to <code>CachingProvider</code> implementation. Otherwise (<code>instanceName</code> not set or instance is a client instance) you must get the instance name from the <code>HazelcastInstance</code> instance via the <code>String getName()</code> method to pass the <code>CachingProvider</code> implementation. Please note that <code>instanceName</code> is not configurable for the client side <code>HazelcastInstance</code> instance and is auto-generated by using group name (if it is specified). In general, <code>String getName()</code> method over <code>HazelcastInstance</code> is safer and the preferable way to get the name of the instance. Multiple <code>CacheManager</code>s created using an equal <code>java.net.URI</code> will share the same <code>HazelcastInstance</code>.</p>
<p>A named scope is applied nearly the same way as the configuration scope: pass in the instance name using the <code>HazelcastCachingProvider#HAZELCAST_INSTANCE_NAME</code> (which resolves to <code>hazelcast.instance.name</code>) property as a mapping inside a <code>java.util.Properties</code> instance to the <code>CachingProvider#getCacheManager(uri, classLoader, properties)</code> call.</p>
<p>Here is an example of Named Instance Scope with specified name.</p>
<pre><code class="lang-java">Config config = new Config();
config.setInstanceName( &quot;my-named-hazelcast-instance&quot; );
// Create a named HazelcastInstance
Hazelcast.newHazelcastInstance( config );

CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a named HazelcastInstance
Properties properties = new Properties();
properties.setProperty( HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME,
     &quot;my-named-hazelcast-instance&quot; );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p>Here is an example of Named Instance Scope with auto-generated name.</p>
<pre><code class="lang-java">Config config = new Config();
// Create a auto-generated named HazelcastInstance
HazelcastInstance instance = Hazelcast.newHazelcastInstance( config );
String instanceName = instance.getName();

CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a named HazelcastInstance
Properties properties = new Properties();
properties.setProperty( HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME, 
     instanceName );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p>Here is an example of Named Instance Scope with auto-generated name on client instance.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientNetworkConfig networkConfig = clientConfig.getNetworkConfig();
networkConfig.addAddress(&quot;127.0.0.1&quot;, &quot;127.0.0.2&quot;);

// Create a client side HazelcastInstance
HazelcastInstance instance = HazelcastClient.newHazelcastClient( clientConfig );
String instanceName = instance.getName();

CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a named HazelcastInstance
Properties properties = new Properties();
properties.setProperty( HazelcastCachingProvider.HAZELCAST_INSTANCE_NAME, 
     instanceName );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p>Here is an example using <code>HazelcastCachingProvider::propertiesByInstanceName</code> method.</p>
<pre><code class="lang-java">Config config = new Config();
config.setInstanceName( &quot;my-named-hazelcast-instance&quot; );
// Create a named HazelcastInstance
Hazelcast.newHazelcastInstance( config );

CachingProvider cachingProvider = Caching.getCachingProvider();

// Create Properties instance pointing to a named HazelcastInstance
Properties properties = HazelcastCachingProvider
    .propertiesByInstanceName( &quot;my-named-hazelcast-instance&quot; );

URI cacheManagerName = new URI( &quot;my-cache-manager&quot; );
CacheManager cacheManager = cachingProvider
    .getCacheManager( cacheManagerName, null, properties );
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The <code>instanceName</code> will not be a part of the resulting identity of the <code>CacheManager</code>.
An attempt to create a <code>CacheManager</code> with a different set of properties but an already used name will result in undefined behavior.</em>
<br></br></p>
<a name="namespacing"></a><h3 id="namespacing">Namespacing</h3>
<p>The <code>java.net.URI</code>s that don&#39;t use the above-mentioned Hazelcast-specific schemes are recognized as namespacing. Those
<code>CacheManager</code>s share the same underlying default <code>HazelcastInstance</code> created (or set) by the <code>CachingProvider</code>, but they cache with the
same names and different namespaces on the <code>CacheManager</code> level, and therefore they won&#39;t share the same data. This is useful where multiple
applications might share the same Hazelcast JCache implementation (e.g., on application or OSGi servers) but are developed by
independent teams. To prevent interfering on caches using the same name, every application can use its own namespace when
retrieving the <code>CacheManager</code>.</p>
<p>Here is an example of using namespacing.</p>
<pre><code class="lang-java">CachingProvider cachingProvider = Caching.getCachingProvider();

URI nsApp1 = new URI( &quot;application-1&quot; );
CacheManager cacheManagerApp1 = cachingProvider.getCacheManager( nsApp1, null );

URI nsApp2 = new URI( &quot;application-2&quot; );
CacheManager cacheManagerApp2 = cachingProvider.getCacheManager( nsApp2, null );
</code></pre>
<p>That way both applications share the same <code>HazelcastInstance</code> instance but not the same caches.</p>

<a name="retrieving-an-icache-instance"></a><h3 id="retrieving-an-icache-instance">Retrieving an ICache Instance</h3>
<p>Besides <a href="#scoping-to-join-clusters">Scoping to Join Clusters</a> and <a href="#namespacing">Namespacing</a>, which are implemented using the URI feature of the
specification, all other extended operations are required to retrieve the <code>com.hazelcast.cache.ICache</code> interface instance from
the JCache <code>javax.cache.Cache</code> instance. For Hazelcast, both interfaces are implemented on the same object instance. It
is recommended that you stay with the specification method to retrieve the <code>ICache</code> version, since <code>ICache</code> might be subject to change without notification.</p>
<p>To retrieve or unwrap the <code>ICache</code> instance, you can execute the following code example:</p>
<pre><code class="lang-java">CachingProvider cachingProvider = Caching.getCachingProvider();
CacheManager cacheManager = cachingProvider.getCacheManager();
Cache&lt;Object, Object&gt; cache = cacheManager.getCache( ... );

ICache&lt;Object, Object&gt; unwrappedCache = cache.unwrap( ICache.class );
</code></pre>
<p>After unwrapping the <code>Cache</code> instance into an <code>ICache</code> instance, you have access to all of the following operations, e.g.,
<a href="#icache-async-methods">ICache Async Methods</a> and <a href="#icache-convenience-methods">ICache Convenience Methods</a>.</p>

<a name="icache-configuration"></a><h3 id="icache-configuration">ICache Configuration</h3>
<p>As mentioned in the <a href="#jcache-declarative-configuration">JCache Declarative Configuration section</a>, the Hazelcast ICache extension offers
additional configuration properties over the default JCache configuration. These additional properties include internal storage format, backup counts, eviction policy and quorum reference.</p>
<p>The declarative configuration for ICache is a superset of the previously discussed JCache configuration:</p>
<pre><code class="lang-xml">&lt;cache&gt;
  &lt;!-- ... default cache configuration goes here ... --&gt;
  &lt;backup-count&gt;1&lt;/backup-count&gt;
  &lt;async-backup-count&gt;1&lt;/async-backup-count&gt;
  &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
  &lt;eviction size=&quot;10000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LRU&quot; /&gt;
  &lt;partition-lost-listeners&gt;
     &lt;partition-lost-listener&gt;CachePartitionLostListenerImpl&lt;/partition-lost-listener&gt;
 &lt;/partition-lost-listeners&gt;
 &lt;quorum-ref&gt;quorum-name&lt;/quorum-ref&gt;
 &lt;disable-per-entry-invalidation-events&gt;true&lt;/disable-per-entry-invalidation-events&gt;
&lt;/cache&gt;
</code></pre>
<ul>
<li><code>backup-count</code>: Number of synchronous backups. Those backups are executed before the mutating cache operation is finished. The mutating operation is blocked. <code>backup-count</code> default value is 1.</li>
<li><code>async-backup-count</code>: Number of asynchronous backups. Those backups are executed asynchronously so the mutating operation is not blocked and it will be done immediately. <code>async-backup-count</code> default value is 0.  </li>
<li><code>in-memory-format</code>: Internal storage format. For more information, please see the <a href="#setting-in-memory-format">In Memory Format section</a>. Default is <code>BINARY</code>.</li>
<li><code>eviction</code>: Defines the used eviction strategies and sizes for the cache. For more information on eviction, please see the <a href="#jcache-eviction">JCache Eviction</a>.<ul>
<li><code>size</code>: Maximum number of records or maximum size in bytes depending on the <code>max-size-policy</code> property. Size can be any integer between <code>0</code> and <code>Integer.MAX_VALUE</code>. Default max-size-policy is <code>ENTRY_COUNT</code> and default size is <code>10.000</code>.</li>
<li><code>max-size-policy</code>: Maximum size. If maximum size is reached, the cache is evicted based on the eviction policy. Default max-size-policy is <code>ENTRY_COUNT</code> and default size is <code>10.000</code>. The following eviction policies are available:<ul>
<li><code>ENTRY_COUNT</code>: Maximum number of cache entries in the cache. <strong>Available on heap based cache record store only.</strong></li>
<li><code>USED_NATIVE_MEMORY_SIZE</code>: Maximum used native memory size in megabytes per cache for each Hazelcast instance. <strong>Available on High-Density Memory cache record store only.</strong></li>
<li><code>USED_NATIVE_MEMORY_PERCENTAGE</code>: Maximum used native memory size percentage per cache for each Hazelcast instance. <strong>Available on High-Density Memory cache record store only.</strong></li>
<li><code>FREE_NATIVE_MEMORY_SIZE</code>: Minimum free native memory size in megabytes for each Hazelcast instance. <strong>Available on High-Density Memory cache record store only.</strong></li>
<li><code>FREE_NATIVE_MEMORY_PERCENTAGE</code>: Minimum free native memory size percentage for each Hazelcast instance. <strong>Available on High-Density Memory cache record store only.</strong></li>
</ul>
</li>
<li><code>eviction-policy</code>: Eviction policy that compares values to find the best matching eviction candidate. Default is <code>LRU</code>.<ul>
<li><code>LRU</code>: Less Recently Used - finds the best eviction candidate based on the lastAccessTime.</li>
<li><code>LFU</code>: Less Frequently Used - finds the best eviction candidate based on the number of hits.</li>
</ul>
</li>
</ul>
</li>
<li><code>partition-lost-listeners</code> : Defines listeners for dispatching partition lost events for the cache. For more information, please see the <a href="#icache-partition-lost-listener">ICache Partition Lost Listener section</a>.</li>
<li><code>quorum-ref</code> : Name of quorum configuration that you want this cache to use.</li>
<li><code>disable-per-entry-invalidation-events</code> : Disables invalidation events for each entry; but full-flush invalidation events are still enabled. Full-flush invalidation means the invalidation of events for all entries when <code>clear</code> is called. The default value is <code>false</code>.</li>
</ul>
<p>Since <code>javax.cache.configuration.MutableConfiguration</code> misses the above additional configuration properties, Hazelcast ICache extension
provides an extended configuration class called <code>com.hazelcast.config.CacheConfig</code>. This class is an implementation of <code>javax.cache.configuration.CompleteConfiguration</code> and all the properties shown above can be configured
using its corresponding setter methods.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>At the client side, ICache can be configured only programmatically.</em>
<br></br></p>

<a name="icache-async-methods"></a><h3 id="icache-async-methods">ICache Async Methods</h3>
<p>As another addition of Hazelcast ICache over the normal JCache specification, Hazelcast provides asynchronous versions of almost
all methods, returning a <code>com.hazelcast.core.ICompletableFuture</code>. By using these methods and the returned future objects, you can use JCache in a reactive way by registering zero or more callbacks on the future to prevent blocking the current thread.</p>
<p>The asynchronous versions of the methods append the phrase <code>Async</code> to the method name. The example code below uses the method <code>putAsync()</code>.</p>
<pre><code class="lang-java">ICache&lt;Integer, String&gt; unwrappedCache = cache.unwrap( ICache.class );
ICompletableFuture&lt;String&gt; future = unwrappedCache.putAsync( 1, &quot;value&quot; );
future.andThen( new ExecutionCallback&lt;String&gt;() {
  public void onResponse( String response ) {
    System.out.println( &quot;Previous value: &quot; + response );
  }

  public void onFailure( Throwable t ) {
    t.printStackTrace();
  }
} );
</code></pre>
<p>Following methods
are available in asynchronous versions:</p>
<ul>
<li><code>get(key)</code>:<ul>
<li><code>getAsync(key)</code></li>
<li><code>getAsync(key, expiryPolicy)</code></li>
</ul>
</li>
<li><code>put(key, value)</code>:<ul>
<li><code>putAsync(key, value)</code></li>
<li><code>putAsync(key, value, expiryPolicy)</code></li>
</ul>
</li>
<li><code>putIfAbsent(key, value)</code>:<ul>
<li><code>putIfAbsentAsync(key, value)</code></li>
<li><code>putIfAbsentAsync(key, value, expiryPolicy)</code></li>
</ul>
</li>
<li><code>getAndPut(key, value)</code>:<ul>
<li><code>getAndPutAsync(key, value)</code></li>
<li><code>getAndPutAsync(key, value, expiryPolicy)</code></li>
</ul>
</li>
<li><code>remove(key)</code>:<ul>
<li><code>removeAsync(key)</code></li>
</ul>
</li>
<li><code>remove(key, value)</code>:<ul>
<li><code>removeAsync(key, value)</code></li>
</ul>
</li>
<li><code>getAndRemove(key)</code>:<ul>
<li><code>getAndRemoveAsync(key)</code></li>
</ul>
</li>
<li><code>replace(key, value)</code>:<ul>
<li><code>replaceAsync(key, value)</code></li>
<li><code>replaceAsync(key, value, expiryPolicy)</code></li>
</ul>
</li>
<li><code>replace(key, oldValue, newValue)</code>:<ul>
<li><code>replaceAsync(key, oldValue, newValue)</code></li>
<li><code>replaceAsync(key, oldValue, newValue, expiryPolicy)</code></li>
</ul>
</li>
<li><code>getAndReplace(key, value)</code>:<ul>
<li><code>getAndReplaceAsync(key, value)</code></li>
<li><code>getAndReplaceAsync(key, value, expiryPolicy)</code></li>
</ul>
</li>
</ul>
<p>The methods with a given <code>javax.cache.expiry.ExpiryPolicy</code> are further discussed in the
<a href="#defining-a-custom-expirypolicy">Defining a Custom ExpiryPolicy</a>.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Asynchronous versions of the methods are not compatible with synchronous events.</em>
<br></br></p>

<a name="defining-a-custom-expirypolicy"></a><h3 id="defining-a-custom-expirypolicy">Defining a Custom ExpiryPolicy</h3>
<p>The JCache specification has an option to configure a single <code>ExpiryPolicy</code> per cache. Hazelcast ICache extension
offers the possibility to define a custom <code>ExpiryPolicy</code> per key by providing a set of method overloads with an <code>expirePolicy</code>
parameter, as in the list of asynchronous methods in the <a href="#icache-async-methods">Async Methods section</a>. This means that you can pass custom expiry policies to a cache operation.</p>
<p>Here is how an <code>ExpirePolicy</code> is set on JCache configuration:</p>
<pre><code class="lang-java">CompleteConfiguration&lt;String, String&gt; config =
    new MutableConfiguration&lt;String, String&gt;()
        setExpiryPolicyFactory(
            AccessedExpiryPolicy.factoryOf( Duration.ONE_MINUTE )
        );
</code></pre>
<p>To pass a custom <code>ExpirePolicy</code>, a set of overloads is provided. You can use them as shown in the following code example.</p>
<pre><code class="lang-java">ICache&lt;Integer, String&gt; unwrappedCache = cache.unwrap( ICache.class );
unwrappedCache.put( 1, &quot;value&quot;, new AccessedExpiryPolicy( Duration.ONE_DAY ) );
</code></pre>
<p>The <code>ExpirePolicy</code> instance can be pre-created, cached, and re-used, but only for each cache instance. This is because <code>ExpirePolicy</code>
implementations can be marked as <code>java.io.Closeable</code>. The following list shows the provided method overloads over <code>javax.cache.Cache</code>
by <code>com.hazelcast.cache.ICache</code> featuring the <code>ExpiryPolicy</code> parameter:</p>
<ul>
<li><code>get(key)</code>:<ul>
<li><code>get(key, expiryPolicy)</code></li>
</ul>
</li>
<li><code>getAll(keys)</code>:<ul>
<li><code>getAll(keys, expirePolicy)</code></li>
</ul>
</li>
<li><code>put(key, value)</code>:<ul>
<li><code>put(key, value, expirePolicy)</code></li>
</ul>
</li>
<li><code>getAndPut(key, value)</code>:<ul>
<li><code>getAndPut(key, value, expirePolicy)</code></li>
</ul>
</li>
<li><code>putAll(map)</code>:<ul>
<li><code>putAll(map, expirePolicy)</code></li>
</ul>
</li>
<li><code>putIfAbsent(key, value)</code>:<ul>
<li><code>putIfAbsent(key, value, expirePolicy)</code></li>
</ul>
</li>
<li><code>replace(key, value)</code>:<ul>
<li><code>replace(key, value, expirePolicy)</code></li>
</ul>
</li>
<li><code>replace(key, oldValue, newValue)</code>:<ul>
<li><code>replace(key, oldValue, newValue, expirePolicy)</code></li>
</ul>
</li>
<li><code>getAndReplace(key, value)</code>:<ul>
<li><code>getAndReplace(key, value, expirePolicy)</code></li>
</ul>
</li>
</ul>
<p>Asynchronous method overloads are not listed here. Please see <a href="#icache-async-methods">ICache Async Methods</a> for the list of asynchronous method overloads.</p>

<a name="jcache-eviction"></a><h3 id="jcache-eviction">JCache Eviction</h3>
<p>Caches are generally not expected to grow to an infinite size. Implementing an <a href="#expirepolicy">expiry policy</a> is one way you can
prevent infinite growth, but sometimes it is hard to define a meaningful expiration timeout. Therefore, Hazelcast JCache provides the eviction feature. Eviction offers the possibility of removing entries based on the cache size or amount of used memory
(Hazelcast Enterprise Only) and not based on timeouts.</p>
<a name="eviction-and-runtime"></a><h4 id="eviction-and-runtime">Eviction and Runtime</h4>
<p>Since a cache is designed for high throughput and fast reads, Hazelcast put a lot of effort into designing the eviction system to be as
predictable as possible. All built-in implementations provide an amortized O(1) runtime. The default operation runtime is
rendered as O(1), but it can be faster than the normal runtime cost if the algorithm finds an expired entry while sampling.</p>
<a name="cache-types"></a><h4 id="cache-types">Cache Types</h4>
<p>Most importantly, typical production systems have two common types of caches:</p>
<ul>
<li><strong>Reference Caches</strong>: Caches for reference data are normally small and are used to speed up the de-referencing as a lookup table. Those
caches are commonly tend to be small and contain a previously known, fixed number of elements (e.g., states of the USA or
abbreviations of elements).</li>
<li><strong>Active DataSet Caches</strong>:  The other type of caches normally caches an active data set. These caches run to their maximum
size and evict the oldest or not frequently used entries to keep in memory bounds. They sit in front of a database or HTML
generators to cache the latest requested data.</li>
</ul>
<p>Hazelcast JCache eviction supports both types of caches using a slightly different approach based on the configured maximum size
of the cache. For detailed information, please see the <a href="#eviction-algorithm">Eviction Algorithm section</a>.</p>
<a name="configuring-eviction-policies"></a><h4 id="configuring-eviction-policies">Configuring Eviction Policies</h4>
<p>Hazelcast JCache provides two commonly known eviction policies, LRU and LFU, but loosens the rules for predictable runtime
behavior. LRU, normally recognized as <code>Least Recently Used</code>, is implemented as <code>Less Recently Used</code>, and LFU known as <code>Least Frequently Used</code> is implemented as
<code>Less Frequently Used</code>. The details about this difference are explained in the
<a href="#eviction-algorithm">Eviction Algorithm section</a>.</p>
<p>Eviction Policies are configured by providing the corresponding abbreviation to the configuration as shown in the <a href="#icache-configuration">ICache Configuration section</a>. As already mentioned, two built-in policies are available:</p>
<p>To configure the use of the LRU (Less Recently Used) policy:</p>
<pre><code class="lang-xml">&lt;eviction size=&quot;10000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LRU&quot; /&gt;
</code></pre>
<p>And to configure the use of the LFU (Less Frequently Used) policy:</p>
<pre><code class="lang-xml">&lt;eviction size=&quot;10000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LFU&quot; /&gt;
</code></pre>
<p>The default eviction policy is LRU. Therefore, Hazelcast JCache does not offer the possibility of performing no eviction.</p>
<a name="custom-eviction-policies"></a><h5 id="custom-eviction-policies">Custom Eviction Policies</h5>
<p>Besides out of the box eviction policies LFU and LRU, you can also specify your custom eviction policies 
through the eviction configuration either programmatically or declaratively.</p>
<p>You can provide your <code>com.hazelcast.cache.CacheEvictionPolicyComparator</code> implementation to compare <code>com.hazelcast.cache.CacheEntryView</code>s. Supplied <code>CacheEvictionPolicyComparator</code> is used to compare cache entry views to select the one with higher priority to evict.</p>
<p>Here is an example for custom eviction policy comparator implementation for JCache:</p>
<pre><code class="lang-java">public class MyCacheEvictionPolicyComparator
        extends CacheEvictionPolicyComparator&lt;Long, String&gt; {

    @Override
    public int compare(CacheEntryView&lt;Long, String&gt; e1, CacheEntryView&lt;Long, String&gt; e2) {
        long id1 = e1.getKey();
        long id2 = e2.getKey();
        if (id1 &gt; id2) {
            return FIRST_ENTRY_HAS_HIGHER_PRIORITY_TO_BE_EVICTED; // -1
        } else if (id1 &lt; id2) {
            return SECOND_ENTRY_HAS_HIGHER_PRIORITY_TO_BE_EVICTED; // +1
        } else {
            return BOTH_OF_ENTRIES_HAVE_SAME_PRIORITY_TO_BE_EVICTED; // 0
        }
    }

}
</code></pre>
<a name="configuration"></a><h6 id="configuration">Configuration</h6>
<p>Custom eviction policy comparator can be specified through the eviction configuration 
by giving the full class name of the <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) 
implementation or by specifying its instance itself.</p>
<p><strong>Programmatic:</strong></p>
<p>You can specify the full class name of custom <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) implementation 
through <code>EvictionConfig</code>. This approach is useful when eviction configuration is specified at the client side 
and custom <code>EvictionPolicyComparator</code> implementation class itself does not exist at the client but at server side.</p>
<pre><code class="lang-java">CacheConfig cacheConfig = new CacheConfig();
...
EvictionConfig evictionConfig = 
    new EvictionConfig(50000, 
                       MaxSizePolicy.ENTRY_COUNT, 
                       &quot;com.mycompany.MyEvictionPolicyComparator&quot;);
cacheConfig.setEvictionConfig(evictionConfig);
</code></pre>
<p>You can specify the custom <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) instance itself directly through <code>EvictionConfig</code>. </p>
<pre><code class="lang-java">CacheConfig cacheConfig = new CacheConfig();
...
EvictionConfig evictionConfig = 
    new EvictionConfig(50000, 
                       MaxSizePolicy.ENTRY_COUNT, 
                       new MyEvictionPolicyComparator());
cacheConfig.setEvictionConfig(evictionConfig);
</code></pre>
<p><strong>Declarative:</strong></p>
<p>You can specify the full class name of custom <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) implementation 
in the <code>&lt;eviction&gt;</code> tag through <code>comparator-class-name</code> attribute in Hazelcast configuration XML file.</p>
<pre><code class="lang-xml">&lt;cache name=&quot;cacheWithCustomEvictionPolicyComparator&quot;&gt; 
    &lt;eviction size=&quot;50000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; comparator-class-name=&quot;com.mycompany.MyEvictionPolicyComparator&quot;/&gt; 
&lt;/cache&gt;
</code></pre>
<p><strong>Declarative for Spring:</strong></p>
<p>You can specify the full class name of custom <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) implementation 
in the <code>&lt;eviction&gt;</code> tag through <code>comparator-class-name</code> attribute in Hazelcast <em>Spring</em> configuration XML file.</p>
<pre><code class="lang-xml">&lt;hz:cache name=&quot;cacheWithCustomEvictionPolicyComparator&quot;&gt;
    &lt;hz:eviction size=&quot;50000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; comparator-class-name=&quot;com.mycompany.MyEvictionPolicyComparator&quot;/&gt;
&lt;/hz:cache&gt;
</code></pre>
<p>You can specify the custom <code>EvictionPolicyComparator</code> (<code>CacheEvictionPolicyComparator</code> for JCache and its near cache) bean in the <code>&lt;eviction&gt;</code> tag 
by referencing through <code>comparator-bean</code> attribute in Hazelcast <em>Spring</em> configuration XML file</p>
<pre><code class="lang-xml">&lt;hz:cache name=&quot;cacheWithCustomEvictionPolicyComparator&quot;&gt;
    &lt;hz:eviction size=&quot;50000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; comparator-bean=&quot;myEvictionPolicyComparatorBean&quot;/&gt;
&lt;/hz:cache&gt;
</code></pre>
<a name="eviction-strategy"></a><h4 id="eviction-strategy">Eviction Strategy</h4>
<p>Eviction strategies implement the logic of selecting one or more eviction candidates from the underlying storage implementation and
passing them to the eviction policies. Hazelcast JCache provides an amortized O(1) cost implementation for this strategy to select a
fixed number of samples from the current partition that it is executed against.</p>
<p>The default implementation is <code>com.hazelcast.cache.impl.eviction.impl.strategy.sampling.SamplingBasedEvictionStrategy</code> which, as
mentioned, samples 15 random elements. A detailed description of the algorithm will be explained in the next section.</p>
<a name="eviction-algorithm"></a><h4 id="eviction-algorithm">Eviction Algorithm</h4>
<p>The Hazelcast JCache eviction algorithm is specially designed for the use case of high performance caches and with predictability
in mind. The built-in implementations provide an amortized O(1) runtime and therefore provide a highly predictable runtime behavior
which does not rely on any kind of background threads to handle the eviction. Therefore, the algorithm takes some assumptions into
account to prevent network operations and concurrent accesses.</p>
<p>As an explanation of how the algorithm works, let&#39;s examine the following flowchart step by step.</p>
<p><img src="images/Flowchart.png" alt="Hazelcast JCache Eviction Algorithm"></p>
<ol>
<li>A new cache is created. Without any special settings, the eviction is configured to kick in when the <strong>cache</strong> exceeds 10.000
elements and an LRU (Less Recently Used) policy is set up.</li>
<li>The user puts in a new entry (e.g., a key-value pair).</li>
<li>For every put, the eviction strategy evaluates the current cache size and decides if an eviction is necessary or not. If not, the entry is stored in step 10.</li>
<li>If eviction is required, a new sampling is started. The built-in sampler is implemented as an lazy iterator.</li>
<li>The sampling algorithm selects a random sample from the underlying data storage.</li>
<li>The eviction strategy tests whether the sampled entry is already expired (lazy expiration). If expired, the sampling stops and the entry is removed in step 9.</li>
<li>If not yet expired, the entry (eviction candidate) is compared to the last best matching candidate (based on the eviction policy) and the new best matching candidate is remembered.</li>
<li>The sampling is repeated 15 times and then the best matching eviction candidate is returned to the eviction strategy.</li>
<li>The expired or best matching eviction candidate is removed from the underlying data storage.</li>
<li>The new put entry is stored.</li>
<li>The put operation returns to the user.</li>
</ol>
<p>As seen in the flowchart, the general eviction operation is easy. As long as the cache does not reach its maximum capacity,
or you execute updates (put/replace), no eviction is executed.</p>
<p>To prevent network operations and concurrent access, as mentioned earlier, the cache size is estimated based on the size of the
currently handled partition. Due to the imbalanced partitions, the single partitions might start to evict
earlier than the other partitions.</p>
<p>As mentioned in the <a href="#cache-types">Cache Types section</a>, typically two types of caches are found in the production systems. For small caches,
referred to as <em>Reference Caches</em>, the eviction algorithm has a special set of rules depending on the maximum configured cache
size. Please see the <a href="#reference-caches">Reference Caches section</a> for details. The other type of cache is referred to as an <em>Active DataSet Cache</em>,
which in most cases makes heavy use of the eviction to keep the most active data set in the memory. Those kinds of caches use a very
simple but efficient way to estimate the cluster-wide cache size.</p>
<p>All of the following calculations have a well known set of fixed variables:</p>
<ul>
<li><code>GlobalCapacity</code>: User defined maximum cache size (cluster-wide).</li>
<li><code>PartitionCount</code>: Number of partitions in the cluster (defaults to 271).</li>
<li><code>BalancedPartitionSize</code>: Number of elements in a balanced partition state, <code>BalancedPartitionSize := GlobalCapacity / PartitionCount</code>.</li>
<li><code>Deviation</code>: An approximated standard deviation (tests proofed it to be pretty near), <code>Deviation := sqrt(BalancedPartitionSize)</code>.</li>
</ul>
<a name="reference-caches"></a><h5 id="reference-caches">Reference Caches</h5>
<p>A Reference Cache is typically small and the number of elements to store in the reference caches is normally 
known prior to creating the cache. Typical examples of reference caches are lookup tables for abbreviations or the states of a
country. They tend to have a fixed but small element number and the eviction is an unlikely event, and rather undesirable behavior.</p>
<p>Since an imbalanced partition is a worse problem in small and mid-sized caches than in caches with millions of entries, the normal
estimation rule (as discussed in a bit) is not applied to these kinds of caches. To prevent unwanted eviction on the small and
mid-sized caches, Hazelcast implements a special set of rules to estimate the cluster size.</p>
<p>To adjust the imbalance of partitions as found in the typical runtime, the actual calculated maximum cache size (known as the eviction
threshold) is slightly higher than the user defined size. That means more elements can be stored into the cache
than expected by the user. This needs to be taken into account especially for large objects, since those can easily exceed the
expected memory consumption!</p>
<p><strong>Small caches:</strong></p>
<p>If a cache is configured with no more than <code>4.000</code> elements, this cache is considered to be a small cache. The actual partition
size is derived from the number of elements (<code>GlobalCapacity</code>) and the deviation using the following formula:</p>
<pre><code class="lang-plain">MaxPartitionSize := Deviation * 5 + BalancedPartitionSize
</code></pre>
<p>This formula ends up with big partition sizes which, summed up, exceed the expected maximum cache size (set by the user). 
Since the small caches typically have a well known maximum number of elements, this is not a big
issue. Only if the small caches are used for a use case other than as a reference cache, this needs to be taken into account.</p>
<p><strong>Mid-sized caches</strong></p>
<p>A mid-sized cache is defined as a cache with a maximum number of elements that is bigger than <code>4.000</code> but not bigger than
<code>1.000.000</code> elements. The calculation of mid-sized caches is similar to that of the small caches but with a different
multiplier. To calculate the maximum number of elements per partition, the following formula is used:</p>
<pre><code class="lang-plain">MaxPartitionSize := Deviation * 3 + BalancedPartitionSize
</code></pre>
<a name="active-dataset-caches"></a><h5 id="active-dataset-caches">Active DataSet Caches</h5>
<p>For large caches, where the maximum cache size is bigger than <code>1.000.000</code> elements, there is no additional calculation needed. The maximum
partition size is considered to be equal to <code>BalancedPartitionSize</code> since statistically big partitions are expected to almost
balance themselves. Therefore, the formula is as easy as the following:</p>
<pre><code class="lang-plain">MaxPartitionSize := BalancedPartitionSize
</code></pre>
<a name="cache-size-estimation"></a><h5 id="cache-size-estimation">Cache Size Estimation</h5>
<p>As mentioned earlier, Hazelcast JCache provides an estimation algorithm to prevent cluster-wide network operations, concurrent
access to other partitions and background tasks. It also offers a highly predictable operation runtime when the eviction is necessary.</p>
<p>The estimation algorithm is based on the previously calculated maximum partition size (please see the <a href="#reference-caches">Reference Caches section</a> and <a href="#active-dataset-caches">Active DataSet Caches section</a>) and is calculated
against the current partition only.</p>
<p>The algorithm to reckon the number of stored entries in the cache (cluster-wide) and decide if the eviction is necessary is shown in the
following pseudo-code example:</p>
<pre><code class="lang-plain">RequiresEviction[Boolean] := CurrentPartitionSize &gt;= MaxPartitionSize
</code></pre>

<a name="jcache-near-cache"></a><h3 id="jcache-near-cache">JCache Near Cache</h3>
<p>Cache entries in Hazelcast are stored as partitioned across the cluster. 
When you try to read a record with the key <code>k</code>, if the current member is not the owner of that key (i.e. not the owner of partition that the key belongs to), 
Hazelcast sends a remote operation to the owner member. Each remote operation means lots of network trips. 
If your cache is used for mostly read operations, it is advised to use a near cache storage in front of the cache itself to read cache records faster and consume less network traffic.
<br><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Near cache for JCache is only available for clients, NOT members.</em>
<br><br></p>
<p>However, using near cache comes with trade-offs in some cases:</p>
<ul>
<li>There will be extra memory consumption for storing near cache records at local.</li>
<li>If invalidation is enabled and entries are updated frequently, there will be many invalidation events across the cluster.</li>
<li>Near cache does not give strong consistency but gives eventual consistency guarantees. It is possible to read stale data.</li>
</ul>
<a name="configuring-invalidation-event-sending"></a><h4 id="configuring-invalidation-event-sending">Configuring Invalidation Event Sending</h4>
<p>Invalidation is the process of removing an entry from the near cache since the entry is not valid anymore (its value is updated or it is removed from actual cache). Near cache invalidation happens asynchronously at the cluster level, but synchronously in real-time at the current member. This means when an entry is updated (explicitly or via entry processor) or removed (deleted explicitly or via entry processor, evicted, expired), it is invalidated from all near caches asynchronously within the whole cluster but updated/removed at/from the current member synchronously. Generally, whenever the state of an entry changes in the record store by updating its value or removing it, the invalidation event is sent for that entry.</p>
<p>Invalidation events can be sent either individually or in batches. If there are lots of mutating operations such as put/remove on the cache, sending the events in batches is advised. This reduces the network traffic and keeps the eventing system less busy. </p>
<p>You can use the following system properties to configure the sending of invalidation events in batches:</p>
<ul>
<li><code>hazelcast.cache.invalidation.batch.enabled</code>: Specifies whether the cache invalidation event batch sending is enabled or not. The default value is <code>true</code>.</li>
<li><code>hazelcast.cache.invalidation.batch.size</code>: Maximum number of cache invalidation events to be drained and sent to the event listeners in a batch. The default value is <code>100</code>.</li>
<li><code>hazelcast.cache.invalidation.batchfrequency.seconds</code>: Cache invalidation event batch sending frequency in seconds. When event size does not reach to <code>hazelcast.cache.invalidation.batch.size</code> in the given time period, those events are gathered into a batch and sent to the target. The default value is <code>10</code> seconds.</li>
</ul>
<p>So if there are many clients or many mutating operations, batching should remain enabled and the batch size should be configured with the <code>hazelcast.cache.invalidation.batch.size</code> system property to a suitable value.</p>
<a name="jcache-near-cache-expiration"></a><h4 id="jcache-near-cache-expiration">JCache Near Cache Expiration</h4>
<p>Expiration means the eviction of expired records. A record is expired: </p>
<ul>
<li>if it is not touched (accessed/read) for <code>&lt;max-idle-seconds&gt;</code>,</li>
<li><code>&lt;time-to-live-seconds&gt;</code> passed since it is put to near-cache.</li>
</ul>
<p>Expiration is performed in two cases:</p>
<ul>
<li>When a record is accessed, it is checked about if it is expired or not. If it is expired, it is evicted and returns <code>null</code> to caller.</li>
<li>In the background, there is an expiration task that periodically (currently 5 seconds) scans records and evicts the expired records.</li>
</ul>
<a name="configuring-jcache-near-cache-eviction"></a><h4 id="configuring-jcache-near-cache-eviction">Configuring JCache Near Cache Eviction</h4>
<p>In the scope of near cache, eviction means evicting (clearing) the entries selected according to the given <code>eviction-policy</code> when the specified <code>max-size-policy</code> has been reached. Eviction is handled with <code>max-size policy</code> and <code>eviction-policy</code> elements. Please see <a href="#configuring-jcache-near-cache">Configuring JCache Near Cache</a>.</p>
<a name="max-size-policy"></a><h5 id="-max-size-policy-"><code>max-size-policy</code></h5>
<p>This element defines the state when the near cache is full and determines whether the eviction should be triggered. The following policies for maximum cache size are supported by the near cache eviction:</p>
<ul>
<li><strong>ENTRY_COUNT:</strong> Maximum size based on the entry count in the near cache. Available only for <code>BINARY</code> and <code>OBJECT</code> in-memory formats.</li>
<li><strong>USED_NATIVE_MEMORY_SIZE:</strong> Maximum used native memory size of the specified near cache in MB to trigger the eviction. If the used native memory size exceeds this threshold, the eviction is triggered.  Available only for <code>NATIVE</code> in-memory format. This is supported only by Hazelcast Enterprise.</li>
<li><strong>USED_NATIVE_MEMORY_PERCENTAGE:</strong> Maximum used native memory percentage of the specified near cache to trigger the eviction. If the native memory usage percentage (relative to maximum native memory size) exceeds this threshold, the eviction is triggered. Available only for <code>NATIVE</code> in-memory format. This is supported only by Hazelcast Enterprise.</li>
<li><strong>FREE_NATIVE_MEMORY_SIZE:</strong> Minimum free native memory size of the specified near cache in MB to trigger the eviction.  If free native memory size goes below this threshold, eviction is triggered. Available only for <code>NATIVE</code> in-memory format. This is supported only by Hazelcast Enterprise.</li>
<li><strong>FREE_NATIVE_MEMORY_PERCENTAGE:</strong> Minimum free native memory percentage of the specified near cache to trigger eviction. If free native memory percentage (relative to maximum native memory size) goes below this threshold, eviction is triggered. Available only for <code>NATIVE</code> in-memory format. This is supported only by Hazelcast Enterprise.</li>
</ul>
<a name="eviction-policy"></a><h5 id="-eviction-policy-"><code>eviction-policy</code></h5>
<p>Once a near cache is full (i.e., has reached its maximum size as specified by the <code>max-size-policy</code> element), an eviction policy determines which, if any, entries must be evicted. Currently, the following eviction policies are supported by near cache eviction:</p>
<ul>
<li>LRU (Least Recently Used)</li>
<li>LFU (Least Frequently Used)</li>
</ul>
<a name="configuring-jcache-near-cache"></a><h4 id="configuring-jcache-near-cache">Configuring JCache Near Cache</h4>
<p>The following are example configurations for JCache near cache.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
    ...
    &lt;near-cache name=&quot;myCache&quot;&gt;
        &lt;in-memory-format&gt;BINARY&lt;/in-memory-format&gt;
        &lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;
        &lt;cache-local-entries&gt;false&lt;/cache-local-entries&gt;
        &lt;time-to-live-seconds&gt;3600000&lt;/time-to-live-seconds&gt;
        &lt;max-idle-seconds&gt;600000&lt;/max-idle-seconds&gt;
        &lt;eviction size=&quot;1000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LFU&quot;/&gt;
    &lt;/near-cache&gt;
    ...
&lt;/hazelcast-client&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">EvictionConfig evictionConfig = new EvictionConfig();
evictionConfig.setMaxSizePolicy(MaxSizePolicy.ENTRY_COUNT);
evictionConfig.setEvictionPolicy(EvictionPolicy.LFU);
evictionConfig.setSize(10000);

NearCacheConfig nearCacheConfig =
    new NearCacheConfig()
        .setName(&quot;myCache&quot;)
        .setInMemoryFormat(InMemoryFormat.BINARY)
        .setInvalidateOnChange(true)
        .setCacheLocalEntries(false)
        .setTimeToLiveSeconds(60 * 60 * 1000) // 1 hour TTL
        .setMaxIdleSeconds(10 * 60 * 1000) // 10 minutes max idle seconds
        .setEvictionConfig(evictionConfig); 
...

clientConfig.addNearCacheConfig(nearCacheConfig);
</code></pre>
<p>The following are the definitions of the configuration elements and attributes.</p>
<ul>
<li><code>in-memory-format</code>: Storage type of near cache entries. Available values are <code>BINARY</code>, <code>OBJECT</code> and <code>NATIVE_MEMORY</code>. <code>NATIVE_MEMORY</code> is available only for Hazelcast Enterprise. Default value is <code>BINARY</code>.</li>
<li><code>invalidate-on-change</code>: Specifies whether the cached entries are evicted when the entries are changed (updated or removed) on the local and global. Available values are <code>true</code> and <code>false</code>. Default value is <code>true</code>.</li>
<li><code>cache-local-entries</code>: Specifies whether the local cache entries are stored eagerly (immediately) to near cache when a put operation from the local is performed on the cache. Available values are <code>true</code> and <code>false</code>. Default value is <code>false</code>.</li>
<li><code>time-to-live-seconds</code>: Maximum number of seconds for each entry to stay in the near cache. Entries that are older than <code>&lt;time-to-live-seconds&gt;</code> will be automatically evicted from the near cache. It can be any integer between <code>0</code> and <code>Integer.MAX_VALUE</code>. <code>0</code> means <strong>infinite</strong>. Default value is <code>0</code>.</li>
<li><code>max-idle-seconds</code>: Maximum number of seconds each entry can stay in the near cache as untouched (not-read). Entries that are not read (touched) more than <code>&lt;max-idle-seconds&gt;</code> value will be removed from the near cache. It can be any integer between <code>0</code> and <code>Integer.MAX_VALUE</code>. <code>0</code> means <code>Integer.MAX_VALUE</code>. Default is <code>0</code>.</li>
<li><code>eviction</code>: Specifies when the eviction is triggered (<code>max-size policy</code>) and which eviction policy (<code>LRU</code> or <code>LFU</code>) is used for the entries to be evicted. The default value for <code>max-size-policy</code> is <code>ENTRY_COUNT</code>, default <code>size</code> is <code>10000</code> and default <code>eviction-policy</code> is <code>LRU</code>. For High-Density Memory Store near cache, since <code>ENTRY_COUNT</code> eviction policy is not supported yet, you must explicitly configure eviction with one of the supported policies:<ul>
<li><code>USED_NATIVE_MEMORY_SIZE</code></li>
<li><code>USED_NATIVE_MEMORY_PERCENTAGE</code></li>
<li><code>FREE_NATIVE_MEMORY_SIZE</code></li>
<li><code>FREE_NATIVE_MEMORY_PERCENTAGE</code>.</li>
</ul>
</li>
</ul>
<p>Near cache can be configured only at the client side.</p>
<p><br><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Specifying a <code>time-to-live-seconds</code> value is recommended in order to guarantee the eventual eviction of invalidated near cache records.</em>
<br><br></p>
<a name="lookup-for-client-near-cache-configuration"></a><h4 id="lookup-for-client-near-cache-configuration">Lookup for Client Near Cache Configuration</h4>
<p>Near cache configuration can be defined at the client side (using <code>hazelcast-client.xml</code> or <code>ClientConfig</code>) as independent configuration (independent from the <code>CacheConfig</code>). Near cache configuration lookup is handled as described below:</p>
<ul>
<li>Look for near cache configuration with the name of the cache given in the client configuration.</li>
<li>If a defined near cache configuration is found, use this near cache configuration defined at the client.</li>
<li>Otherwise: <ul>
<li>If a defined default near cache configuration is found, use this default near cache configuration.</li>
<li>If there is no default near cache configuration, it means there is no near cache configuration for cache.</li>
</ul>
</li>
</ul>

<a name="icache-convenience-methods"></a><h3 id="icache-convenience-methods">ICache Convenience Methods</h3>
<p>In addition to the operations explained in <a href="#icache-async-methods">ICache Async Methods</a> and <a href="#defining-a-custom-expirypolicy">Defining a Custom ExpiryPolicy</a>, Hazelcast ICache also provides a set of convenience methods. These methods are not part of the JCache specification.</p>
<ul>
<li><code>size()</code>: Returns the estimated size of the distributed cache.</li>
<li><code>destroy()</code>: Destroys the cache and removes the data from memory. This is different from the method <code>javax.cache.Cache::close</code>.</li>
<li><code>getLocalCacheStatistics()</code>: Returns a <code>com.hazelcast.cache.CacheStatistics</code> instance providing the same statistics data as the JMX beans. This method is not available yet on Hazelcast clients--the exception <code>java.lang.UnsupportedOperationException</code> is thrown when you use this method on a Hazelcast client.</li>
</ul>

<a name="implementing-backupawareentryprocessor"></a><h3 id="implementing-backupawareentryprocessor">Implementing BackupAwareEntryProcessor</h3>
<p>Another feature, especially interesting for distributed environments like Hazelcast, is the JCache specified
<code>javax.cache.processor.EntryProcessor</code>. For more general information, please see the <a href="#implementing-entryprocessor">Implementing EntryProcessor section</a>.</p>
<p>Since Hazelcast provides backups of cached entries on other members, the default way to backup an object changed by an
<code>EntryProcessor</code> is to serialize the complete object and send it to the backup partition. This can be a huge network overhead for big objects.</p>
<p>Hazelcast offers a sub-interface for <code>EntryProcessor</code> called <code>com.hazelcast.cache.BackupAwareEntryProcessor</code>. This allows you to create or pass another <code>EntryProcessor</code> to run on backup
partitions and apply delta changes to the backup entries.</p>
<p>The backup partition <code>EntryProcessor</code> can either be the currently running processor (by returning <code>this</code>) or it can be
a specialized <code>EntryProcessor</code> implementation (different from the currently running one) that does different operations or leaves
out operations, e.g. sending emails.</p>
<p>If we again take the <code>EntryProcessor</code> example from the demonstration application provided in the <a href="#implementing-entryprocessor">Implementing EntryProcessor section</a>,
the changed code will look like the following snippet.</p>
<pre><code class="lang-java">public class UserUpdateEntryProcessor
    implements BackupAwareEntryProcessor&lt;Integer, User, User&gt; {

  @Override
  public User process( MutableEntry&lt;Integer, User&gt; entry, Object... arguments )
      throws EntryProcessorException {

    // Test arguments length
    if ( arguments.length &lt; 1 ) {
      throw new EntryProcessorException( &quot;One argument needed: username&quot; );
    }

    // Get first argument and test for String type
    Object argument = arguments[0];
    if ( !( argument instanceof String ) ) {
      throw new EntryProcessorException(
          &quot;First argument has wrong type, required java.lang.String&quot; );
    }

    // Retrieve the value from the MutableEntry
    User user = entry.getValue();

    // Retrieve the new username from the first argument
    String newUsername = ( String ) arguments[0];

    // Set the new username
    user.setUsername( newUsername );

    // Set the changed user to mark the entry as dirty
    entry.setValue( user );

    // Return the changed user to return it to the caller
    return user;
  }

  public EntryProcessor&lt;K, V, T&gt; createBackupEntryProcessor() {
    return this;
  }
}
</code></pre>
<p>You can use the additional method <code>BackupAwareEntryProcessor::createBackupEntryProcessor</code> to create or return the <code>EntryProcessor</code>
implementation to run on the backup partition (in the example above, the same processor again).</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>For the backup runs, the returned value from the backup processor is ignored and not
returned to the user.</em>
<br></br></p>

<a name="icache-partition-lost-listener"></a><h3 id="icache-partition-lost-listener">ICache Partition Lost Listener</h3>
<p>You can listen to <code>CachePartitionLostEvent</code> instances by registering an implementation
of <code>CachePartitionLostListener</code>, which is also a sub-interface of <code>java.util.EventListener</code>
from <code>ICache</code>.</p>
<p>Let&#39;s consider the following example code:</p>
<pre><code class="lang-java">  public static void main(String[] args) {
    CachingProvider cachingProvider = Caching.getCachingProvider();
    CacheManager cacheManager = cachingProvider.getCacheManager();
    Cache&lt;Object, Object&gt; cache = cacheManager.getCache( ... );

    ICache&lt;Object, Object&gt; unwrappedCache = cache.unwrap( ICache.class );

    unwrappedCache.addPartitionLostListener(new CachePartitionLostListener() {
     @Override
     public void partitionLost(CachePartitionLostEvent event) {
       System.out.println(event);
     }
    });
  }
</code></pre>
<p>Within this example code, a <code>CachePartitionLostListener</code> implementation is registered to a cache and assumes that this cache is configured with one backup. For this particular cache and any of the partitions in the
system, if the partition owner member and its first backup member crash simultaneously, the
given <code>CachePartitionLostListener</code> receives a
corresponding <code>CachePartitionLostEvent</code>. If only a single member crashes in the cluster,
a <code>CachePartitionLostEvent</code> is not fired for this cache since backups for the partitions
owned by the crashed member are kept on other members.</p>
<p>Please refer to the <a href="#listening-for-partition-lost-events">Partition Lost Listener section</a> for more
information about partition lost detection and partition lost events.</p>

<a name="jcache-split-brain"></a><h3 id="jcache-split-brain">JCache Split-Brain</h3>
<p>Split-Brain handling is internally supported as a service inside Hazelcast (see <a href="#network-partitioning-split-brain-syndrome">Network Partitioning</a> for more details) and <code>JCache</code> uses same infrastructure with <code>IMap</code> to support Split-Brain. You can specify cache merge policy to determine which entry is used while merging. You can also provide your own cache merge policy implementations through <code>CacheMergePolicyInterface</code>.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Split-Brain is only supported for heap-based JCache but not for HD-JCache, since merging a high volume of data in consistent way may cause significant performance loss on the system.</em></p>
<a name="cachemergepolicy-interface"></a><h4 id="-cachemergepolicy-interface"><code>CacheMergePolicy</code> Interface</h4>
<p>After split clusters are joined again, they merge their entries with each other. This merge process is handled over the <code>CacheMergePolicy</code> interface.
The <code>CacheMergePolicy</code> instance takes two entries: the owned entry, and the merging entry which comes from the joined cluster. The <code>CacheEntryView</code> instance wraps the key, value, and some metadata about the entry (such as creation time, expiration time, and access hit). Then the <code>CacheMergePolicy</code> instance selects one of the entries and returns it.
The returned entry is used as the stored cache entry.</p>
<a name="cacheentryview"></a><h5 id="-cacheentryview-"><code>CacheEntryView</code></h5>
<p>Wraps key, value and some metadata (such as expiration time, last access time, and access hit of cache entry) and exposes them to outside as read only.</p>
<a name="cachemergepolicy"></a><h5 id="-cachemergepolicy-"><code>CacheMergePolicy</code></h5>
<p>Policy for merging cache entries. Entries from joined clusters are merged by using this policy to select one of them from source and target. 
Passed <code>CacheEntryView</code> instances wrap the key and value as their original types, with conversion to object from their storage types. 
If the user doesn&#39;t need the original types of key and value, you should use <code>StorageTypeAwareCacheMergePolicy</code>, which is a sub-type of this interface.</p>
<a name="storagetypeawarecachemergepolicy"></a><h5 id="-storagetypeawarecachemergepolicy-"><code>StorageTypeAwareCacheMergePolicy</code></h5>
<p>Marker interface indicating that the key and value wrapped by <code>CacheEntryView</code> will be not converted to their original types. 
The motivation of this interface is that while merging cache entries, actual key and value are not usually not checked. Therefore, there is no need to convert them to their original types.</p>
<p>At worst case, value is returned from the merge method as selected, meaning that in all cases, value is accessed. So even if the the conversion is done as lazy, it will be processed at this point. By default, key and value are converted to their original types unless this <code>StorageTypeAwareCacheMergePolicy</code> is used.</p>
<p>Another motivation for using this interface is that at the member side, there is no need to locate classes of stored entries. Entries can be put from the client with <code>BINARY</code> in-memory format and the classpath of the client can be different from the member. So in this case, if entries try to convert to their original types while merging, <code>ClassNotFoundException</code> is thrown here.</p>
<p>As a result, both for performance and for the <code>ClassNotFoundException</code> mentioned above, it is strongly recommended that you use this interface if the original values of key and values are not needed.</p>
<a name="configuration"></a><h4 id="configuration">Configuration</h4>
<p>There are four built-in cache merge policies:</p>
<ul>
<li><strong>Pass Through:</strong> Merges cache entry from source to destination directly. You can specify this policy with its full class name as <code>com.hazelcast.cache.merge.PassThroughCacheMergePolicy</code> or with its constant name as <code>PASS_THROUGH</code>.</li>
<li><strong>Put If Absent:</strong> Merges cache entry from source to destination if it does not exist in the destination cache. You can specify this policy with its full class name as <code>com.hazelcast.cache.merge.PutIfAbsentCacheMergePolicy</code> or with its constant name as <code>PUT_IF_ABSENT</code>.</li>
<li><strong>Higher Hits:</strong> Merges cache entry from source to destination cache if the source entry has more hits than the destination one. You can specify this policy with its full class name as <code>com.hazelcast.cache.merge.HigherHitsCacheMergePolicy</code> or with its constant name as <code>HIGHER_HITS</code>.</li>
<li><strong>Latest Access:</strong> Merges cache entry from source to destination cache if the source entry has been accessed more recently than the destination entry. You can specify this policy with its full class name as <code>com.hazelcast.cache.merge.LatestAccessCacheMergePolicy</code> or with its constant name as <code>LATEST_ACCESS</code>.</li>
</ul>
<p>You can access full class names or constant names of all built-in cache merge policies over <code>com.hazelcast.cache.BuiltInCacheMergePolicies</code> enum. You can specify merge policy configuration for cache declaratively or programmatically.</p>
<p>The following are example configurations for JCache Split-Brain.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">&lt;cache name=&quot;cacheWithBuiltInMergePolicyAsConstantName&quot;&gt;
    ...
    &lt;merge-policy&gt;HIGHER_HITS&lt;/merge-policy&gt;
    ...
&lt;/cache&gt;&lt;cache name=&quot;cacheWithBuiltInMergePolicyAsFullClassName&quot;&gt;
    ...
    &lt;merge-policy&gt;com.hazelcast.cache.merge.LatestAccessCacheMergePolicy&lt;/merge-policy&gt;
    ...
&lt;/cache&gt;
&lt;cache name=&quot;cacheWithBuiltInMergePolicyAsCustomImpl&quot;&gt;
    ...
    &lt;merge-policy&gt;com.mycompany.cache.merge.MyCacheMergePolicy&lt;/merge-policy&gt;
    ...
&lt;/cache&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">CacheConfig cacheConfigWithBuiltInMergePolicyAsConstantName = new CacheConfig();
cacheConfig.setMergePolicy(BuiltInCacheMergePolicies.HIGGER_HITS.name());

CacheConfig cacheConfigWithBuiltInMergePolicyAsFullClassName = new CacheConfig();
cacheConfig.setMergePolicy(BuiltInCacheMergePolicies.LATEST_ACCESS.getImplementationClassName());

CacheConfig cacheConfigWithBuiltInMergePolicyAsCustomImpl = new CacheConfig();
cacheConfig.setMergePolicy(&quot;com.mycompany.cache.merge.MyCacheMergePolicy&quot;);
</code></pre>

<a name="testing-for-jcache-specification-compliance"></a><h2 id="testing-for-jcache-specification-compliance">Testing for JCache Specification Compliance</h2>
<p>Hazelcast JCache is fully compliant with the JSR 107 TCK (Technology Compatibility Kit),and therefore is officially a JCache
implementation. </p>
<p>You can test Hazelcast JCache for compliance by executing the TCK. Just perform the instructions below:</p>
<ol>
<li>Checkout the TCK from <a href="https://github.com/jsr107/jsr107tck" target="_blank">https://github.com/jsr107/jsr107tck</a>.</li>
<li>Change the properties in <code>tck-parent/pom.xml</code> as shown below.</li>
<li>Run the TCK by <code>mvn clean install</code>.</li>
</ol>
<pre><code class="lang-xml">&lt;properties&gt;
  &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;

  &lt;CacheInvocationContextImpl&gt;
    javax.cache.annotation.impl.cdi.CdiCacheKeyInvocationContextImpl
  &lt;/CacheInvocationContextImpl&gt;

  &lt;domain-lib-dir&gt;${project.build.directory}/domainlib&lt;/domain-lib-dir&gt;
  &lt;domain-jar&gt;domain.jar&lt;/domain-jar&gt;


  &lt;!-- ################################################################# --&gt;
  &lt;!-- Change the following properties on the command line
       to override with the coordinates for your implementation--&gt;
  &lt;implementation-groupId&gt;com.hazelcast&lt;/implementation-groupId&gt;
  &lt;implementation-artifactId&gt;hazelcast&lt;/implementation-artifactId&gt;
  &lt;implementation-version&gt;3.4&lt;/implementation-version&gt;

  &lt;!-- Change the following properties to your CacheManager and
       Cache implementation. Used by the unwrap tests. --&gt;
  &lt;CacheManagerImpl&gt;
    com.hazelcast.client.cache.impl.HazelcastClientCacheManager
  &lt;/CacheManagerImpl&gt;
  &lt;CacheImpl&gt;com.hazelcast.cache.ICache&lt;/CacheImpl&gt;
  &lt;CacheEntryImpl&gt;
    com.hazelcast.cache.impl.CacheEntry
  &lt;/CacheEntryImpl&gt;

  &lt;!-- Change the following to point to your MBeanServer, so that
       the TCK can resolve it. --&gt;
  &lt;javax.management.builder.initial&gt;
    com.hazelcast.cache.impl.TCKMBeanServerBuilder
  &lt;/javax.management.builder.initial&gt;
  &lt;org.jsr107.tck.management.agentId&gt;
    TCKMbeanServer
  &lt;/org.jsr107.tck.management.agentId&gt;
  &lt;jsr107.api.version&gt;1.0.0&lt;/jsr107.api.version&gt;

  &lt;!-- ################################################################# --&gt;
&lt;/properties&gt;
</code></pre>
<p>This will run the tests using an embedded Hazelcast Member.</p>

<a name="integrated-clustering"></a><h1 id="integrated-clustering">Integrated Clustering</h1>
<p>In this chapter, we show you how Hazelcast is integrated with Hibernate 2nd level cache and Spring, and how Hazelcast helps with your Filter, Tomcat and Jetty based web session replications.</p>
<p>The <a href="#hibernate-second-level-cache">Hibernate Second Level Cache section</a> tells how you should configure both Hazelcast and Hibernate to integrate them. It explains the modes of Hazelcast that can be used by Hibernate and also provides how to perform advanced settings like accessing the underlying Hazelcast instance used by Hibernate.</p>
<p>The <a href="#web-session-replication">Web Session Replication section</a> tells how to cluster user HTTP sessions automatically. You will learn how to enable session replication using filter based solution. In addition, Tomcat and Jetty specific modules will be explained.</p>
<p>The <a href="#spring-integration">Spring Integration section</a> tells how you can integrate Hazelcast into a Spring project by explaining the Hazelcast instance and client configurations with the <em>hazelcast</em> namespace. It also lists the supported Spring bean attributes. </p>

<a name="hibernate-second-level-cache"></a><h2 id="hibernate-second-level-cache">Hibernate Second Level Cache</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Hazelcast provides distributed second level cache for your Hibernate entities, collections and queries. This feature is offered as Hazelcast plugins. We have two plugin repositories for Hibernate 2nd Level Cache. Please see their own GitHub repositories at <a href="https://github.com/hazelcast/hazelcast-hibernate" target="_blank">Hazelcast Hibernate</a> (for Hibernate 3.x and 4.x) and <a href="https://github.com/hazelcast/hazelcast-hibernate5" target="_blank">Hazelcast Hibernate 5</a> (for Hibernate 5.x) for information on configuring and using them.</p>

<a name="web-session-replication"></a><h2 id="web-session-replication">Web Session Replication</h2>
<p>This section explains how you can cluster your web sessions using Servlet Filter, Tomcat, and Jetty based solutions. Each web session clustering is explained in the following subsections.</p>
<p><a href="#filter-based-web-session-replication">Filter</a> based web session replication has the option to use a map with High-Density Memory Store to keep your session objects. Note that High-Density Memory Store is available in <font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>. Please refer to the <a href="#high-density-memory-store">High-Density Memory Store section</a> for details on this feature.</p>
<a name="filter-based-web-session-replication"></a><h3 id="filter-based-web-session-replication">Filter Based Web Session Replication</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84"></p>
<p><br></br></p>
<p><strong><em>Sample Code</em></strong>: <em>Please see our <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/filter-based-session-replication" target="_blank">sample application</a> for Filter Based Web Session Replication.</em>
<br></br></p>
<p>Assume that you have more than one web server (A, B, C) with a load balancer in front of it. If server A goes down, your users on that server will be directed to one of the live servers (B or C), but their sessions will be lost.</p>
<p>We need to have all these sessions backed up somewhere if we do not want to lose the sessions upon server crashes. Hazelcast Web Manager (WM) allows you to cluster user HTTP sessions automatically. </p>
<p>Filter Based Web Session Replication is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-wm" target="_blank">Hazelcast Filter Based Web Session Replication</a> for information on configuring and using it.</p>

<a name="tomcat-based-web-session-replication"></a><h3 id="tomcat-based-web-session-replication">Tomcat Based Web Session Replication</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Tomcat based web session replication is offered through Hazelcast Tomcat Session Manager. It is a container specific module that enables session replication for JEE Web Applications without requiring changes to the application. Tomcat Session Manager is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-tomcat-sessionmanager" target="_blank">Hazelcast Tomcat Session Manager</a> for information on configuring and using it.</p>

<a name="jetty-based-web-session-replication"></a><h3 id="jetty-based-web-session-replication">Jetty Based Web Session Replication</h3>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Jetty based web session replication is offered through Hazelcast Jetty Session Manager. It is a container specific module that enables session replication for JEE Web Applications without requiring changes to the application. Jetty Session Manager is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-jetty-sessionmanager" target="_blank">Hazelcast Jetty Session Manager</a> for information on configuring and using it.</p>

<a name="spring-integration"></a><h2 id="spring-integration">Spring Integration</h2>
<p>You can integrate Hazelcast with Spring and this chapter explains the configuration of Hazelcast within Spring context. </p>
<a name="supported-versions"></a><h3 id="supported-versions">Supported Versions</h3>
<ul>
<li>Spring 2.5+</li>
</ul>
<a name="configuring-spring"></a><h3 id="configuring-spring">Configuring Spring</h3>
<p><strong><em>Sample Code</em></strong>: <em>Please see our <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/spring-configuration" target="_blank">sample application</a> for Spring Configuration.</em>
<br></br></p>
<a name="declaring-beans-by-spring-beans-namespace"></a><h4 id="declaring-beans-by-spring-beans-namespace">Declaring Beans by Spring <em>beans</em> Namespace</h4>
<p><strong><em>Classpath Configuration</em></strong> </p>
<p>This configuration requires the following jar file in the classpath:</p>
<ul>
<li><code>hazelcast-</code>&lt;<em>version</em>&gt;<code>.jar</code></li>
</ul>
<p><strong><em>Bean Declaration</em></strong> </p>
<p>You can declare Hazelcast Objects using the default Spring <em>beans</em> namespace. Example code for a Hazelcast Instance declaration is listed below.</p>
<pre><code class="lang-xml">&lt;bean id=&quot;instance&quot; class=&quot;com.hazelcast.core.Hazelcast&quot; factory-method=&quot;newHazelcastInstance&quot;&gt;
  &lt;constructor-arg&gt;
    &lt;bean class=&quot;com.hazelcast.config.Config&quot;&gt;
      &lt;property name=&quot;groupConfig&quot;&gt;
        &lt;bean class=&quot;com.hazelcast.config.GroupConfig&quot;&gt;
          &lt;property name=&quot;name&quot; value=&quot;dev&quot;/&gt;
          &lt;property name=&quot;password&quot; value=&quot;pwd&quot;/&gt;
        &lt;/bean&gt;
      &lt;/property&gt;
      &lt;!-- and so on ... --&gt;
    &lt;/bean&gt;
  &lt;/constructor-arg&gt;
&lt;/bean&gt;

&lt;bean id=&quot;map&quot; factory-bean=&quot;instance&quot; factory-method=&quot;getMap&quot;&gt;
  &lt;constructor-arg value=&quot;map&quot;/&gt;
&lt;/bean&gt;
</code></pre>
<a name="declaring-beans-by-hazelcast-namespace"></a><h4 id="declaring-beans-by-hazelcast-namespace">Declaring Beans by <em>hazelcast</em> Namespace</h4>
<p><strong><em>Configuring Classpath</em></strong> </p>
<p>Hazelcast-Spring integration requires the following JAR files in the classpath:</p>
<ul>
<li><code>hazelcast-spring-</code>&lt;<em>version</em>&gt;<code>.jar</code></li>
<li><code>hazelcast-</code>&lt;<em>version</em>&gt;<code>.jar</code></li>
</ul>
<p>or</p>
<ul>
<li><code>hazelcast-all-</code>&lt;<em>version</em>&gt;<code>.jar</code></li>
</ul>
<p><strong><em>Declaring Beans</em></strong> </p>
<p>Hazelcast has its own namespace <strong>hazelcast</strong> for bean definitions. You can easily add the namespace declaration <em>xmlns:hz=&quot;<a href="http://www.hazelcast.com/schema/spring">http://www.hazelcast.com/schema/spring</a>&quot;</em> to the <code>beans</code> element in the context file so that <em>hz</em> namespace shortcut can be used as a bean declaration.</p>
<p>Here is an example schema definition for Hazelcast 3.3.x:</p>
<pre><code class="lang-xml">&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xmlns:hz=&quot;http://www.hazelcast.com/schema/spring&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans
                http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
                http://www.hazelcast.com/schema/spring
                http://www.hazelcast.com/schema/spring/hazelcast-spring.xsd&quot;&gt;
</code></pre>
<a name="supported-configurations-with-hazelcast-namespace"></a><h4 id="supported-configurations-with-hazelcast-namespace">Supported Configurations with <em>hazelcast</em> Namespace</h4>
<ul>
<li><strong>Configuring Hazelcast Instance</strong></li>
</ul>
<pre><code class="lang-xml">&lt;hz:hazelcast id=&quot;instance&quot;&gt;
  &lt;hz:config&gt;
    &lt;hz:group name=&quot;dev&quot; password=&quot;password&quot;/&gt;
    &lt;hz:network port=&quot;5701&quot; port-auto-increment=&quot;false&quot;&gt;
      &lt;hz:join&gt;
        &lt;hz:multicast enabled=&quot;false&quot;
                      multicast-group=&quot;224.2.2.3&quot;
                      multicast-port=&quot;54327&quot;/&gt;
        &lt;hz:tcp-ip enabled=&quot;true&quot;&gt;
          &lt;hz:members&gt;10.10.1.2, 10.10.1.3&lt;/hz:members&gt;
        &lt;/hz:tcp-ip&gt;
      &lt;/hz:join&gt;
    &lt;/hz:network&gt;
    &lt;hz:map name=&quot;map&quot;
            backup-count=&quot;2&quot;
            max-size=&quot;0&quot;
            eviction-percentage=&quot;30&quot;
            read-backup-data=&quot;true&quot;
            eviction-policy=&quot;NONE&quot;
            merge-policy=&quot;com.hazelcast.map.merge.PassThroughMergePolicy&quot;/&gt;
  &lt;/hz:config&gt;
&lt;/hz:hazelcast&gt;
</code></pre>
<ul>
<li><strong>Configuring Hazelcast Client</strong></li>
</ul>
<pre><code class="lang-xml">&lt;hz:client id=&quot;client&quot;&gt;
  &lt;hz:group name=&quot;${cluster.group.name}&quot; password=&quot;${cluster.group.password}&quot; /&gt;
  &lt;hz:network connection-attempt-limit=&quot;3&quot;
              connection-attempt-period=&quot;3000&quot;
              connection-timeout=&quot;1000&quot;
              redo-operation=&quot;true&quot;
              smart-routing=&quot;true&quot;&gt;
    &lt;hz:member&gt;10.10.1.2:5701&lt;/hz:member&gt;
    &lt;hz:member&gt;10.10.1.3:5701&lt;/hz:member&gt;
  &lt;/hz:network&gt;
&lt;/hz:client&gt;
</code></pre>
<ul>
<li><p><strong>Hazelcast Supported Type Configurations and Examples</strong></p>
<ul>
<li><code>map</code></li>
<li><code>multiMap</code></li>
<li><code>replicatedmap</code></li>
<li><code>queue</code></li>
<li><code>topic</code></li>
<li><code>set</code></li>
<li><code>list</code></li>
<li><code>executorService</code></li>
<li><code>idGenerator</code></li>
<li><code>atomicLong</code></li>
<li><code>atomicReference</code></li>
<li><code>semaphore</code></li>
<li><code>countDownLatch</code></li>
<li><code>lock</code></li>
</ul>
</li>
</ul>
<pre><code class="lang-xml">&lt;hz:map id=&quot;map&quot; instance-ref=&quot;client&quot; name=&quot;map&quot; lazy-init=&quot;true&quot; /&gt;
&lt;hz:multiMap id=&quot;multiMap&quot; instance-ref=&quot;instance&quot; name=&quot;multiMap&quot;
    lazy-init=&quot;false&quot; /&gt;
&lt;hz:replicatedmap id=&quot;replicatedmap&quot; instance-ref=&quot;instance&quot; 
    name=&quot;replicatedmap&quot; lazy-init=&quot;false&quot; /&gt;
&lt;hz:queue id=&quot;queue&quot; instance-ref=&quot;client&quot; name=&quot;queue&quot; 
    lazy-init=&quot;true&quot; depends-on=&quot;instance&quot;/&gt;
&lt;hz:topic id=&quot;topic&quot; instance-ref=&quot;instance&quot; name=&quot;topic&quot; 
    depends-on=&quot;instance, client&quot;/&gt;
&lt;hz:set id=&quot;set&quot; instance-ref=&quot;instance&quot; name=&quot;set&quot; /&gt;
&lt;hz:list id=&quot;list&quot; instance-ref=&quot;instance&quot; name=&quot;list&quot;/&gt;
&lt;hz:executorService id=&quot;executorService&quot; instance-ref=&quot;client&quot; 
    name=&quot;executorService&quot;/&gt;
&lt;hz:idGenerator id=&quot;idGenerator&quot; instance-ref=&quot;instance&quot; 
    name=&quot;idGenerator&quot;/&gt;
&lt;hz:atomicLong id=&quot;atomicLong&quot; instance-ref=&quot;instance&quot; name=&quot;atomicLong&quot;/&gt;
&lt;hz:atomicReference id=&quot;atomicReference&quot; instance-ref=&quot;instance&quot; 
    name=&quot;atomicReference&quot;/&gt;
&lt;hz:semaphore id=&quot;semaphore&quot; instance-ref=&quot;instance&quot; name=&quot;semaphore&quot;/&gt;
&lt;hz:countDownLatch id=&quot;countDownLatch&quot; instance-ref=&quot;instance&quot; 
    name=&quot;countDownLatch&quot;/&gt;
&lt;hz:lock id=&quot;lock&quot; instance-ref=&quot;instance&quot; name=&quot;lock&quot;/&gt;
</code></pre>
<ul>
<li><strong>Supported Spring Bean Attributes</strong></li>
</ul>
<p>Hazelcast also supports <code>lazy-init</code>, <code>scope</code> and <code>depends-on</code> bean attributes.</p>
<pre><code class="lang-xml">&lt;hz:hazelcast id=&quot;instance&quot; lazy-init=&quot;true&quot; scope=&quot;singleton&quot;&gt;
  ...
&lt;/hz:hazelcast&gt;
&lt;hz:client id=&quot;client&quot; scope=&quot;prototype&quot; depends-on=&quot;instance&quot;&gt;
  ...
&lt;/hz:client&gt;
</code></pre>
<ul>
<li><strong>Configuring MapStore and NearCache</strong></li>
</ul>
<p>For map-store, you should set either the <em>class-name</em> or the <em>implementation</em> attribute.</p>
<pre><code class="lang-xml">&lt;hz:config&gt;
  &lt;hz:map name=&quot;map1&quot;&gt;
    &lt;hz:near-cache time-to-live-seconds=&quot;0&quot; max-idle-seconds=&quot;60&quot;
        eviction-policy=&quot;LRU&quot; max-size=&quot;5000&quot;  invalidate-on-change=&quot;true&quot;/&gt;

    &lt;hz:map-store enabled=&quot;true&quot; class-name=&quot;com.foo.DummyStore&quot;
        write-delay-seconds=&quot;0&quot;/&gt;
  &lt;/hz:map&gt;

  &lt;hz:map name=&quot;map2&quot;&gt;
    &lt;hz:map-store enabled=&quot;true&quot; implementation=&quot;dummyMapStore&quot;
        write-delay-seconds=&quot;0&quot;/&gt;
  &lt;/hz:map&gt;

  &lt;bean id=&quot;dummyMapStore&quot; class=&quot;com.foo.DummyStore&quot; /&gt;
&lt;/hz:config&gt;
</code></pre>
<a name="enabling-springaware-objects"></a><h3 id="enabling-springaware-objects">Enabling SpringAware Objects</h3>
<p>You can mark Hazelcast Distributed Objects with @SpringAware if the object wants:</p>
<ul>
<li>to apply bean properties,</li>
<li>to apply factory callbacks such as <code>ApplicationContextAware</code>, <code>BeanNameAware</code>,</li>
<li>to apply bean post-processing annotations such as <code>InitializingBean</code>, <code>@PostConstruct</code>.</li>
</ul>
<p>Hazelcast Distributed <code>ExecutorService</code>, or more generally any Hazelcast managed object, can benefit from these features. To enable SpringAware objects, you must first configure <code>HazelcastInstance</code> using <em>hazelcast</em> namespace as explained in <a href="#configuring-spring">Configuring Spring</a> and add <code>&lt;hz:spring-aware /&gt;</code> tag.</p>
<a name="springaware-examples"></a><h4 id="springaware-examples">SpringAware Examples</h4>
<ul>
<li>Configure a Hazelcast Instance (3.3.x) via Spring Configuration and define <em>someBean</em> as Spring Bean.</li>
<li>Add <code>&lt;hz:spring-aware /&gt;</code> to Hazelcast configuration to enable @SpringAware.</li>
</ul>
<pre><code class="lang-xml">&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;
       xmlns:hz=&quot;http://www.hazelcast.com/schema/spring&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans
                http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
                http://www.springframework.org/schema/context
                http://www.springframework.org/schema/context/spring-context-3.0.xsd
                http://www.hazelcast.com/schema/spring
                http://www.hazelcast.com/schema/spring/hazelcast-spring.xsd&quot;&gt;

  &lt;context:annotation-config /&gt;

  &lt;hz:hazelcast id=&quot;instance&quot;&gt;
    &lt;hz:config&gt;
      &lt;hz:spring-aware /&gt;
      &lt;hz:group name=&quot;dev&quot; password=&quot;password&quot;/&gt;
      &lt;hz:network port=&quot;5701&quot; port-auto-increment=&quot;false&quot;&gt;
        &lt;hz:join&gt;
          &lt;hz:multicast enabled=&quot;false&quot; /&gt;
          &lt;hz:tcp-ip enabled=&quot;true&quot;&gt;
            &lt;hz:members&gt;10.10.1.2, 10.10.1.3&lt;/hz:members&gt;
          &lt;/hz:tcp-ip&gt;
        &lt;/hz:join&gt;
      &lt;/hz:network&gt;
      ...
    &lt;/hz:config&gt;
  &lt;/hz:hazelcast&gt;

  &lt;bean id=&quot;someBean&quot; class=&quot;com.hazelcast.examples.spring.SomeBean&quot;
      scope=&quot;singleton&quot; /&gt;
  ...
&lt;/beans&gt;
</code></pre>
<p><strong>Distributed Map SpringAware Example:</strong></p>
<ul>
<li>Create a class called <code>SomeValue</code> which contains Spring Bean definitions like <code>ApplicationContext</code> and <code>SomeBean</code>.</li>
</ul>
<pre><code class="lang-java">@SpringAware
@Component(&quot;someValue&quot;)
@Scope(&quot;prototype&quot;)
public class SomeValue implements Serializable, ApplicationContextAware {

  private transient ApplicationContext context;

  private transient SomeBean someBean;

  private transient boolean init = false;

  public void setApplicationContext( ApplicationContext applicationContext )
    throws BeansException {
    context = applicationContext;
  }

  @Autowired
  public void setSomeBean( SomeBean someBean)  {
    this.someBean = someBean;
  }

  @PostConstruct
  public void init() {
    someBean.doSomethingUseful();
    init = true;
  }
  ...
}
</code></pre>
<ul>
<li>Get <code>SomeValue</code> Object from Context and put it into Hazelcast Distributed Map on the first member.</li>
</ul>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = 
    (HazelcastInstance) context.getBean( &quot;hazelcast&quot; );
SomeValue value = (SomeValue) context.getBean( &quot;someValue&quot; )
IMap&lt;String, SomeValue&gt; map = hazelcastInstance.getMap( &quot;values&quot; );
map.put( &quot;key&quot;, value );
</code></pre>
<ul>
<li>Read <code>SomeValue</code> Object from Hazelcast Distributed Map and assert that <code>init</code> method is called since it is annotated with <code>@PostConstruct</code>.</li>
</ul>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = 
    (HazelcastInstance) context.getBean( &quot;hazelcast&quot; );
IMap&lt;String, SomeValue&gt; map = hazelcastInstance.getMap( &quot;values&quot; );
SomeValue value = map.get( &quot;key&quot; );
Assert.assertTrue( value.init );
</code></pre>
<p><strong>ExecutorService SpringAware Example:</strong></p>
<ul>
<li>Create a Callable Class called SomeTask which contains Spring Bean definitions like <code>ApplicationContext</code>, <code>SomeBean</code>.</li>
</ul>
<pre><code class="lang-java">@SpringAware
public class SomeTask
    implements Callable&lt;Long&gt;, ApplicationContextAware, Serializable {

  private transient ApplicationContext context;

  private transient SomeBean someBean;

  public Long call() throws Exception {
    return someBean.value;
  }

  public void setApplicationContext( ApplicationContext applicationContext )
      throws BeansException {
    context = applicationContext;
  }

  @Autowired
  public void setSomeBean( SomeBean someBean ) {
    this.someBean = someBean;
  }
}
</code></pre>
<ul>
<li>Submit <code>SomeTask</code> to two Hazelcast Members and assert that <code>someBean</code> is autowired.</li>
</ul>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance =
    (HazelcastInstance) context.getBean( &quot;hazelcast&quot; );
SomeBean bean = (SomeBean) context.getBean( &quot;someBean&quot; );

Future&lt;Long&gt; f = hazelcastInstance.getExecutorService().submit(new SomeTask());
Assert.assertEquals(bean.value, f.get().longValue());

// choose a member
Member member = hazelcastInstance.getCluster().getMembers().iterator().next();

Future&lt;Long&gt; f2 = (Future&lt;Long&gt;) hazelcast.getExecutorService()
    .submitToMember(new SomeTask(), member);
Assert.assertEquals(bean.value, f2.get().longValue());
</code></pre>
<p><br><br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Spring managed properties/fields are marked as <code>transient</code>.</em></p>
<a name="adding-caching-to-spring"></a><h3 id="adding-caching-to-spring">Adding Caching to Spring</h3>
<p><strong><em>Sample Code</em></strong>: <em>Please see our sample application for <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/spring-cache-manager" target="_blank">Spring Cache</a>.</em>
<br></br></p>
<p>As of version 3.1, Spring Framework provides support for adding caching into an existing Spring application. Spring 3.2 and later versions support JCache compliant caching providers. You can also use JCache caching backed by Hazelcast if your Spring version supports JCache.</p>
<a name="declarative-spring-cache-configuration"></a><h4 id="declarative-spring-cache-configuration">Declarative Spring Cache Configuration</h4>
<pre><code class="lang-xml">&lt;cache:annotation-driven cache-manager=&quot;cacheManager&quot; /&gt;

&lt;hz:hazelcast id=&quot;hazelcast&quot;&gt;
  ...
&lt;/hz:hazelcast&gt;

&lt;bean id=&quot;cacheManager&quot; class=&quot;com.hazelcast.spring.cache.HazelcastCacheManager&quot;&gt;
  &lt;constructor-arg ref=&quot;instance&quot;/&gt;
&lt;/bean&gt;
</code></pre>
<p>Hazelcast uses its Map implementation for underlying cache. You can configure a map with your cache&#39;s name if you want to set additional configuration such as <code>ttl</code>.</p>
<pre><code class="lang-xml">&lt;cache:annotation-driven cache-manager=&quot;cacheManager&quot; /&gt;

&lt;hz:hazelcast id=&quot;hazelcast&quot;&gt;
  &lt;hz:config&gt;
    ...

    &lt;hz:map name=&quot;city&quot; time-to-live-seconds=&quot;0&quot; in-memory-format=&quot;BINARY&quot; /&gt;
&lt;/hz:hazelcast&gt;

&lt;bean id=&quot;cacheManager&quot; class=&quot;com.hazelcast.spring.cache.HazelcastCacheManager&quot;&gt;
  &lt;constructor-arg ref=&quot;instance&quot;/&gt;
&lt;/bean&gt;
</code></pre>
<pre><code>public interface IDummyBean {
  @Cacheable(&quot;city&quot;)
  String getCity();
}
</code></pre><a name="declarative-hazelcast-jcache-based-caching-configuration"></a><h4 id="declarative-hazelcast-jcache-based-caching-configuration">Declarative Hazelcast JCache Based Caching Configuration</h4>
<pre><code class="lang-xml">&lt;cache:annotation-driven cache-manager=&quot;cacheManager&quot; /&gt;

&lt;hz:hazelcast id=&quot;hazelcast&quot;&gt;
  ...
&lt;/hz:hazelcast&gt;

&lt;hz:cache-manager id=&quot;hazelcastJCacheCacheManager&quot; instance-ref=&quot;instance&quot; name=&quot;hazelcastJCacheCacheManager&quot;/&gt;

&lt;bean id=&quot;cacheManager&quot; class=&quot;org.springframework.cache.jcache.JCacheCacheManager&quot;&gt;
    &lt;constructor-arg ref=&quot;hazelcastJCacheCacheManager&quot; /&gt;
&lt;/bean&gt;
</code></pre>
<p>You can use JCache implementation in both member and client mode. A cache manager should be bound to an instance. Instance can be referenced by <code>instance-ref</code> attribute or provided by <code>hazelcast.instance.name</code> property which is passed to CacheManager. Instance should be specified using one of these methods.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Instance name provided in properties overrides <code>instance-ref</code> attribute.</em></p>
<p>You can specify an URI for each cache manager with <code>uri</code> attribute.</p>
<pre><code class="lang-xml">&lt;hz:cache-manager id=&quot;cacheManager2&quot; name=&quot;cacheManager2&quot; uri=&quot;testURI&quot;&gt;
    &lt;hz:properties&gt;
        &lt;hz:property name=&quot;hazelcast.instance.name&quot;&gt;named-spring-hz-instance&lt;/hz:property&gt;
        &lt;hz:property name=&quot;testProperty&quot;&gt;testValue&lt;/hz:property&gt;
    &lt;/hz:properties&gt;
&lt;/hz:cache-manager&gt;
</code></pre>
<a name="annotation-based-spring-cache-configuration"></a><h4 id="annotation-based-spring-cache-configuration">Annotation-Based Spring Cache Configuration</h4>
<p>Annotation-Based Configuration does not require any XML definition. To perform Annotation-Based Configuration:</p>
<ul>
<li>Implement a <code>CachingConfiguration</code> class with related Annotations.</li>
</ul>
<pre><code class="lang-java">@Configuration
@EnableCaching
public class CachingConfiguration implements CachingConfigurer{
    @Bean
    public CacheManager cacheManager() {
        ClientConfig config = new ClientConfig();
        HazelcastInstance client = HazelcastClient.newHazelcastClient(config);
        return new HazelcastCacheManager(client);
    }
    @Bean
    public KeyGenerator keyGenerator() {
        return null;
    }
</code></pre>
<ul>
<li>Launch Application Context and register <code>CachingConfiguration</code>.</li>
</ul>
<pre><code class="lang-java">AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();
context.register(CachingConfiguration.class);
context.refresh();
</code></pre>
<p>For more information about Spring Cache, please see <a href="http://static.springsource.org/spring/docs/3.1.x/spring-framework-reference/html/cache.html" target="_blank">Spring Cache Abstraction</a>.</p>
<a name="configuring-hibernate-second-level-cache"></a><h3 id="configuring-hibernate-second-level-cache">Configuring Hibernate Second Level Cache</h3>
<p><strong><em>Sample Code</em></strong>: <em>Please see our <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/spring-hibernate-2ndlevel-cache" target="_blank">sample application</a> for Hibernate 2nd Level Cache Config.</em>
<br></br></p>
<p>If you are using Hibernate with Hazelcast as a second level cache provider, you can easily create <code>RegionFactory</code> instances within Spring configuration (by Spring version 3.1). That way, you can use the same <code>HazelcastInstance</code> as Hibernate L2 cache instance.</p>
<pre><code class="lang-xml">&lt;hz:hibernate-region-factory id=&quot;regionFactory&quot; instance-ref=&quot;instance&quot;
    mode=&quot;LOCAL&quot; /&gt;
...
&lt;bean id=&quot;sessionFactory&quot; 
      class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot; 
      scope=&quot;singleton&quot;&gt;
  &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;
  &lt;property name=&quot;cacheRegionFactory&quot; ref=&quot;regionFactory&quot; /&gt;
  ...
&lt;/bean&gt;
</code></pre>
<p><strong>Hibernate RegionFactory Modes</strong></p>
<ul>
<li>LOCAL</li>
<li>DISTRIBUTED </li>
</ul>
<p>Please refer to Hibernate <a href="#configuring-regionfactory">Configuring RegionFactory</a> for more information.</p>
<a name="configuring-hazelcast-transaction-manager"></a><h3 id="configuring-hazelcast-transaction-manager">Configuring Hazelcast Transaction Manager</h3>
<p><strong><em>Sample Code</em></strong>: <em>Please see our <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/spring-transaction-manager" target="_blank">sample application</a> for Hazelcast Transaction Manager in our code samples repository.</em>
<br></br></p>
<p>Starting with Hazelcast 3.7, you can get rid of the boilerplate code to begin, commit or rollback transactions by using <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast-spring/src/main/java/com/hazelcast/spring/transaction/HazelcastTransactionManager.java" target="_blank">HazelcastTransactionManager</a>
which is a <code>PlatformTransactionManager</code> implementation to be used with Spring Transaction API.</p>
<a name="sample-configuration-for-hazelcast-transaction-manager"></a><h4 id="sample-configuration-for-hazelcast-transaction-manager">Sample Configuration for Hazelcast Transaction Manager</h4>
<p>You need to register <code>HazelcastTransactionManager</code> as your transaction manager implementation and also you need to
register <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast-spring/src/main/java/com/hazelcast/spring/transaction/ManagedTransactionalTaskContext.java" target="_blank">ManagedTransactionalTaskContext</a>
to access transactional data structures within your service class.</p>
<pre><code class="lang-xml">...
&lt;hz:hazelcast id=&quot;instance&quot;&gt;
      ...
&lt;/hz:hazelcast&gt;
...
&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt;
&lt;bean id=&quot;transactionManager&quot; class=&quot;com.hazelcast.spring.transaction.HazelcastTransactionManager&quot;&gt;
    &lt;constructor-arg ref=&quot;instance&quot;/&gt;
&lt;/bean&gt;
&lt;bean id=&quot;transactionalContext&quot; class=&quot;com.hazelcast.spring.transaction.ManagedTransactionalTaskContext&quot;&gt;
    &lt;constructor-arg ref=&quot;transactionManager&quot;/&gt;
&lt;/bean&gt;
&lt;bean id=&quot;YOUR_SERVICE&quot; class=&quot;YOUR_SERVICE_CLASS&quot;&gt;
    &lt;property name=&quot;transactionalTaskContext&quot; ref=&quot;transactionalContext&quot;/&gt;
&lt;/bean&gt;
...
</code></pre>
<a name="sample-transactional-method"></a><h4 id="sample-transactional-method">Sample Transactional Method</h4>
<pre><code class="lang-java">public class ServiceWithTransactionalMethod {

    private TransactionalTaskContext transactionalTaskContext;

    @Transactional
    public void transactionalPut(String key, String value) {
        transactionalTaskContext.getMap(&quot;testMap&quot;).put(key, value);
    }

    ...
}
</code></pre>
<p>After marking your method as <code>Transactional</code> either declaratively or by annotation and accessing the data structure
through the <code>TransactionalTaskContext</code>, <code>HazelcastTransactionManager</code> will begin, commit or rollback the transaction for you.</p>
<a name="best-practices"></a><h3 id="best-practices">Best Practices</h3>
<p>Spring tries to create a new <code>Map</code>/<code>Collection</code> instance and fill the new instance by iterating and converting values of the original <code>Map</code>/<code>Collection</code> (<code>IMap</code>, <code>IQueue</code>, etc.) to required types when generic type parameters of the original <code>Map</code>/<code>Collection</code> and the target property/attribute do not match.</p>
<p>Since Hazelcast <code>Map</code>s/<code>Collection</code>s are designed to hold very large data which a single machine cannot carry, iterating through whole values can cause out of memory errors.</p>
<p>To avoid this issue, the target property/attribute can be declared as un-typed <code>Map</code>/<code>Collection</code> as shown below.</p>
<pre><code class="lang-java">public class SomeBean {
  @Autowired
  IMap map; // instead of IMap&lt;K, V&gt; map

  @Autowired
  IQueue queue; // instead of IQueue&lt;E&gt; queue

  ...
}
</code></pre>
<p>Or, parameters of injection methods (constructor, setter) can be un-typed as shown below.</p>
<pre><code class="lang-java">public class SomeBean {

  IMap&lt;K, V&gt; map;

  IQueue&lt;E&gt; queue;

  // Instead of IMap&lt;K, V&gt; map
  public SomeBean(IMap map) {
    this.map = map;
  }

  ...

  // Instead of IQueue&lt;E&gt; queue
  public void setQueue(IQueue queue) {
    this.queue = queue;
  }
  ...
}
</code></pre>
<p><br> </br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>For more information please see <a href="https://jira.springsource.org/browse/SPR-3407" target="_blank">Spring issue-3407</a>.</em></p>

<a name="storage"></a><h1 id="storage">Storage</h1>
<p>This chapter describes Hazelcast&#39;s High-Density Memory Store and Hot Restart Persistence features along with their configurations, and gives recommendations on the storage sizing.</p>
<a name="high-density-memory-store"></a><h2 id="high-density-memory-store">High-Density Memory Store</h2>
<p><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>
<br></br></p>
<p>By default, data structures in Hazelcast store data on heap in serialized form for highest data compaction; yet, these data structures are still subject to Java Garbage Collection (GC). Modern hardware has much more available memory. If you want to make use of that hardware and scale up by specifying higher heap sizes, GC becomes an increasing problem: the application faces long GC pauses that make the application unresponsive. Also, you may get out of memory errors if you fill your whole heap. Garbage collection, which is the automatic process that manages applications runtime memory, often forces you into configurations where multiple JVMs with small heaps (sizes of 2-4GB per member) run on a single physical hardware device to avoid garbage collection pauses. This results in oversized clusters to hold the data and leads to performance level requirements.</p>
<p>In <font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>, the High-Density Memory Store is Hazelcasts new enterprise in-memory storage solution. It solves garbage collection limitations so that applications can exploit hardware memory more efficiently without the need of oversized clusters. High-Density Memory Store is designed as a pluggable memory manager which enables multiple memory stores for different data structures. These memory stores are all accessible by a common access layer that scales up to terabytes of the main memory on a single JVM by minimizing the GC pressure. High-Density Memory Store enables predictable application scaling and boosts performance and latency while minimizing garbage collection pauses.</p>
<p>This foundation includes, but is not limited to, storing keys and values next to the heap in a native memory region.</p>
<p>High-Density Memory Store is currently provided for the following Hazelcast features and implementations:</p>
<ul>
<li><a href="#using-high-density-memory-store-with-map">Map</a> and <a href="#using-high-density-memory-store-with-near-cache">near cache</a></li>
<li>JCache Implementation</li>
<li><a href="#hot-restart-persistence">Hot Restart Persistence</a></li>
<li><a href="#using-high-density-memory-store-with-java-client">Java Client</a>, when using the near cache for client</li>
<li><a href="#using-high-density-memory-store">Web Session Replications</a></li>
<li>Hibernate 2nd Level Caching</li>
</ul>

<a name="configuring-high-density-memory-store"></a><h3 id="configuring-high-density-memory-store">Configuring High-Density Memory Store</h3>
<p>To use the High-Density memory storage, the native memory usage must be enabled using the programmatic or declarative configuration.
Also, you can configure its size, memory allocator type, minimum block size, page size and metadata space percentage.</p>
<ul>
<li><strong>size:</strong> Size of the total native memory to allocate. Default value is <strong>512 MB</strong>.</li>
<li><p><strong>allocator type</strong>: Type of the memory allocator. Available values are as follows:</p>
<ul>
<li><p><strong>STANDARD</strong>: This option is used internally by Hazelcast&#39;s POOLED allocator type or for debugging/testing purposes.</p>
<ul>
<li>With this option, the memory is allocated or deallocated using your operating system&#39;s default memory manager. <ul>
<li>It uses GNU C Library&#39;s standard <code>malloc()</code> and <code>free()</code> methods which are subject to contention on multithreaded/multicore systems.</li>
<li>Memory operations may become slower when you perform a lot of small allocations and deallocations. </li>
<li>It may cause large memory fragmentations, unless you use a method in the background that emphasizes fragmentation avoidance, such as <code>jemalloc()</code>. Note that a large memory fragmentation can trigger the Linux Out of Memory Killer if there is no swap space enabled in your system. Even if the swap space is enabled, the killer can be again triggered if there is not enough swap space left. </li>
<li>If you still want to use the operating system&#39;s default memory management, you can set the allocator type to STANDARD in your native memory configuration.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>POOLED</strong>: This is the default option, Hazelcast&#39;s own pooling memory allocator.</p>
<ul>
<li>With this option, memory blocks are managed using internal memory pools. </li>
<li>It allocates memory blocks, each of which has a 4MB page size by default, and splits them into chunks or merges them to create larger chunks when required. Sizing of these chunks follows the <a href="https://en.wikipedia.org/wiki/Buddy_memory_allocation">buddy memory allocation</a> algorithm, i.e. power-of-two sizing.</li>
<li>It never frees memory blocks back to the operating system. It marks disposed memory blocks as available to be used later, meaning that these blocks are reusable.</li>
<li>Memory allocation and deallocation operations (except the ones requiring larger sizes than the page size) do not interact with the operating system mostly.</li>
<li>For memory allocation, it tries to find the requested memory size inside the internal memory pools. If it cannot be found, then it interacts with the operating system. </li>
</ul>
</li>
</ul>
</li>
<li><p><strong>minimum block size:</strong> Minimum size of the blocks in bytes to split and fragment a page block to assign to an allocation request. It is used only by the <strong>POOLED</strong> memory allocator. Default value is <strong>16</strong>.</p>
</li>
<li><strong>page size:</strong> Size of the page in bytes to allocate memory as a block. It is used only by the <strong>POOLED</strong> memory allocator. Default value is <code>1 &lt;&lt; 22</code> = <strong>4194304 Bytes</strong>, about <strong>4 MB</strong>.</li>
<li><strong>metadata space percentage:</strong> Defines the percentage of the allocated native memory that is used for internal memory structures by the High-Density Memory for tracking the used and available memory blocks. It is used only by the <strong>POOLED</strong> memory allocator. Default value is <strong>12.5</strong>.</li>
</ul>
<p>The following is the programmatic configuration example.</p>
<pre><code class="lang-java">MemorySize memorySize = new MemorySize(512, MemoryUnit.MEGABYTES);
NativeMemoryConfig nativeMemoryConfig =
                new NativeMemoryConfig()
                        .setAllocatorType(NativeMemoryConfig.MemoryAllocatorType.POOLED)
                        .setSize(memorySize)
                        .setEnabled(true)
                        .setMinBlockSize(16)
                        .setPageSize(1 &lt;&lt; 20);
</code></pre>
<p>The following is the declarative configuration example.</p>
<pre><code class="lang-xml">&lt;native-memory enabled=&quot;true&quot; allocator-type=&quot;POOLED&quot;&gt;
  &lt;size value=&quot;512&quot; unit=&quot;MEGABYTES&quot;/&gt;
&lt;/native-memory&gt;
</code></pre>

<a name="sizing-practices"></a><h2 id="sizing-practices">Sizing Practices</h2>
<p>Data in Hazelcast is both active data and backup data for high availability, so the total memory footprint is the size of active data plus the size of backup data. If you use a single backup, it means the total memory footprint is two times the active data (active data + backup data). If you use, for example, two backups, then the total memory footprint is three times the active data (active data + backup data + backup data).</p>
<p>If you use only heap memory, each Hazelcast member with a 4 GB heap should accommodate a maximum of 3.5 GB of total data (active and backup). If you use the High-Density Memory Store, up to 75% of the configured physical memory footprint may be used for active and backup data, with headroom of 25% for normal memory fragmentation. In both cases, however, you should also keep some memory headroom available to handle any member failure or explicit member shutdown. When a member leaves the cluster, the data previously owned by the newly offline member will be distributed among the remaining members. For this reason, we recommend that you plan to use only 60% of available memory, with 40% headroom to handle member failure or shutdown.</p>

<a name="hot-restart-persistence"></a><h2 id="hot-restart-persistence">Hot Restart Persistence</h2>
<p><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>
<br></br></p>
<p>This chapter explains the Hazelcast&#39;s Hot Restart Persistence feature, introduced with Hazelcast 3.6. Hot Restart Persistence provides fast cluster restarts by storing the states of the cluster members on the disk. This feature is currently provided for the Hazelcast map data structure and the Hazelcast JCache implementation.</p>
<a name="hot-restart-persistence-overview"></a><h3 id="hot-restart-persistence-overview">Hot Restart Persistence Overview</h3>
<p>Hot Restart Persistence enables you to get your cluster up and running swiftly after a cluster restart. A restart can be caused by a planned shutdown (including rolling upgrades) or a sudden cluster-wide crash (e.g. power outage). For Hot Restart Persistence, required states for Hazelcast clusters and members are introduced. Please refer to the <a href="#managing-cluster-and-member-states">Managing Cluster and Member States section</a> for information on the cluster and member states.</p>
<a name="hot-restart-types"></a><h4 id="hot-restart-types">Hot Restart Types</h4>
<p>The Hot Restart feature is supported for the following restart types:</p>
<ul>
<li><p><strong>Restart after a planned shutdown</strong>:</p>
<ul>
<li><p>The cluster is shutdown completely and restarted with the exact same previous setup and data.</p>
<p>  You can shutdown the cluster completely using the method <code>HazelcastInstance.getCluster().shutdown()</code> or you can manually change the cluster state to <code>PASSIVE</code> and then shut down each member one by one. When you send the command to shut the cluster down, i.e. <code>HazelcastInstance.getCluster().shutdown()</code>, the members that are not in the <code>PASSIVE</code> state change their states to <code>PASSIVE</code>. Then, each member shuts itself down by calling the method <code>HazelcastInstance.shutdown()</code>.</p>
</li>
<li><p>Rolling upgrade: The cluster is restarted intentionally member by member. For example, this could be done to install an operating system patch or new hardware.</p>
<p>  To be able to shutdown the cluster member by member as part of a planned restart, each member in the cluster should be in the <code>FROZEN</code> or <code>PASSIVE</code> state. After the cluster state is changed to <code>FROZEN</code> or <code>PASSIVE</code>, you can manually shutdown each member by calling the method <code>HazelcastInstance.shutdown()</code>. When that member is restarted, it will rejoin the running cluster. After all members are restarted, the cluster state can be changed back to <code>ACTIVE</code>.</p>
</li>
</ul>
</li>
<li><p><strong>Restart after a cluster crash</strong>: The cluster is restarted after all its members crashed at the same time due to a power outage, networking interruptions, etc.</p>
</li>
</ul>
<a name="the-restart-process"></a><h4 id="the-restart-process">The Restart Process</h4>
<p>During the restart process, each member waits to load data until all the members in the partition table are started. During this process, no operations are allowed. Once all cluster members are started, Hazelcast changes the cluster state to <code>PASSIVE</code> and starts to load data. When all data is loaded, Hazelcast changes the cluster state to its previous known state before shutdown and starts to accept the operations which are allowed by the restored cluster state.</p>
<p>If a member fails to either start, join the cluster in time (within the timeout), or load its data, then that member will be terminated immediately. After the problems causing the failure are fixed, that member can be restarted. If the cluster start cannot be completed in time, then all members will fail to start. Please refer to the <a href="#configuring-hot-restart">Configuring Hot Restart section</a> for defining timeouts.</p>
<p>In the case of a restart after a cluster crash, the Hot Restart feature realizes that it was not a clean shutdown and Hazelcast tries to restart the cluster with the last saved data following the process explained above. In some cases, specifically when the cluster crashes while it has an ongoing partition migration process, currently it is not possible to restore the last saved state.</p>
<a name="force-start"></a><h4 id="force-start">Force Start</h4>
<p>A member can crash permanently and then be unable to recover from the failure. In that case, restart process cannot be completed since some of the members do not start or fail to load their own data. In that case, you can force the cluster to clean its persisted data and make a fresh start. This process is called <strong>force start</strong>. </p>
<p>You can trigger the force start process using the Management Center, REST API and cluster management scripts. Force start process is managed by the master member. Therefore, you should trigger the force start on master member.</p>
<p>Please refer to the <a href="#hot-restart">Hot Restart functionality</a> of the Management Center section to learn how you can perform a force start using the Management Center.</p>
<a name="configuring-hot-restart"></a><h3 id="configuring-hot-restart">Configuring Hot Restart</h3>
<p>You can configure Hot Restart programmatically or declaratively. The configuration includes elements to enable/disable the feature, to specify the directory where the Hot Restart data will be stored, and to define timeout values.</p>
<a name="hot-restart-configuration-elements"></a><h4 id="hot-restart-configuration-elements">Hot Restart Configuration Elements</h4>
<p>The following are the descriptions of the Hot Restart configuration elements.</p>
<ul>
<li><code>hot-restart-persistence</code>: The configuration that enables the Hot Restart feature. It includes the element <code>base-dir</code> that is used to specify the directory where the Hot Restart data will be stored. The default value for <code>base-dir</code> is <code>hot-restart</code>. You can use the default value, or you can specify the value of another folder containing the Hot Restart configuration, but it is mandatory that this <code>hot-restart</code> element has a value. This directory will be created automatically if it does not exist.</li>
<li><code>validation-timeout-seconds</code>: Validation timeout for the Hot Restart process when validating the cluster members expected to join and the partition table on the whole cluster.</li>
<li><code>data-load-timeout-seconds</code>: Data load timeout for the Hot Restart process. All members in the cluster should finish restoring their local data before this timeout.</li>
<li><code>hot-restart</code>: The configuration that enables or disables the Hot Restart feature per data structure. This element is used for the supported data structures (in the above examples, you can see that it is included in <code>map</code> and <code>cache</code>). Turning on <code>fsync</code> guarantees that data is persisted to the disk device when a write operation returns successful response to the caller. By default, <code>fsync</code> is turned off. That means data will be persisted to the disk device eventually, instead of on every disk write. This generally provides better performance.</li>
</ul>
<a name="hot-restart-configuration-examples"></a><h4 id="hot-restart-configuration-examples">Hot Restart Configuration Examples</h4>
<p>The following are example configurations for a Hazelcast map and JCache implementation.</p>
<p><strong>Declarative Configuration</strong>:</p>
<p>An example configuration is shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
   ...
   &lt;hot-restart-persistence enabled=&quot;true&quot;&gt;
       &lt;base-dir&gt;/mnt/hot-restart&lt;/base-dir&gt;
       &lt;validation-timeout-seconds&gt;120&lt;/validation-timeout-seconds&gt;
       &lt;data-load-timeout-seconds&gt;900&lt;/data-load-timeout-seconds&gt;
   &lt;/hot-restart-persistence&gt;
   ...
   &lt;map&gt;
       &lt;hot-restart enabled=&quot;true&quot;&gt;
           &lt;fsync&gt;false&lt;/fsync&gt;
       &lt;/hot-restart&gt;
   &lt;/map&gt;
   ...
   &lt;cache&gt;
       &lt;hot-restart enabled=&quot;true&quot;&gt;
           &lt;fsync&gt;false&lt;/fsync&gt;
       &lt;/hot-restart&gt;
   &lt;/cache&gt;
   ...
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic Configuration</strong>:</p>
<p>The programmatic equivalent of the above declarative configuration is shown below.</p>
<pre><code class="lang-java">HotRestartPersistenceConfig hotRestartPersistenceConfig = new HotRestartPersistenceConfig();
hotRestartPersistenceConfig.setEnabled(true);
hotRestartPersistenceConfig.setBaseDir(new File(&quot;/mnt/hot-restart&quot;));
hotRestartPersistenceConfig.setValidationTimeoutSeconds(120);
hotRestartPersistenceConfig.setDataLoadTimeoutSeconds(900);
config.setHotRestartPersistenceConfig(hotRestartPersistenceConfig);

...
MapConfig mapConfig = new MapConfig();
mapConfig.getHotRestartConfig().setEnabled(true);

...
CacheConfig cacheConfig = new CacheConfig();
cacheConfig.getHotRestartConfig().setEnabled(true);
</code></pre>
<a name="hot-restart-and-ip-address-port"></a><h3 id="hot-restart-and-ip-address-port">Hot Restart and IP Address-Port</h3>
<p>Hazelcast relies on the IP address-port pair as a unique identifier for a cluster member. The member must restart with these address-port settings the same as before shutdown. Otherwise, Hot Restart fails.</p>
<a name="hot-restart-persistence-design-details"></a><h3 id="hot-restart-persistence-design-details">Hot Restart Persistence Design Details</h3>
<p>Hazelcast&#39;s Hot Restart Persistence uses the log-structured storage approach. The following is a top-level design description:</p>
<ul>
<li>The only kind of update operation on persistent data is <em>appending</em>.</li>
<li>What is appended are facts about events that happened to the data model represented by the store; either a new value was assigned to a key or a key was removed.</li>
<li>Each record associated with a key makes stale the previous record that was associated with that key.</li>
<li>Stale records contribute to the amount of <em>garbage</em> present in the persistent storage.</li>
<li>Measures are taken to remove garbage from the storage.</li>
</ul>
<p>This kind of design focuses almost all of the system&#39;s complexity into the garbage collection (GC) process, stripping down the client&#39;s operation to the bare necessity of guaranteeing persistent behavior: a simple file append operation. Consequently, the latency of operations is close to the theoretical minimum in almost all cases. Complications arise only during prolonged periods of maximum load; this is where the details of the GC process begin to matter.</p>
<a name="concurrent-incremental-generational-gc"></a><h3 id="concurrent-incremental-generational-gc">Concurrent, Incremental, Generational GC</h3>
<p>In order to maintain the lowest possible footprint in the update operation latency, the following properties are built into the garbage collection process:</p>
<ul>
<li>A dedicated thread performs the GC. In Hazelcast terms, this thread is called the Collector and the application thread is called the Mutator.
<br></br></li>
<li>On each update there is metadata to be maintained; this is done asynchronously by the Collector thread. The Mutator enqueues update events to the Collector&#39;s work queue.
<br></br></li>
<li>The Collector keeps draining its work queue at all times, including the time it goes through the GC cycle. Updates are taken into account at each stage in the GC cycle, preventing the copying of already dead records into compacted files.
<br></br></li>
<li>All GC-induced I/O competes for the same resources as the Mutator&#39;s update operations. Therefore, measures are taken to minimize the impact of I/O done during GC:<ul>
<li>data is never read from files, but from RAM;</li>
<li>a heuristic scheme is employed which minimizes the number of bytes written to disk for each kilobyte of reclaimed garbage;</li>
<li>measures are also taken to achieve a good interleaving of Collector and Mutator operations, minimizing latency outliers perceived by the Mutator.</li>
</ul>
</li>
</ul>
<a name="io-minimization-scheme"></a><h4 id="i-o-minimization-scheme">I/O Minimization Scheme</h4>
<p>The success of this scheme is subject to a bet on the Weak Generational Garbage Hypothesis, which states that a new record entering the system is likely to become garbage soon. In other words, a key updated now is more likely than average to be updated again soon.</p>
<p>The scheme was taken from the seminal Sprite LFS paper, <a href="http://www.cs.berkeley.edu/~brewer/cs262/LFS.pdf">Rosenblum, Ousterhout, <em>The Design and Implementation of a Log-Structured File System</em></a>. The following is an outline of the paper:</p>
<ul>
<li>Data is not written to one huge file, but to many files of moderate size (8 MB) called &quot;chunks&quot;.</li>
<li>Garbage is collected incrementally, i.e. by choosing several chunks, then copying all their live data to new chunks, then deleting the old ones.</li>
<li>I/O is minimized using a collection technique which results in a bimodal distribution of chunks with respect to their garbage content: most files are either almost all live data or they are all garbage.</li>
<li>The technique consists of two main principles:<ol>
<li>Chunks are selected based on their <em>Cost-Benefit factor</em> (see below).</li>
<li>Records are sorted by age before copying to new chunks.</li>
</ol>
</li>
</ul>
<a name="cost-benefit-factor"></a><h4 id="cost-benefit-factor">Cost-Benefit Factor</h4>
<p>The Cost-Benefit factor of a chunk consists of two components multiplied together:</p>
<ol>
<li>The ratio of benefit (amount of garbage that can be collected) to I/O cost (amount of live data to be written).</li>
<li>The age of the data in the chunk, measured as the age of the youngest record it contains.</li>
</ol>
<p>The essence is in the second component: given equal amount of garbage in all chunks, it will make the young ones less attractive to the Collector. Assuming the generational garbage hypothesis, this will allow the young chunks to quickly accumulate more garbage. On the flip side, it will also ensure that even files with little garbage are eventually garbage collected. This removes garbage which would otherwise linger on, thinly spread across many chunk files.</p>
<p>Sorting records by age will group young records together in a single chunk and will do the same for older records. Therefore the chunks will either tend to keep their data live for a longer time, or quickly become full of garbage.</p>
<a name="hot-restart-performance-considerations"></a><h3 id="hot-restart-performance-considerations">Hot Restart Performance Considerations</h3>
<p>In this section you can find performance test summaries which are results of benchmark tests performed with a single Hazelcast member running on a physical server and on AWS R3. </p>
<a name="performance-on-a-physical-server"></a><h4 id="performance-on-a-physical-server">Performance on a Physical Server</h4>
<p>The member has the following:</p>
<ul>
<li>An IMap data structure with High-Density Memory Store. </li>
<li>Its data size is changed for each test, started from 10 GB to 500 GB (each map entry has a value of 1 KB). </li>
</ul>
<p>The tests investigate the write and read performance of Hot Restart Persistence and are performed on HP ProLiant servers with RHEL 7 operating system using Hazelcast Simulator. </p>
<p>The following are the specifications of the server hardware used for the test:</p>
<ul>
<li>CPU: 2x Intel(R) Xeon(R) CPU E5-2687W v3 @ 3.10GHz  with 10 cores per processor. Total 20 cores, 40 with hyper threading enabled.</li>
<li>Memory: 768GB 2133 MHz memory 24x HP 32GB 4Rx4 PC4-2133P-L Kit</li>
</ul>
<p>The following are the storage media used for the test:</p>
<ul>
<li>A hot-pluggable 2.5 inch HDD with 1 TB capacity and 10K RPM.</li>
<li>An SSD, Light Endurance PCle Workload Accelerator.</li>
</ul>
<p>The below table shows the test results.</p>
<p><img src="images/HotRestartPerf.png" alt="image"></p>
<a name="performance-on-aws-r3"></a><h4 id="performance-on-aws-r3">Performance on AWS R3</h4>
<p>The member has the following:</p>
<ul>
<li>An IMap data structure with High-Density Memory Store. </li>
<li>IMap has 40 million distinct keys, each map entry is 1 KB. </li>
<li>High-Density Memory Store is 59 GiB whose 19% is metadata. </li>
<li>Hot Restart is configured with <code>fsync</code> turned off. </li>
<li>Data size reloaded on restart is 38 GB.</li>
</ul>
<p>The tests investigate the write and read performance of Hot Restart Persistence and are performed on R3.2xlarge and R3.4xlarge EC2 instances using Hazelcast Simulator.</p>
<p>The following are the AWS storage types used for the test:</p>
<ul>
<li>Elastic Block Storage (EBS) General Purpose SSD (GP2).</li>
<li>Elastic Block Storage with Provisioned IOPS (IO1). Provisioned 10,000 IOPS on a 340 GiB volume, enabled EBS-optimized on instance.</li>
<li>SSD-backed instance store.</li>
</ul>
<p>The below table shows the test results.</p>
<p><img src="images/HotRestartPerf2.png" alt="image"></p>

<a name="hazelcast-java-client"></a><h1 id="hazelcast-java-client">Hazelcast Java Client</h1>
<p>There are currently three ways to connect to a running Hazelcast cluster:</p>
<ul>
<li>Native Clients (Java, C++, .NET)</li>
<li><a href="#memcache-client">Memcache Client</a></li>
<li><a href="#rest-client">REST Client</a></li>
</ul>
<p>Native Clients enable you to perform almost all Hazelcast operations without being a member of the cluster. It connects to one of the cluster members and delegates all cluster wide operations to it (<em>dummy client</em>), or it connects to all of them and delegates operations smartly (<em>smart client</em>). When the relied cluster member dies, the client will transparently switch to another live member.</p>
<p>Hundreds or even thousands of clients can be connected to the cluster. By default, there are <em>core count</em> <em> </em>10* threads on the server side that will handle all the requests (e.g. if the server has 4 cores, there will be 40 threads).</p>
<p>Imagine a trading application where all the trading data are stored and managed in a Hazelcast cluster with tens of members. Swing/Web applications at the traders&#39; desktops can use Native Clients to access and modify the data in the Hazelcast cluster.</p>
<p>Currently, Hazelcast has Native Java, C++ and .NET Clients available. This chapter describes the Java Client.</p>
<p><br><br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>IMPORTANT:</em></strong> <em>Starting with the Hazelcast 3.6, a new Java Native Client Library is introduced in the release package: <code>hazelcast-client-new-&lt;version&gt;.jar</code>. This library contains clients which use the new Hazelcast Binary Client Protocol. This library does not exist for the releases before 3.5. For 3.5, it can be used experimentally.</em></p>
<p><br><br></p>
<a name="hazelcast-clients-feature-comparison"></a><h2 id="hazelcast-clients-feature-comparison">Hazelcast Clients Feature Comparison</h2>
<p>Before detailing the Java Client, this section provides the below comparison matrix to show which features are supported by the Hazelcast <strong>native</strong> clients.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Java Client</th>
<th>.NET Client</th>
<th>C++ Client</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Queue</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Set</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>List</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>MultiMap</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Replicated Map</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Topic</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>MapReduce</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Lock</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Semaphore</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>AtomicLong</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>AtomicReference</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>IdGenerator</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>CountDownLatch</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Transactional Map</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Transactional MultiMap</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Transactional Queue</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Transactional List</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Transactional Set</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>JCache</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Ringbuffer</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Reliable Topic</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Hot Restart</td>
<td>Yes (with a near cache)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Client Configuration Import</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Hazelcast Client Protocol</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Fail Fast on Invalid Conviguration</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Sub-Listener Interfaces for Map ListenerMap</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Continuous Query</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Listener with Predicate</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Distributed Executor Service</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Query</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Near Cache</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Heartbeat</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Declarative Configuration</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Programmatic Configuration</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>SSL Support</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>XA Transactions</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Smart Client</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Dummy Client</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Lifecycle Service</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Event Listeners</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>DataSerializable</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>IdentifiedDataSerializable</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Portable</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><br><br></p>

<a name="java-client-overview"></a><h2 id="java-client-overview">Java Client Overview</h2>
<p>The Java client is the most full featured Hazelcast native client. It is offered both with Hazelcast and Hazelcast Enterprise.  The main idea behind the Java client is to provide the same Hazelcast functionality by proxying each operation through a Hazelcast member. It can access and change distributed data, and it can listen to distributed events of an already established Hazelcast cluster from another Java application.</p>
<a name="including-dependencies-for-java-clients"></a><h3 id="including-dependencies-for-java-clients">Including Dependencies for Java Clients</h3>
<p>You should include two dependencies in your classpath to start using the Hazelcast client: <code>hazelcast.jar</code> and <code>hazelcast-client.jar</code>.</p>
<p>After adding these dependencies, you can start using the Hazelcast client as if you are using the Hazelcast API. The differences are discussed in the below sections.</p>
<p>If you prefer to use maven, add the following lines to your <code>pom.xml</code>.</p>
<pre><code class="lang-xml">&lt;dependency&gt;
    &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
    &lt;artifactId&gt;hazelcast-client&lt;/artifactId&gt;
    &lt;version&gt;$LATEST_VERSION$&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.hazelcast&lt;/groupId&gt;
    &lt;artifactId&gt;hazelcast&lt;/artifactId&gt;
    &lt;version&gt;$LATEST_VERSION$&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<a name="getting-started-with-client-api"></a><h3 id="getting-started-with-client-api">Getting Started with Client API</h3>
<p>The first step is configuration. You can configure the Java client declaratively or programmatically. We will use the programmatic approach throughout this tutorial. Please refer to the <a href="#configuring-java-client">Java Client Declarative Configuration section</a> for details.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.getGroupConfig().setName(&quot;dev&quot;).setPassword(&quot;dev-pass&quot;);
clientConfig.getNetworkConfig().addAddress(&quot;10.90.0.1&quot;, &quot;10.90.0.2:5702&quot;);
</code></pre>
<p>The second step is to initialize the HazelcastInstance to be connected to the cluster.</p>
<pre><code class="lang-java">HazelcastInstance client = HazelcastClient.newHazelcastClient(clientConfig);
</code></pre>
<p><em>This client interface is your gateway to access all Hazelcast distributed objects.</em></p>
<p>Let&#39;s create a map and populate it with some data.</p>
<pre><code class="lang-java">
IMap&lt;String, Customer&gt; mapCustomers = client.getMap(&quot;customers&quot;); //creates the map proxy

mapCustomers.put(&quot;1&quot;, new Customer(&quot;Joe&quot;, &quot;Smith&quot;));
mapCustomers.put(&quot;2&quot;, new Customer(&quot;Ali&quot;, &quot;Selam&quot;));
mapCustomers.put(&quot;3&quot;, new Customer(&quot;Avi&quot;, &quot;Noyan&quot;));
</code></pre>
<p>As a final step, if you are done with your client, you can shut it down as shown below. This will release all the used resources and will close connections to the cluster.</p>
<pre><code class="lang-java">
client.shutdown();
</code></pre>
<a name="java-client-operation-modes"></a><h3 id="java-client-operation-modes">Java Client Operation Modes</h3>
<p>The client has two operation modes because of the distributed nature of the data and cluster.</p>
<p><strong>Smart Client</strong>: In smart mode, clients connect to each cluster member. Since each data partition uses the well known and consistent hashing algorithm, each client can send an operation to the relevant cluster member, which increases the overall throughput and efficiency. Smart mode is the default mode.</p>
<p><strong>Dummy Client</strong>: For some cases, the clients can be required to connect to a single member instead of to each member in the cluster. Firewalls, security, or some custom networking issues can be the reason for these cases.</p>
<p>In dummy client mode, the client will only connect to one of the configured addresses. This single member will behave as a gateway to the other members. For any operation requested from the client, it will redirect the request to the relevant member and return the response back to the client returned from this member.</p>
<a name="handling-failures"></a><h3 id="handling-failures">Handling Failures</h3>
<p>There are two main failure cases you should be aware of, and configurations you can perform to achieve proper behavior.</p>
<a name="handling-client-connection-failure"></a><h4 id="handling-client-connection-failure">Handling Client Connection Failure</h4>
<p>While the client is trying to connect initially to one of the members in the <code>ClientNetworkConfig.addressList</code>, all the members might be not available. Instead of giving up, throwing an exception and stopping the client, the client will retry as many as <code>connectionAttemptLimit</code> times. </p>
<p>You can configure <code>connectionAttemptLimit</code> for the number of times you want the client to retry connecting. Please see <a href="#setting-connection-attempt-limit">Setting Connection Attempt Limit</a>.</p>
<p>The client executes each operation through the already established connection to the cluster. If this connection(s) disconnects or drops, the client will try to reconnect as configured.</p>
<a name="handling-retry-able-operation-failure"></a><h4 id="handling-retry-able-operation-failure">Handling Retry-able Operation Failure</h4>
<p>While sending the requests to related members, operations can fail due to various reasons. Read-only operations are retried by default. If you want to enable retry for the other operations, set the <code>redoOperation</code> to <code>true</code>. Please see <a href="#enabling-redo-operation">Enabling Redo Operation</a>.</p>
<p>You can set a timeout for retrying the operations sent to a member. This can be provided by using the property <code>hazelcast.client.invocation.timeout.seconds</code> in <code>ClientProperties</code>. The client will retry an operation within this given period, of course, if it is a read-only operation or you enabled the <code>redoOperation</code> as stated in the above paragraph. This timeout value is important when there is a failure resulted by either of the following causes: </p>
<ul>
<li>Member throws an exception.</li>
<li>Connection between the client and member is closed.</li>
<li>Client&#39;s heartbeat requests are timed out.</li>
</ul>
<p>Please see the <a href="#client-system-properties">Client System Properties section</a>.</p>
<a name="using-supported-distributed-data-structures"></a><h3 id="using-supported-distributed-data-structures">Using Supported Distributed Data Structures</h3>
<p>Most of the Distributed Data Structures are supported by the Java client. When you use clients in other languages, you should check for the exceptions.</p>
<p>As a general rule, you configure these data structures on the server side and access them through a proxy on the client side.</p>
<a name="using-map-with-java-client"></a><h4 id="using-map-with-java-client">Using Map with Java Client</h4>
<p>You can use any <a href="#map">Distributed Map</a> object with the client, as shown below.</p>
<pre><code class="lang-java">Imap&lt;Integer, String&gt; map = client.getMap(myMap);

map.put(1, Ali);
String value= map.get(1);
map.remove(1);
</code></pre>
<p>Locality is ambiguous for the client, so <code>addLocalEntryListener</code> and <code>localKeySet</code> are not supported. Please see the <a href="#map">Distributed Map section</a> for more information.</p>
<a name="using-multimap-with-java-client"></a><h4 id="using-multimap-with-java-client">Using MultiMap with Java Client</h4>
<p>A MultiMap usage example is shown below.</p>
<pre><code class="lang-java">MultiMap&lt;Integer, String&gt; multiMap = client.getMultiMap(&quot;myMultiMap&quot;);

multiMap.put(1,ali);
multiMap.put(1,veli);

Collection&lt;String&gt; values = multiMap.get(1);
</code></pre>
<p><code>addLocalEntryListener</code>, <code>localKeySet</code> and  <code>getLocalMultiMapStats</code> are not supported because locality is ambiguous for the client. Please see the <a href="#multimap">Distributed MultiMap section</a> for more information.</p>
<a name="using-queue-with-java-client"></a><h4 id="using-queue-with-java-client">Using Queue with Java Client</h4>
<p>A sample usage is shown below.</p>
<pre><code class="lang-java">IQueue&lt;String&gt; myQueue = client.getQueue(theQueue);
myQueue.offer(ali)
</code></pre>
<p><code>getLocalQueueStats</code> is not supported because locality is ambiguous for the client. Please see the <a href="#queue">Distributed Queue section</a> for more information.</p>
<a name="using-topic-with-java-client"></a><h4 id="using-topic-with-java-client">Using Topic with Java Client</h4>
<p><code>getLocalTopicStats</code> is not supported because locality is ambiguous for the client.</p>
<a name="using-other-supported-distributed-structures"></a><h4 id="using-other-supported-distributed-structures">Using Other Supported Distributed Structures</h4>
<p>The distributed data structures listed below are also supported by the client. Since their logic is the same in both the member side and client side, you can refer to their sections as listed below.</p>
<ul>
<li><a href="#replicated-map">Replicated Map</a></li>
<li><a href="#mapreduce">MapReduce</a></li>
<li><a href="#list">List</a></li>
<li><a href="#set">Set</a></li>
<li><a href="#iatomiclong">IAtomicLong</a></li>
<li><a href="#iatomicreference">IAtomicReference</a></li>
<li><a href="#icountdownlatch">ICountDownLatch</a></li>
<li><a href="#isemaphore">ISemaphore</a></li>
<li><a href="#idgenerator">IdGenerator</a></li>
<li><a href="#lock">Lock</a></li>
</ul>
<a name="using-client-services"></a><h3 id="using-client-services">Using Client Services</h3>
<p>Hazelcast provides the services discussed below for some common functionalities on the client side.</p>
<a name="using-distributed-executor-service"></a><h4 id="using-distributed-executor-service">Using Distributed Executor Service</h4>
<p>The distributed executor service is for distributed computing. It can be used to execute tasks on the cluster on a designated partition or on all the partitions. It can also be used to process entries. Please see the <a href="#executor-service">Distributed Executor Service section</a> for more information.</p>
<pre><code class="lang-java">IExecutorService executorService = client.getExecutorService(&quot;default&quot;);
</code></pre>
<p>After getting an instance of <code>IExecutorService</code>, you can use the instance as the interface with the one provided on the server side. Please see the <a href="#distributed-computing">Distributed Computing chapter</a> for detailed usage.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>This service is only supported by the Java client.</em></p>
<a name="listening-to-client-connection"></a><h4 id="listening-to-client-connection">Listening to Client Connection</h4>
<p>If you need to track clients and you want to listen to their connection events, you can use the <code>clientConnected</code> and <code>clientDisconnected</code> methods of the <code>ClientService</code> class. This class must be run on the <strong>member</strong> side. The following is an example code.</p>
<pre><code class="lang-java">final ClientService clientService = hazelcastInstance.getClientService();
final Collection&lt;Client&gt; connectedClients = clientService.getConnectedClients();

clientService.addClientListener(new ClientListener() {
    @Override
    public void clientConnected(Client client) {
    //Handle client connected event
    }

    @Override
    public void clientDisconnected(Client client) {
      //Handle client disconnected event
    }
});
</code></pre>
<a name="finding-the-partition-of-a-key"></a><h4 id="finding-the-partition-of-a-key">Finding the Partition of a Key</h4>
<p>You use partition service to find the partition of a key. It will return all partitions. See the example code below.</p>
<pre><code class="lang-java">PartitionService partitionService = client.getPartitionService();

//partition of a key
Partition partition = partitionService.getPartition(key);

//all partitions
Set&lt;Partition&gt; partitions = partitionService.getPartitions();
</code></pre>
<a name="handling-lifecycle"></a><h4 id="handling-lifecycle">Handling Lifecycle</h4>
<p>Lifecycle handling performs the following:</p>
<ul>
<li>checks to see if the client is running,</li>
<li>shuts down the client gracefully,</li>
<li>terminates the client ungracefully (forced shutdown), and</li>
<li>adds/removes lifecycle listeners.</li>
</ul>
<pre><code class="lang-java">LifecycleService lifecycleService = client.getLifecycleService();

if(lifecycleService.isRunning()){
    //it is running
}

//shutdown client gracefully
lifecycleService.shutdown();
</code></pre>
<a name="client-listeners"></a><h3 id="client-listeners">Client Listeners</h3>
<p>You can configure listeners to listen to various event types on the client side. You can configure global events not relating to any distributed object through <a href="#configuring-client-listeners">Client ListenerConfig</a>. You should configure distributed object listeners like map entry listeners or list item listeners through their proxies. You can refer to the related sections under each distributed data structure in this reference manual.</p>
<a name="client-transactions"></a><h3 id="client-transactions">Client Transactions</h3>
<p>Transactional distributed objects are supported on the client side. Please see the <a href="#transactions">Transactions chapter</a> on how to use them.</p>

<a name="configuring-java-client"></a><h2 id="configuring-java-client">Configuring Java Client</h2>
<p>You can configure Hazelcast Java Client declaratively (XML) or programmatically (API).</p>
<p>For declarative configuration, the Hazelcast client looks at the following places for the client configuration file.</p>
<ul>
<li><p><strong>System property</strong>: The client first checks if <code>hazelcast.client.config</code> system property is set to a file path, e.g. <code>-Dhazelcast.client.config=C:/myhazelcast.xml</code>.</p>
</li>
<li><p><strong>Classpath</strong>: If config file is not set as a system property, the client checks the classpath for <code>hazelcast-client.xml</code> file.</p>
</li>
</ul>
<p>If the client does not find any configuration file, it starts with the default configuration (<code>hazelcast-client-default.xml</code>) located in the <code>hazelcast-client.jar</code> library. Before configuring the client, please try to work with the default configuration to see if it works for you. The default should be just fine for most users. If not, then consider custom configuration for your environment.</p>
<p>If you want to specify your own configuration file to create a <code>Config</code> object, the Hazelcast client supports the following.</p>
<ul>
<li><p><code>Config cfg = new XmlClientConfigBuilder(xmlFileName).build();</code></p>
</li>
<li><p><code>Config cfg = new XmlClientConfigBuilder(inputStream).build();</code></p>
</li>
</ul>
<p>For programmatic configuration of the Hazelcast Java Client, just instantiate a <code>ClientConfig</code> object and configure the desired aspects. An example is shown below.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.setGroupConfig(new GroupConfig(&quot;dev&quot;,&quot;dev-pass);
clientConfig.setLoadBalancer(yourLoadBalancer);
...
...
</code></pre>
<a name="configuring-client-network"></a><h3 id="configuring-client-network">Configuring Client Network</h3>
<p>All network related configuration of Hazelcast Java Client is performed via the <code>network</code> element in the declarative configuration file, or in the class <code>ClientNetworkConfig</code> when using programmatic configuration. Let&#39;s first give the examples for these two approaches. Then we will look at its sub-elements and attributes.</p>
<a name="declarative-client-network-configuration"></a><h4 id="declarative-client-network-configuration">Declarative Client Network Configuration</h4>
<p>Here is an example of configuring network for Java Client declaratively.</p>
<pre><code class="lang-xml">&lt;hazelcast-client xsi:schemaLocation=
    &quot;http://www.hazelcast.com/schema/client-config hazelcast-client-config-&lt;version&gt;.xsd&quot;
                  xmlns=&quot;http://www.hazelcast.com/schema/client-config&quot;
                  xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;
...
&lt;network&gt;
  &lt;cluster-members&gt;
    &lt;address&gt;127.0.0.1&lt;/address&gt;
    &lt;address&gt;127.0.0.2&lt;/address&gt;
  &lt;/cluster-members&gt;
  &lt;smart-routing&gt;true&lt;/smart-routing&gt;
  &lt;redo-operation&gt;true&lt;/redo-operation&gt;
  &lt;socket-interceptor enabled=&quot;true&quot;&gt;
    &lt;class-name&gt;com.hazelcast.XYZ&lt;/class-name&gt;
    &lt;properties&gt;
      &lt;property name=&quot;kerberos-host&quot;&gt;kerb-host-name&lt;/property&gt;
      &lt;property name=&quot;kerberos-config-file&quot;&gt;kerb.conf&lt;/property&gt;
    &lt;/properties&gt;
   &lt;/socket-interceptor&gt;
  &lt;aws enabled=&quot;true&quot; connection-timeout-seconds=&quot;11&quot;&gt;
    &lt;inside-aws&gt;false&lt;/inside-aws&gt;
    &lt;access-key&gt;my-access-key&lt;/access-key&gt;
    &lt;secret-key&gt;my-secret-key&lt;/secret-key&gt;
    &lt;iam-role&gt;s3access&lt;/iam-role&gt;
    &lt;region&gt;us-west-1&lt;/region&gt;
    &lt;host-header&gt;ec2.amazonaws.com&lt;/host-header&gt;
    &lt;security-group-name&gt;hazelcast-sg&lt;/security-group-name&gt;
    &lt;tag-key&gt;type&lt;/tag-key&gt;
    &lt;tag-value&gt;hz-members&lt;/tag-value&gt;
  &lt;/aws&gt;
&lt;/network&gt;
</code></pre>
<a name="programmatic-client-network-configuration"></a><h4 id="programmatic-client-network-configuration">Programmatic Client Network Configuration</h4>
<p>Here is an example of configuring network for Java Client programmatically.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientNetworkConfig networkConfig = clientConfig.getNetworkConfig();
</code></pre>
<a name="configuring-address-list"></a><h4 id="configuring-address-list">Configuring Address List</h4>
<p>Address List is the initial list of cluster addresses to which the client will connect. The client uses this list to find an alive member. Although it may be enough to give only one address of a member in the cluster (since all members communicate with each other), it is recommended that you give the addresses for all the members.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  ...
  &lt;network&gt;
    &lt;cluster-members&gt;
      &lt;address&gt;10.1.1.21&lt;/address&gt;
      &lt;address&gt;10.1.1.22:5703&lt;/address&gt;
    &lt;/cluster-members&gt;
  ...
  &lt;/network&gt;
...
&lt;/hazelcast-client&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientNetworkConfig networkConfig = clientConfig.getNetworkConfig();
networkConfig.addAddress(&quot;10.1.1.21&quot;, &quot;10.1.1.22:5703&quot;);
</code></pre>
<p>If the port part is omitted, then 5701, 5702, and 5703 will be tried in random order.</p>
<p>You can provide multiple addresses with ports provided or not, as seen above. The provided list is shuffled and tried in random order. Default value is <em>localhost</em>.</p>
<a name="setting-smart-routing"></a><h4 id="setting-smart-routing">Setting Smart Routing</h4>
<p>Smart routing defines whether the client mode is smart or dummy. The following are example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
...
  &lt;smart-routing&gt;true&lt;/smart-routing&gt;
...
&lt;/network&gt;
...
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientNetworkConfig networkConfig = clientConfig.getNetworkConfig();
networkConfig().setSmartRouting(true);
</code></pre>
<p>The default is <em>smart client</em> mode.</p>
<a name="enabling-redo-operation"></a><h4 id="enabling-redo-operation">Enabling Redo Operation</h4>
<p>It enables/disables redo-able operations as described in <a href="#handling-retry-able-operation-failure">Handling Retry-able Operation Failure</a>. The following are the example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
...  
  &lt;redo-operation&gt;true&lt;/redo-operation&gt;
...
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientNetworkConfig networkConfig = clientConfig.getNetworkConfig();
networkConfig().setRedoOperation(true);
</code></pre>
<p>Default is <em>disabled</em>.</p>
<a name="setting-connection-timeout"></a><h4 id="setting-connection-timeout">Setting Connection Timeout</h4>
<p>Connection timeout is the timeout value in milliseconds for members to accept client connection requests. The following are the example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
...
  &lt;connection-timeout&gt;5000&lt;/connection-timeout&gt;
...
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.getNetworkConfig().setConnectionTimeout(5000);
</code></pre>
<p>The default value is <em>5000</em> milliseconds.</p>
<a name="setting-connection-attempt-limit"></a><h4 id="setting-connection-attempt-limit">Setting Connection Attempt Limit</h4>
<p>While the client is trying to connect initially to one of the members in the <code>ClientNetworkConfig.addressList</code>, all members might be not available. Instead of giving up, throwing an exception and stopping the client, the client will retry as many as <code>ClientNetworkConfig.connectionAttemptLimit</code> times. This is also the case when an existing client-member connection goes down. The following are example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
...
  &lt;connection-attempt-limit&gt;5&lt;/connection-attempt-limit&gt;
...
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.getNetworkConfig().setConnectionAttemptLimit(5);
</code></pre>
<p>Default value is <em>2</em>.</p>
<a name="setting-connection-attempt-period"></a><h4 id="setting-connection-attempt-period">Setting Connection Attempt Period</h4>
<p>Connection timeout period is the duration in milliseconds between the connection attempts defined by <code>ClientNetworkConfig.connectionAttemptLimit</code>. The following are example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
...
  &lt;connection-attempt-period&gt;5000&lt;/connection-attempt-period&gt;
...
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.getNetworkConfig().setConnectionAttemptPeriod(5000);
</code></pre>
<p>Default value is <em>3000</em>.</p>
<a name="setting-a-socket-interceptor"></a><h4 id="setting-a-socket-interceptor">Setting a Socket Interceptor</h4>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Following is a client configuration to set a socket intercepter. Any class implementing <code>com.hazelcast.nio.SocketInterceptor</code> is a socket interceptor.</p>
<pre><code class="lang-java">public interface SocketInterceptor {
    void init(Properties properties);
    void onConnect(Socket connectedSocket) throws IOException;
}
</code></pre>
<p><code>SocketInterceptor</code> has two steps. First, it will be initialized by the configured properties. Second, it will be informed just after the socket is connected using <code>onConnect</code>.</p>
<pre><code class="lang-java">SocketInterceptorConfig socketInterceptorConfig = clientConfig
               .getNetworkConfig().getSocketInterceptorConfig();

MyClientSocketInterceptor myClientSocketInterceptor = new MyClientSocketInterceptor();

socketInterceptorConfig.setEnabled(true);
socketInterceptorConfig.setImplementation(myClientSocketInterceptor);
</code></pre>
<p>If you want to configure the socket connector with a class name instead of an instance, see the example below.</p>
<pre><code class="lang-java">SocketInterceptorConfig socketInterceptorConfig = clientConfig
            .getNetworkConfig().getSocketInterceptorConfig();

MyClientSocketInterceptor myClientSocketInterceptor = new MyClientSocketInterceptor();

socketInterceptorConfig.setEnabled(true);

//These properties are provided to interceptor during init
socketInterceptorConfig.setProperty(&quot;kerberos-host&quot;,&quot;kerb-host-name&quot;);
socketInterceptorConfig.setProperty(&quot;kerberos-config-file&quot;,&quot;kerb.conf&quot;);

socketInterceptorConfig.setClassName(myClientSocketInterceptor);
</code></pre>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please see the <a href="#socket-interceptor">Socket Interceptor section</a> for more information.</em>
<br></br></p>
<a name="configuring-network-socket-options"></a><h4 id="configuring-network-socket-options">Configuring Network Socket Options</h4>
<p>You can configure the network socket options using <code>SocketOptions</code>. It has the following methods.</p>
<ul>
<li><p><code>socketOptions.setKeepAlive(x)</code>: Enables/disables the <em>SO_KEEPALIVE</em> socket option. The default value is <code>true</code>.</p>
</li>
<li><p><code>socketOptions.setTcpNoDelay(x)</code>: Enables/disables the <em>TCP_NODELAY</em> socket option. The default value is <code>true</code>.</p>
</li>
<li><p><code>socketOptions.setReuseAddress(x)</code>: Enables/disables the <em>SO_REUSEADDR</em> socket option. The default value is <code>true</code>.</p>
</li>
<li><p><code>socketOptions.setLingerSeconds(x)</code>: Enables/disables <em>SO_LINGER</em> with the specified linger time in seconds. The default value is <code>3</code>.</p>
</li>
<li><p><code>socketOptions.setBufferSize(x)</code>: Sets the <em>SO_SNDBUF</em> and <em>SO_RCVBUF</em> options to the specified value in KB for this Socket. The default value is <code>32</code>.</p>
</li>
</ul>
<pre><code class="lang-java">SocketOptions socketOptions = clientConfig.getNetworkConfig().getSocketOptions();
socketOptions.setBufferSize(32);
socketOptions.setKeepAlive(true);
socketOptions.setTcpNoDelay(true);
socketOptions.setReuseAddress(true);
socketOptions.setLingerSeconds(3);
</code></pre>
<a name="enabling-client-ssl"></a><h4 id="enabling-client-ssl">Enabling Client SSL</h4>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>You can use SSL to secure the connection between the client and the members. If you want SSL enabled for the client-cluster connection, you should set <code>SSLConfig</code>. Once set, the connection (socket) is established out of an SSL factory defined either by a factory class name or factory implementation. Please see the <code>SSLConfig</code> class in the <code>com.hazelcast.config</code> package at the JavaDocs page of the <a href="http://www.hazelcast.org/documentation" target="_blank">Hazelcast Documentation</a> web site.</p>
<a name="configuring-client-for-aws"></a><h4 id="configuring-client-for-aws">Configuring Client for AWS</h4>
<p>The example declarative and programmatic configurations below show how to configure a Java client for connecting to a Hazelcast cluster in AWS.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">...
&lt;network&gt;
  &lt;aws enabled=&quot;true&quot;&gt;
    &lt;inside-aws&gt;false&lt;/inside-aws&gt;
    &lt;access-key&gt;my-access-key&lt;/access-key&gt;
    &lt;secret-key&gt;my-secret-key&lt;/secret-key&gt;
    &lt;iam-role&gt;s3access&lt;/iam-role&gt;
    &lt;region&gt;us-west-1&lt;/region&gt;
    &lt;host-header&gt;ec2.amazonaws.com&lt;/host-header&gt;
    &lt;security-group-name&gt;hazelcast-sg&lt;/security-group-name&gt;
    &lt;tag-key&gt;type&lt;/tag-key&gt;
    &lt;tag-value&gt;hz-members&lt;/tag-value&gt;
  &lt;/aws&gt;
...
&lt;/network&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ClientAwsConfig clientAwsConfig = new ClientAwsConfig();
clientAwsConfig.setInsideAws( false )
               .setAccessKey( &quot;my-access-key&quot; )
               .setSecretKey( &quot;my-secret-key&quot; )
               .setRegion( &quot;us-west-1&quot; )
               .setHostHeader( &quot;ec2.amazonaws.com&quot; )
               .setSecurityGroupName( &quot;&gt;hazelcast-sg&quot; )
               .setTagKey( &quot;type&quot; )
               .setTagValue( &quot;hz-members&quot; )
               .setIamRole( &quot;s3access&quot; )
               .setEnabled( true );
clientConfig.getNetworkConfig().setAwsConfig( clientAwsConfig );
HazelcastInstance client = HazelcastClient.newHazelcastClient( clientConfig );
</code></pre>
<p>You can refer to the <a href="#aws-element">aws element section</a> for the descriptions of above AWS configuration elements except <code>inside-aws</code> and <code>iam-role</code>, which are explained below.</p>
<p>If the <code>inside-aws</code> element is not set, the private addresses of cluster members will always be converted to public addresses. Also, the client will use public addresses to connect to the members. In order to use private addresses, set the <code>inside-aws</code> parameter to <code>true</code>. Also note that, when connecting outside from AWS, setting the <code>inside-aws</code> parameter to <code>true</code> will cause the client to not be able to reach the members.</p>
<p>IAM roles are used to make secure requests from your clients. You can provide the name of your IAM role that you created previously on your AWS console using the <code>iam-role</code> or <code>setIamRole()</code> method.</p>
<a name="configuring-client-load-balancer"></a><h3 id="configuring-client-load-balancer">Configuring Client Load Balancer</h3>
<p><code>LoadBalancer</code> allows you to send operations to one of a number of endpoints (Members). Its main purpose is to determine the next <code>Member</code> if queried.  It is up to your implementation to use different load balancing policies. You should implement the interface <code>com.hazelcast.client.LoadBalancer</code> for that purpose.</p>
<p>If the client is configured in smart mode, only the operations that are not key-based will be routed to the endpoint that is returned by the <code>LoadBalancer</code>. If the client is not a smart client, <code>LoadBalancer</code> will be ignored.</p>
<p>The following are example configurations.</p>
<p><strong>Declarative</strong>:</p>
<pre><code class="lang-xml">&lt;hazelcast-client&gt;
  ...
  &lt;load-balancer type=random&gt;
    yourLoadBalancer
  &lt;/load-balancer&gt;
  ...
&lt;/hazelcast-client&gt;
</code></pre>
<p><strong>Programmatic</strong>:</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.setLoadBalancer(yourLoadBalancer);
</code></pre>
<a name="configuring-client-near-cache"></a><h3 id="configuring-client-near-cache">Configuring Client Near Cache</h3>
<p>Hazelcast distributed map has a Near Cache feature to reduce network latencies. Since the client always requests data from the cluster members, it can be helpful for some of your use cases to configure a near cache on the client side.
The client supports the same Near Cache that is used in Hazelcast distributed map.</p>
<p>You can create Near Cache on the client side by providing a configuration per map name, as shown below.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
NearCacheConfig nearCacheConfig = new NearCacheConfig();
nearCacheConfig.setName(&quot;mapName&quot;);
clientConfig.addNearCacheConfig(nearCacheConfig);
</code></pre>
<p>You can use wildcards for the map name, as shown below.</p>
<pre><code class="lang-java">nearCacheConfig.setName(&quot;map*&quot;);
nearCacheConfig.setName(&quot;*map&quot;);
</code></pre>
<p>The following is an example declarative configuration for Near Cache. </p>
<pre><code class="lang-xml">&lt;/hazelcast-client&gt;
    ...
    ...
    &lt;near-cache name=&quot;MENU&quot;&gt;
        &lt;max-size&gt;2000&lt;/max-size&gt;
        &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
        &lt;max-idle-seconds&gt;0&lt;/max-idle-seconds&gt;
        &lt;eviction-policy&gt;LFU&lt;/eviction-policy&gt;
        &lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;
        &lt;in-memory-format&gt;OBJECT&lt;/in-memory-format&gt;
    &lt;/near-cache&gt;
    ...
&lt;/hazelcast-client&gt;
</code></pre>
<p>Name of Near Cache on the client side must be the same as the name of IMap on the server for which this Near Cache is being created.</p>
<p>Near Cache can have its own <code>in-memory-format</code> which is independent of the <code>in-memory-format</code> of the servers.</p>
<a name="client-group-configuration"></a><h3 id="client-group-configuration">Client Group Configuration</h3>
<p>Clients should provide a group name and password in order to connect to the cluster.
You can configure them using <code>GroupConfig</code>, as shown below.</p>
<pre><code class="lang-java">clientConfig.setGroupConfig(new GroupConfig(&quot;dev&quot;,&quot;dev-pass&quot;));
</code></pre>
<a name="client-security-configuration"></a><h3 id="client-security-configuration">Client Security Configuration</h3>
<p>In the cases where the security established with <code>GroupConfig</code> is not enough and you want your clients connecting securely to the cluster, you can use <code>ClientSecurityConfig</code>. This configuration has a <code>credentials</code> parameter to set the IP address and UID. Please see <code>ClientSecurityConfig.java</code> in our code.</p>
<a name="client-serialization-configuration"></a><h3 id="client-serialization-configuration">Client Serialization Configuration</h3>
<p>For the client side serialization, use Hazelcast configuration. Please refer to the <a href="#serialization">Serialization chapter</a>.</p>
<a name="configuring-client-listeners"></a><h3 id="configuring-client-listeners">Configuring Client Listeners</h3>
<p>You can configure global event listeners using <code>ListenerConfig</code> as shown below.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ListenerConfig listenerConfig = new ListenerConfig(LifecycleListenerImpl);
clientConfig.addListenerConfig(listenerConfig);
</code></pre>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
ListenerConfig listenerConfig = new ListenerConfig(&quot;com.hazelcast.example.MembershipListenerImpl&quot;);
clientConfig.addListenerConfig(listenerConfig);
</code></pre>
<p>You can add three types of event listeners.</p>
<ul>
<li>LifecycleListener</li>
<li>MembershipListener</li>
<li>DistributedObjectListener</li>
</ul>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/LifecycleListener.java" target="_blank">LifecycleListener</a>, <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/MembershipListener.java" target="_blank">MembershipListener</a> and <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/DistributedObjectListener.java" target="_blank">DistributedObjectListener</a>.</em>
<br></br></p>
<a name="executorpoolsize"></a><h3 id="executorpoolsize">ExecutorPoolSize</h3>
<p>Hazelcast has an internal executor service (different from the data structure <em>Executor Service</em>) that has threads and queues to perform internal operations such as handling responses. This parameter specifies the size of the pool of threads which perform these operations laying in the executor&#39;s queue. If not configured, this parameter has the value as <strong>5 * <em>core size of the client</em></strong> (i.e. it is 20 for a machine that has 4 cores).</p>
<a name="classloader"></a><h3 id="classloader">ClassLoader</h3>
<p>You can configure a custom <code>classLoader</code>. It will be used by the serialization service and to load any class configured in configuration, such as event listeners or ProxyFactories.</p>

<a name="client-system-properties"></a><h2 id="client-system-properties">Client System Properties</h2>
<p>There are some advanced client configuration properties to tune some aspects of Hazelcast Client. You can set them as property name and value pairs through declarative configuration, programmatic configuration, or JVM system property. Please see the <a href="#system-properties">System Properties section</a> to learn how to set these properties.</p>
<p>The table below lists the client configuration properties with their descriptions.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default Value</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>hazelcast.client.event.queue.capacity</code></td>
<td style="text-align:left">1000000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Default value of the capacity of executor that handles incoming event packets.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.event.thread.count</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Thread count for handling incoming event packets.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.heartbeat.interval</code></td>
<td style="text-align:left">10000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Frequency of heartbeat messages sent by the clients to members.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.heartbeat.timeout</code></td>
<td style="text-align:left">60000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Timeout for the heartbeat messages sent by the client to members. If no messages pass between client and member within the given time via this property in milliseconds, the connection will be closed.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.max.concurrent.invocations</code></td>
<td style="text-align:left">Integer.MAX_VALUE</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Maximum allowed number of concurrent invocations. You can apply a constraint on the number of concurrent invocations in order to prevent the system from overloading. If the maximum number of concurrent invocations is exceeded and a new invocation comes in, Hazelcast throws <code>HazelcastOverloadException</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.invocation.timeout.seconds</code></td>
<td style="text-align:left">120</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Time to give up the invocation when a member in the member list is not reachable.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.shuffle.member.list</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">string</td>
<td style="text-align:left">The client shuffles the given member list to prevent all clients to connect to the same member when this property is <code>false</code>. When it is set to <code>true</code>, the client tries to connect to the members in the given order.</td>
</tr>
</tbody>
</table>
<a name="sample-codes-for-client"></a><h2 id="sample-codes-for-client">Sample Codes for Client</h2>
<p>Please refer to <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/clients" target="_blank">Client Code Samples</a>.</p>

<a name="using-high-density-memory-store-with-java-client"></a><h2 id="using-high-density-memory-store-with-java-client">Using High-Density Memory Store with Java Client</h2>
<p><font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>
<br></br></p>
<p>If you have <font color="##153F75"><strong>Hazelcast Enterprise HD</strong></font>, your Hazelcast Java client&#39;s near cache can benefit from the High-Density Memory Store. </p>
<p>Let&#39;s recall the Java client&#39;s near cache configuration (please see the <a href="#configuring-client-near-cache">Configuring Client Near Cache section</a>) <strong>without</strong> High-Density Memory Store:</p>
<pre><code class="lang-xml">&lt;/hazelcast-client&gt;
    ...
    ...
    &lt;near-cache name=&quot;MENU&quot;&gt;
        &lt;max-size&gt;2000&lt;/max-size&gt;
        &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
        &lt;max-idle-seconds&gt;0&lt;/max-idle-seconds&gt;
        &lt;eviction-policy&gt;LFU&lt;/eviction-policy&gt;
        &lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;
        &lt;in-memory-format&gt;OBJECT&lt;/in-memory-format&gt;
    &lt;/near-cache&gt;
    ...
&lt;/hazelcast-client&gt;
</code></pre>
<p>You can configure this near cache to use Hazelcast&#39;s High-Density Memory Store by setting the in-memory format to NATIVE. Please see the following configuration example:</p>
<pre><code class="lang-xml">&lt;/hazelcast-client&gt;
    ...
    ...
    &lt;near-cache&gt;
       ...
       &lt;time-to-live-seconds&gt;0&lt;/time-to-live-seconds&gt;
       &lt;max-idle-seconds&gt;0&lt;/max-idle-seconds&gt;
       &lt;invalidate-on-change&gt;true&lt;/invalidate-on-change&gt;
       &lt;in-memory-format&gt;NATIVE&lt;/in-memory-format&gt;
       &lt;eviction size=&quot;1000&quot; max-size-policy=&quot;ENTRY_COUNT&quot; eviction-policy=&quot;LFU&quot;/&gt;
       ...
    &lt;/near-cache&gt;
&lt;/hazelcast-client&gt;
</code></pre>
<p>Please notice that when the in-memory format is NATIVE, i.e. High-Density Memory Store is enabled, the configuration element <code>&lt;eviction&gt;</code> is used to specify the eviction behavior of your client&#39;s near cache. In this case, the elements <code>&lt;max-size&gt;</code> and <code>&lt;eviction-policy&gt;</code> used in the configuration of a near cache without High-Density Memory Store do not have any impact. </p>
<p>The element <code>&lt;eviction&gt;</code> has the following attributes:</p>
<ul>
<li><code>size</code>: Maximum size (entry count) of the near cache.</li>
<li><code>max-size-policy</code>: Maximum size policy for eviction of the near cache. Available values are as follows:<ul>
<li>ENTRY_COUNT: Maximum entry count per member.</li>
<li>USED_NATIVE_MEMORY_SIZE: Maximum used native memory size in megabytes.</li>
<li>USED_NATIVE_MEMORY_PERCENTAGE: Maximum used native memory percentage.</li>
<li>FREE_NATIVE_MEMORY_SIZE: Minimum free native memory size to trigger cleanup.</li>
<li>FREE_NATIVE_MEMORY_PERCENTAGE: Minimum free native memory percentage to trigger cleanup.</li>
</ul>
</li>
<li><code>eviction-policy</code>: Eviction policy configuration. Its default values is NONE. Available values are as follows:<ul>
<li>NONE: No items will be evicted and the property max-size will be ignored. You still can combine it with time-to-live-seconds.</li>
<li>LRU:     Least Recently Used.</li>
<li>LFU:     Least Frequently Used.</li>
</ul>
</li>
</ul>
<p>Keep in mind that you should have already enabled the High-Density Memory Store usage for your client, using the <code>&lt;native-memory&gt;</code> element in the client&#39;s configuration.</p>
<p>Please see the <a href="#high-density-memory-store">High-Density Memory Store section</a> for more information on Hazelcast&#39;s High-Density Memory Store feature.</p>

<a name="other-client-and-language-implementations"></a><h1 id="other-client-and-language-implementations">Other Client and Language Implementations</h1>
<p>Besides Java client, Hazelcast offers the following clients and language APIs to extend the benefits of operational in-memory computing to applications:</p>
<ul>
<li>Native C++ client</li>
<li>Native .NET client</li>
<li>Python client</li>
<li>Node.js client</li>
<li>Hazelcast Scala API</li>
<li>REST client</li>
<li>Memcache client</li>
</ul>
<p>Please refer to <a href="http://hazelcast.org/clients-languages/" target="_blank">Feature Comparison Matrix</a> to see the features implemented across the clients and language APIs.</p>
<p>Following sections describe each client and language API. Most of them will direct you to their own GitHub repositories.</p>

<a name="c-client"></a><h2 id="c-client">C++ Client</h2>
<p>You can use Native C++ Client to connect to Hazelcast cluster members and perform almost all operations that a member can perform. Clients differ from members in that clients do not hold data. The C++ Client is by default a smart client, i.e., it knows where the data is and asks directly for the correct member. You can disable this feature (using the <code>ClientConfig::setSmart</code> method) if you do not want the clients to connect to every member.</p>
<p>The features of C++ Clients are listed below:</p>
<ul>
<li>Access to distributed data structures (IMap, IQueue, MultiMap, ITopic, etc.).</li>
<li>Access to transactional distributed data structures (TransactionalMap, TransactionalQueue, etc.).</li>
<li>Ability to add cluster listeners to a cluster and entry/item listeners to distributed data structures.</li>
<li>Distributed synchronization mechanisms with ILock, ISemaphore and ICountDownLatch.</li>
</ul>
<p>Please refer to C++ client&#39;s own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-cpp-client" target="_blank">Hazelcast C++ Client</a> for information on setting the client up, installing and compiling it, its serialization support, and APIs such as raw pointer and query. </p>

<a name="net-client"></a><h2 id="-net-client">.NET Client</h2>
<p>You can use the native .NET client to connect to Hazelcast client members. You need to add <code>HazelcastClient3x.dll</code> into your .NET project references. The API is very similar to the Java native client. </p>
<p>Please refer to .NET client&#39;s own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-csharp-client" target="_blank">Hazelcast .NET Client</a> for information on configuring and starting the client. </p>

<a name="rest-client"></a><h2 id="rest-client">REST Client</h2>
<p>Hazelcast provides a REST interface, i.e. it provides an HTTP service in each cluster member so that you can access your <code>map</code> and <code>queue</code> using HTTP protocol. Assuming <code>mapName</code> and <code>queueName</code> are already configured in your Hazelcast, its structure is shown below.</p>
<p><code>http://member IP address:port/hazelcast/rest/maps/mapName/key</code></p>
<p><code>http://member IP address:port/hazelcast/rest/queues/queueName</code></p>
<p>For the operations to be performed, standard REST conventions for HTTP calls are used.</p>
<p><br> </br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>REST client request listener service is not enabled by default. You should enable it on your cluster members to use REST client. It can be enabled using the system property <code>hazelcast.rest.enabled</code>. Please refer to the <a href="#system-properties">System Properties section</a> for the definition of this property and how to set a system property.</em></p>
<a name="rest-client-getpostdelete-examples"></a><h3 id="rest-client-get-post-delete-examples">REST Client GET/POST/DELETE Examples</h3>
<p>In the following GET, POST, and DELETE examples, assume that your cluster members are as shown below.</p>
<pre><code class="lang-plain">Members [5] {
  Member [10.20.17.1:5701]
  Member [10.20.17.2:5701]
  Member [10.20.17.4:5701]
  Member [10.20.17.3:5701]
  Member [10.20.17.5:5701]
}
</code></pre>
<hr>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>All of the requests below can return one of the following responses in case of a failure.</em></p>
<ul>
<li>If the HTTP request syntax is not known, the following response will be returned.</li>
</ul>
<pre><code class="lang-plain">HTTP/1.1 400 Bad Request
Content-Length: 0
</code></pre>
<ul>
<li>In case of an unexpected exception, the following response will be returned.</li>
</ul>
<pre><code class="lang-plain">&lt; HTTP/1.1 500 Internal Server Error
&lt; Content-Length: 0
</code></pre>
<hr>
<a name="creatingupdating-entries-in-a-map-for-rest-client"></a><h4 id="creating-updating-entries-in-a-map-for-rest-client">Creating/Updating Entries in a Map for REST Client</h4>
<p>You can put a new <code>key1/value1</code> entry into a map by using POST call to 
<code>http://10.20.17.1:5701/hazelcast/
rest/maps/mapName/key1</code> URL. This call&#39;s content body should contain the value of the key. Also, if the call contains the MIME type, Hazelcast stores this information, too. </p>
<p>A sample POST call is shown below.</p>
<pre><code class="lang-plain">$ curl -v -X POST -H &quot;Content-Type: text/plain&quot; -d &quot;bar&quot; 
    http://10.20.17.1:5701/hazelcast/rest/maps/mapName/foo
</code></pre>
<p>It will return the following response if successful:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 0
</code></pre>
<a name="retrieving-entries-from-a-map-for-rest-client"></a><h4 id="retrieving-entries-from-a-map-for-rest-client">Retrieving Entries from a Map for REST Client</h4>
<p>If you want to retrieve an entry, you can use a GET call to <code>http://10.20.17.1:5701/hazelcast/rest/maps/mapName/key1</code>. You can also retrieve this entry from another member of your cluster, such as 
<code>http://10.20.17.3:5701/hazelcast/rest/
maps/mapName/key1</code>.</p>
<p>An example of a GET call is shown below.</p>
<pre><code class="lang-plain">$ curl -X GET http://10.20.17.3:5701/hazelcast/rest/maps/mapName/foo
</code></pre>
<p>It will return the following response if there is a corresponding value:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 3
bar
</code></pre>
<p>This GET call returned a value, its length, and also the MIME type (<code>text/plain</code>) since the POST call example shown above included the MIME type.</p>
<p>It will return the following if there is no mapping for the given key:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 204 No Content
&lt; Content-Length: 0
</code></pre>
<a name="removing-entries-from-a-map-for-rest-client"></a><h4 id="removing-entries-from-a-map-for-rest-client">Removing Entries from a Map for REST Client</h4>
<p>You can use a DELETE call to remove an entry. A sample DELETE call is shown below with its response.</p>
<pre><code class="lang-plain">$ curl -v -X DELETE http://10.20.17.1:5701/hazelcast/rest/maps/mapName/foo
</code></pre>
<pre><code>&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 0
</code></pre><p>If you leave the key empty as follows, DELETE will delete all entries from the map.</p>
<pre><code class="lang-plain">$ curl -v -X DELETE http://10.20.17.1:5701/hazelcast/rest/maps/mapName
</code></pre>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 0
</code></pre>
<a name="offering-items-on-a-queue-for-rest-client"></a><h4 id="offering-items-on-a-queue-for-rest-client">Offering Items on a Queue for REST Client</h4>
<p>You can use a POST call to create an item on the queue. A sample is shown below.</p>
<pre><code class="lang-plain">$ curl -v -X POST -H &quot;Content-Type: text/plain&quot; -d &quot;foo&quot; 
    http://10.20.17.1:5701/hazelcast/rest/queues/myEvents
</code></pre>
<p>The above call is equivalent to <code>HazelcastInstance#getQueue(&quot;myEvents&quot;).offer(&quot;foo&quot;);</code>.</p>
<p>It will return the following if successful:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 0
</code></pre>
<p>It will return the following if the queue is full and the item is not able to be offered to the queue:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 503 Service Unavailable
&lt; Content-Length: 0
</code></pre>
<a name="retrieving-items-from-a-queue-for-rest-client"></a><h4 id="retrieving-items-from-a-queue-for-rest-client">Retrieving Items from a Queue for REST Client</h4>
<p>You can use a DELETE call for retrieving items from a queue. Note that you should state the poll timeout while polling for queue events by an extra path parameter. </p>
<p>An example is shown below (<strong>10</strong> being the timeout value).</p>
<pre><code class="lang-plain">$ curl -v -X DELETE \http://10.20.17.1:5701/hazelcast/rest/queues/myEvents/10
</code></pre>
<p>The above call is equivalent to <code>HazelcastInstance#getQueue(&quot;myEvents&quot;).poll(10, SECONDS);</code>. Below is the response.</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 3
foo
</code></pre>
<p>When the timeout is reached, the response will be <code>No Content</code> success, i.e. there is no item on the queue to be returned.</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 204 No Content
&lt; Content-Length: 0
</code></pre>
<a name="getting-the-size-of-the-queue-for-rest-client"></a><h4 id="getting-the-size-of-the-queue-for-rest-client">Getting the size of the queue for REST Client</h4>
<pre><code class="lang-plain">$ curl -v -X GET \http://10.20.17.1:5701/hazelcast/rest/queues/myEvents/size
</code></pre>
<p>The above call is equivalent to <code>HazelcastInstance#getQueue(&quot;myEvents&quot;).size();</code>. Below is a sample response.</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain
&lt; Content-Length: 1
5
</code></pre>
<hr>
<a name="checking-the-status-of-the-cluster-for-rest-client"></a><h3 id="checking-the-status-of-the-cluster-for-rest-client">Checking the Status of the Cluster for REST Client</h3>
<p>Besides the above operations, you can check the status of your cluster, a sample of which is shown below.</p>
<pre><code class="lang-plain">$ curl -v http://127.0.0.1:5701/hazelcast/rest/cluster
</code></pre>
<p>The return will be similar to the following:</p>
<pre><code class="lang-plain">&lt; HTTP/1.1 200 OK
&lt; Content-Length: 119

Members [5] {
  Member [10.20.17.1:5701] this
  Member [10.20.17.2:5701]
  Member [10.20.17.4:5701]
  Member [10.20.17.3:5701]
  Member [10.20.17.5:5701]
}

ConnectionCount: 5
AllConnectionCount: 20
</code></pre>
<hr>
<p>RESTful access is provided through any member of your cluster. You can even put an HTTP load-balancer in front of your cluster members for load balancing and fault tolerance.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>You need to handle the failures on REST polls as there is no transactional guarantee.</em></p>

<a name="memcache-client"></a><h2 id="memcache-client">Memcache Client</h2>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Hazelcast Memcache Client only supports ASCII protocol. Binary Protocol is not supported.</em></p>
<p>A Memcache client written in any language can talk directly to a Hazelcast cluster. No additional configuration is required.</p>
<p><br> </br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE</em></strong>: <em>Memcache client request listener service is not enabled by default. You should enable it on your cluster members to use Memcache client. It can be enabled using the system property <code>hazelcast.memcache.enabled</code>. Please refer to the <a href="#system-properties">System Properties section</a> for the definition of this property and how to set a system property.</em></p>
<a name="memcache-client-code-examples"></a><h3 id="memcache-client-code-examples">Memcache Client Code Examples</h3>
<p>Assume that your cluster members are as shown below.</p>
<pre><code class="lang-plain">Members [5] {
  Member [10.20.17.1:5701]
  Member [10.20.17.2:5701]
  Member [10.20.17.4:5701]
  Member [10.20.17.3:5701]
  Member [10.20.17.5:5701]
}
</code></pre>
<p>Assume that you have a PHP application that uses PHP Memcache client to cache things in Hazelcast. All you need to do is have your PHP Memcache client connect to one of these members. It does not matter which member the client connects to because the Hazelcast cluster looks like one giant machine (Single System Image). Here is a PHP client code example.</p>
<pre><code class="lang-php">&lt;?php
  $memcache = new Memcache;
  $memcache-&gt;connect( &#39;10.20.17.1&#39;, 5701 ) or die ( &quot;Could not connect&quot; );
  $memcache-&gt;set( &#39;key1&#39;, &#39;value1&#39;, 0, 3600 );
  $get_result = $memcache-&gt;get( &#39;key1&#39; ); // retrieve your data
  var_dump( $get_result ); // show it
?&gt;
</code></pre>
<p>Notice that Memcache client connects to <code>10.20.17.1</code> and uses port <code>5701</code>. Here is a Java client code example with SpyMemcached client:</p>
<pre><code class="lang-java">MemcachedClient client = new MemcachedClient(
    AddrUtil.getAddresses( &quot;10.20.17.1:5701 10.20.17.2:5701&quot; ) );
client.set( &quot;key1&quot;, 3600, &quot;value1&quot; );
System.out.println( client.get( &quot;key1&quot; ) );
</code></pre>
<p>If you want your data to be stored in different maps (e.g. to utilize per map configuration), you can do that with a map name prefix as in the following example code.</p>
<pre><code class="lang-java">MemcachedClient client = new MemcachedClient(
    AddrUtil.getAddresses( &quot;10.20.17.1:5701 10.20.17.2:5701&quot; ) );
client.set( &quot;map1:key1&quot;, 3600, &quot;value1&quot; ); // store to *hz_memcache_map1
client.set( &quot;map2:key1&quot;, 3600, &quot;value1&quot; ); // store to hz_memcache_map2
System.out.println( client.get( &quot;key1&quot; ) ); // get from hz_memcache_map1
System.out.println( client.get( &quot;key2&quot; ) ); // get from hz_memcache_map2
</code></pre>
<p><em>hz_memcache prefix_</em> separates Memcache maps from Hazelcast maps. If no map name is given, it will be stored
in a default map named <em>hz_memcache_default</em>.</p>
<p>An entry written with a Memcache client can be read by another Memcache client written in another language.</p>
<a name="unsupported-operations-for-memcache"></a><h3 id="unsupported-operations-for-memcache">Unsupported Operations for Memcache</h3>
<ul>
<li><p>CAS operations are not supported. In operations that get CAS parameters, such as append, CAS values are ignored.</p>
</li>
<li><p>Only a subset of statistics are supported. Below is the list of supported statistic values.</p>
<ul>
<li>cmd_set</li>
<li>cmd_get</li>
<li>incr_hits</li>
<li>incr_misses</li>
<li>decr_hits</li>
<li>decr_misses</li>
</ul>
</li>
</ul>

<a name="serialization"></a><h1 id="serialization">Serialization</h1>
<p>Hazelcast needs to serialize the Java objects that you put into Hazelcast because Hazelcast is a distributed system. The data and its replicas are stored in different partitions on multiple cluster members. The data you need may not be present on the local member, and in that case, Hazelcast retrieves that data from another member. This requires serialization.</p>
<p>Hazelcast serializes all your objects into an instance of <code>com.hazelcast.nio.serialization.Data</code>. <code>Data</code> is the binary representation of an object. </p>
<p>Serialization is used when:</p>
<ul>
<li>key/value objects are added to a map,</li>
<li>items are put in a queue/set/list,</li>
<li>a runnable is sent using an executor service,</li>
<li>an entry processing is performed within a map,</li>
<li>an object is locked, and</li>
<li>a message is sent to a topic.</li>
</ul>
<p>Hazelcast optimizes the serialization for the basic types and their array types. You cannot override this behavior.</p>
<p><strong>Default Types</strong>; </p>
<ul>
<li>Byte, Boolean, Character, Short, Integer, Long, Float, Double, String</li>
<li>byte[], boolean[], char[], short[], int[], long[], float[], double[], String[]</li>
<li><code>java.util.Date</code>, <code>java.math.BigInteger</code>, <code>java.math.BigDecimal</code>, <code>java.lang.Class</code></li>
</ul>
<p><br><br>
Hazelcast optimizes all of the above object types. You do not need to worry about their (de)serializations.</p>

<a name="serialization-interface-types"></a><h2 id="serialization-interface-types">Serialization Interface Types</h2>
<p>For complex objects, use the following interfaces for serialization and deserialization.</p>
<ul>
<li><p><code>java.io.Serializable</code>: Please see the <a href="#implementing-java-serializable-and-externalizable">Implementing Java Serializable and Externalizable section</a>.</p>
</li>
<li><p><code>java.io.Externalizable</code>: Please see the <a href="#implementing-java-externalizable">Implementing Java Externalizable section</a>.</p>
</li>
<li><p><code>com.hazelcast.nio.serialization.DataSerializable</code>: Please see the <a href="#implementing-dataserializable">Implementing DataSerializable section</a>.</p>
</li>
<li><p><code>com.hazelcast.nio.serialization.IdentifiedDataSerializable</code>: Please see the <a href="#identifieddataserializable">IdentifiedDataSerializable section</a>.</p>
</li>
<li><p><code>com.hazelcast.nio.serialization.Portable</code>: Please see the <a href="#implementing-portable-serialization">Implementing Portable Serialization section</a>.</p>
</li>
<li><p>Custom Serialization (using <a href="#implementing-streamserializer">StreamSerializer</a> and <a href="#implementing-bytearrayserializer">ByteArraySerializer</a>).</p>
</li>
<li><p>Global Serializer: Please see the <a href="#global-serializer">Global Serializer section</a> for details.</p>
</li>
</ul>
<p>When Hazelcast serializes an object into <code>Data</code>:</p>
<p><strong>(1)</strong> It first checks whether the object is <code>null</code>.</p>
<p><strong>(2)</strong> If the above check fails, then Hazelcast checks if it is an instance of <code>com.hazelcast.nio.serialization.DataSerializable</code> or <code>com.hazelcast.nio.serialization.IdentifiedDataSerializable</code>.</p>
<p><strong>(3)</strong> If the above check fails, then Hazelcast checks if it is an instance of <code>com.hazelcast.nio.serialization.Portable</code>.</p>
<p><strong>(4)</strong> If the above check fails, then Hazelcast checks if it is an instance of one of the default types (see the <a href="#serialization">Serialization chapter introduction</a> for default types).</p>
<p><strong>(5)</strong> If the above check fails, then Hazelcast looks for a user-specified <a href="#custom-serialization">Custom Serializer</a>, i.e. an implementation of <code>ByteArraySerializer</code> or <code>StreamSerializer</code>. Custom serializer is searched using the input Object&#39;s Class and its parent class up to Object. If parent class search fails, all interfaces implemented by the class are also checked (excluding <code>java.io.Serializable</code> and <code>java.io.Externalizable</code>). </p>
<p><strong>(6)</strong> If the above check fails, then Hazelcast checks if it is an instance of <code>java.io.Serializable</code> or <code>java.io.Externalizable</code> and a Global Serializer is not registered with Java Serialization Override feature.</p>
<p><strong>(7)</strong> If the above check fails, Hazelcast will use the registered Global Serializer if one exists.</p>
<p>If all of the above checks fail, then serialization will fail. When a class implements multiple interfaces, the above steps are important to determine the serialization mechanism that Hazelcast will use. When a class definition is required for any of these serializations, you need to have all the classes needed by the application on your classpath because Hazelcast does not download them automatically.</p>

<a name="comparing-serialization-interfaces"></a><h2 id="comparing-serialization-interfaces">Comparing Serialization Interfaces</h2>
<p>The table below provides a comparison between the interfaces listed in the previous section to help you in deciding which interface to use in your applications.</p>
<table>
<thead>
<tr>
<th style="text-align:left"><em>Serialization Interface</em></th>
<th style="text-align:left"><em>Advantages</em></th>
<th style="text-align:left"><em>Drawbacks</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Serializable</strong></td>
<td style="text-align:left">- A standard and basic Java interface <br> - Requires no implementation</td>
<td style="text-align:left">- More time and CPU usage <br> - More space occupancy <br> - Not supported by Native clients</td>
</tr>
<tr>
<td style="text-align:left"><strong>Externalizable</strong></td>
<td style="text-align:left">- A standard Java interface <br> - More CPU and memory usage efficient than Serializable</td>
<td style="text-align:left">- Serialization interface must be implemented <br> - Not supported by Native clients</td>
</tr>
<tr>
<td style="text-align:left"><strong>DataSerializable</strong></td>
<td style="text-align:left">- More CPU and memory usage efficient than Serializable</td>
<td style="text-align:left">- Specific to Hazelcast <br> - Not supported by Native clients</td>
</tr>
<tr>
<td style="text-align:left"><strong>IdentifiedDataSerializable</strong></td>
<td style="text-align:left">- More CPU and memory usage efficient than Serializable <br> - Reflection is not used during deserialization <br> - Supported by all Native Clients</td>
<td style="text-align:left">- Specific to Hazelcast <br> - Serialization interface must be implemented <br> - A Factory and configuration must be implemented</td>
</tr>
<tr>
<td style="text-align:left"><strong>Portable</strong></td>
<td style="text-align:left">- More CPU and memory usage efficient than Serializable <br> - Reflection is not used during deserialization <br> - Versioning is supported <br> Partial deserialization is supported during Queries <br> - Supported by all Native Clients</td>
<td style="text-align:left">- Specific to Hazelcast <br> - Serialization interface must be implemented <br> - A Factory and configuration must be implemented <br> - Class definition is also sent with data but stored only once per class</td>
</tr>
<tr>
<td style="text-align:left"><strong>Custom Serialization</strong></td>
<td style="text-align:left">- Does not require class to implement an interface <br> - Convenient and flexible <br> - Can be based on StreamSerializer ByteArraySerializer</td>
<td style="text-align:left">- Serialization interface must be implemented <br> - Plug in and configuration is required</td>
</tr>
</tbody>
</table>
<p>Let&#39;s dig into the details of the above serialization mechanisms in the following sections.</p>

<a name="implementing-java-serializable-and-externalizable"></a><h2 id="implementing-java-serializable-and-externalizable">Implementing Java Serializable and Externalizable</h2>
<p>A class often needs to implement the <code>java.io.Serializable</code> interface; native Java serialization is the easiest way to do serialization.</p>
<p>Let&#39;s take a look at the example code below for Java Serializable.</p>
<pre><code class="lang-java">public class Employee implements Serializable { 
  private static final long serialVersionUID = 1L;
  private String surname;

  public Employee( String surname ) { 
    this.surname = surname;
  } 
}
</code></pre>
<p>Here, the fields that are non-static and non-transient are automatically serialized. To eliminate class compatibility issues, it is recommended that you add a <code>serialVersionUID</code>, as shown above. Also, when you are using methods that perform byte-content comparisons (e.g. <code>IMap.replace()</code>) and if byte-content of equal objects is different, you may face unexpected behaviors. For example, if the class relies on a hash map, the <code>replace</code> method may fail. The reason for this is the hash map is a serialized data structure with unreliable byte-content.</p>
<a name="implementing-java-externalizable"></a><h3 id="implementing-java-externalizable">Implementing Java Externalizable</h3>
<p>Hazelcast also supports <code>java.io.Externalizable</code>. This interface offers more control on the way fields are serialized or deserialized. Compared to native Java serialization, it also can have a positive effect on performance. With <code>java.io.Externalizable</code>, there is no need to add <code>serialVersionUID</code>.</p>
<p>Let&#39;s take a look at the example code below.</p>
<pre><code class="lang-java">public class Employee implements Externalizable { 
  private String surname;
  public Employee(String surname) { 
        this.surname = surname;
  }

  @Override
  public void readExternal( ObjectInput in )
      throws IOException, ClassNotFoundException {
    this.surname = in.readUTF();
  }

  @Override
  public void writeExternal( ObjectOutput out )
      throws IOException {
    out.writeUTF(surname); 
  }
}
</code></pre>
<p>You explicitly perform writing and reading of fields. Perform reading in the same order as writing.</p>

<a name="implementing-dataserializable"></a><h2 id="implementing-dataserializable">Implementing DataSerializable</h2>
<p>As mentioned in <a href="#implementing-java-serializable-and-externalizable">Implementing Java Serializable &amp; Externalizable</a>, Java serialization is an easy mechanism. However, it does not control how fields are serialized or deserialized. Moreover, Java serialization can lead to excessive CPU loads since it keeps track of objects to handle the cycles and streams class descriptors. These are performance decreasing factors; thus, serialized data may not have an optimal size.</p>
<p>The <code>DataSerializable</code> interface of Hazelcast overcomes these issues. Here is an example of a class implementing the <code>com.hazelcast.nio.serialization.DataSerializable</code> interface.</p>
<pre><code class="lang-java">public class Address implements DataSerializable {
  private String street;
  private int zipCode;
  private String city;
  private String state;

  public Address() {}

  //getters setters..

  public void writeData( ObjectDataOutput out ) throws IOException {
    out.writeUTF(street);
    out.writeInt(zipCode);
    out.writeUTF(city);
    out.writeUTF(state);
  }

  public void readData( ObjectDataInput in ) throws IOException {
    street = in.readUTF();
    zipCode = in.readInt();
    city = in.readUTF();
    state = in.readUTF();
  }
}
</code></pre>
<a name="reading-and-writing-and-dataserializable"></a><h4 id="reading-and-writing-and-dataserializable">Reading and Writing and DataSerializable</h4>
<p>Let&#39;s take a look at another example which encapsulates a <code>DataSerializable</code> field. </p>
<p>Since the <code>address</code> field itself is <code>DataSerializable</code>, it calls <code>address.writeData(out)</code> when writing and <code>address.readData(in)</code> when reading. Also note that you should have writing and reading of the fields occur 
in the same order. When Hazelcast serializes a <code>DataSerializable</code>, it writes the <code>className</code> first. When Hazelcast deserializes it, <code>className</code> is used to instantiate the object using reflection.</p>
<pre><code class="lang-java">public class Employee implements DataSerializable {
  private String firstName;
  private String lastName;
  private int age;
  private double salary;
  private Address address; //address itself is DataSerializable

  public Employee() {}

  //getters setters..

  public void writeData( ObjectDataOutput out ) throws IOException {
    out.writeUTF(firstName);
    out.writeUTF(lastName);
    out.writeInt(age);
    out.writeDouble (salary);
    address.writeData (out);
  }

  public void readData( ObjectDataInput in ) throws IOException {
    firstName = in.readUTF();
    lastName = in.readUTF();
    age = in.readInt();
    salary = in.readDouble();
    address = new Address();
    // since Address is DataSerializable let it read its own internal state
    address.readData(in);
  }
}
</code></pre>
<p>As you can see, since the <code>address</code> field itself is <code>DataSerializable</code>, it calls <code>address.writeData(out)</code> when writing and <code>address.readData(in)</code> when reading. Also note that you should have writing and reading of the fields occur in the same order. While Hazelcast serializes a <code>DataSerializable</code>, it writes the <code>className</code> first. When Hazelcast deserializes it, <code>className</code> is used to instantiate the object using reflection.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Since Hazelcast needs to create an instance during deserialization,<code>DataSerializable</code> class has a no-arg constructor.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>DataSerializable</code> is a good option if serialization is only needed for in-cluster communication.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>DataSerializable</code> is not supported by non-Java clients as it uses Java reflection. If you need non-Java clients, please use <a href="#identifieddataserializable"><code>IdentifiedDataSerializable</code></a> or <a href="#implementing-portable-serialization"><code>Portable</code></a>.</em></p>
<a name="identifieddataserializable"></a><h3 id="identifieddataserializable">IdentifiedDataSerializable</h3>
<p>For a faster serialization of objects, avoiding reflection and long class names, Hazelcast recommends you implement <code>com.hazelcast.nio.serialization.IdentifiedDataSerializable</code> which is a slightly better version of <code>DataSerializable</code>.</p>
<p><code>DataSerializable</code> uses reflection to create a class instance, as mentioned in <a href="#implementing-dataserializable">Implementing DataSerializable</a>. But <code>IdentifiedDataSerializable</code> uses a factory for this purpose and it is faster during deserialization, which requires new instance creations.</p>
<a name="getid-and-getfactoryid-methods"></a><h4 id="getid-and-getfactoryid-methods">getID and getFactoryId Methods</h4>
<p><code>IdentifiedDataSerializable</code> extends <code>DataSerializable</code> and introduces two new methods.</p>
<ul>
<li><code>int getId();</code></li>
<li><code>int getFactoryId();</code></li>
</ul>
<p><code>IdentifiedDataSerializable</code> uses <code>getId()</code> instead of class name, and it uses <code>getFactoryId()</code> to load the class when given the Id. To complete the implementation, you should also implement  <code>com.hazelcast.nio.serialization.DataSerializableFactory</code> and register it into <code>SerializationConfig</code>, which can be accessed from <code>Config.getSerializationConfig()</code>. Factory&#39;s responsibility is to return an instance of the right <code>IdentifiedDataSerializable</code> object, given the Id. This is currently the most efficient way of Serialization that Hazelcast supports off the shelf.</p>
<a name="implementing-identifieddataserializable"></a><h4 id="implementing-identifieddataserializable">Implementing IdentifiedDataSerializable</h4>
<p>Let&#39;s take a look at the following example code and configuration to see <code>IdentifiedDataSerializable</code> in action.</p>
<pre><code class="lang-java">public class Employee
    implements IdentifiedDataSerializable {

  private String surname;

  public Employee() {}

  public Employee( String surname ) { 
    this.surname = surname;
  }

  @Override
  public void readData( ObjectDataInput in ) 
      throws IOException {
    this.surname = in.readUTF();
  }

  @Override
  public void writeData( ObjectDataOutput out )
      throws IOException { 
    out.writeUTF( surname );
  }

  @Override
  public int getFactoryId() { 
    return EmployeeDataSerializableFactory.FACTORY_ID;
  }

  @Override
  public int getId() { 
    return EmployeeDataSerializableFactory.EMPLOYEE_TYPE;
  }

  @Override
  public String toString() {
    return String.format( &quot;Employee(surname=%s)&quot;, surname ); 
  }
}
</code></pre>
<p>The methods <code>getId</code> and <code>getFactoryId</code> return a unique positive number within the <code>EmployeeDataSerializableFactory</code>. Now, let&#39;s create an instance of this <code>EmployeeDataSerializableFactory</code>.</p>
<pre><code class="lang-java">public class EmployeeDataSerializableFactory 
    implements DataSerializableFactory{

  public static final int FACTORY_ID = 1;

  public static final int EMPLOYEE_TYPE = 1;

  @Override
  public IdentifiedDataSerializable create(int typeId) {
    if ( typeId == EMPLOYEE_TYPE ) { 
      return new Employee();
    } else {
      return null; 
    }
  }
}
</code></pre>
<p>The only method you should implement is <code>create</code>, as seen in the above example. It is recommended that you use a <code>switch-case</code> statement instead of multiple <code>if-else</code> blocks if you have a lot of subclasses. Hazelcast throws an exception if null is returned for <code>typeId</code>.</p>
<a name="registering-employeedataserializablefactory"></a><h4 id="registering-employeedataserializablefactory">Registering EmployeeDataSerializableFactory</h4>
<p>As the last step, you need to register <code>EmployeeDataSerializableFactory</code> declaratively (declare in the configuration file <code>hazelcast.xml</code>) as shown below. Note that <code>factory-id</code> has the same value of <code>FACTORY_ID</code> in the above code. This is crucial to enable Hazelcast to find the correct factory.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt; 
  ...
  &lt;serialization&gt;
    &lt;data-serializable-factories&gt;
      &lt;data-serializable-factory factory-id=&quot;1&quot;&gt;
        EmployeeDataSerializableFactory
      &lt;/data-serializable-factory&gt;
    &lt;/data-serializable-factories&gt;
  &lt;/serialization&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#serialization-configuration-wrap-up">Serialization Configuration Wrap-Up section</a> for a full description of Hazelcast Serialization configuration.</em></p>

<a name="implementing-portable-serialization"></a><h2 id="implementing-portable-serialization">Implementing Portable Serialization</h2>
<p>As an alternative to the existing serialization methods, Hazelcast offers a language/platform independent Portable serialization that has the following advantages:</p>
<ul>
<li>Supports multi-version of the same object type.</li>
<li>Fetches individual fields without having to rely on reflection.</li>
<li>Queries and indexing support without deserialization and/or reflection.</li>
</ul>
<p>In order to support these features, a serialized Portable object contains meta information like the version and the concrete location of the each field in the binary data. This way, Hazelcast navigates in the <code>byte[]</code> and deserializes only the required field without actually deserializing the whole object. This improves the Query performance.</p>
<p>With multi-version support, you can have two cluster members where each has different versions of the same object. Hazelcast will store both meta information and use the correct one to serialize and deserialize Portable objects depending on the member. This is very helpful when you are doing a rolling upgrade without shutting down the cluster.</p>
<p>Portable serialization is totally language independent and is used as the binary protocol between Hazelcast server and clients.</p>
<a name="portable-serialization-example-code"></a><h3 id="portable-serialization-example-code">Portable Serialization Example Code</h3>
<p>Here is example code for Portable implementation of a Foo class.</p>
<pre><code class="lang-java">public class Foo implements Portable{
  final static int ID = 5;

  private String foo;

  public String getFoo() {
    return foo;
  }

  public void setFoo( String foo ) {
    this.foo = foo;
  }

  @Override
  public int getFactoryId() {
    return 1;
  }

  @Override
  public int getClassId() {
    return ID;
  }

  @Override
  public void writePortable( PortableWriter writer ) throws IOException {
    writer.writeUTF( &quot;foo&quot;, foo );
  }

  @Override
  public void readPortable( PortableReader reader ) throws IOException {
    foo = reader.readUTF( &quot;foo&quot; );
  }
}
</code></pre>
<p>Similar to <code>IdentifiedDataSerializable</code>, a Portable Class must provide <code>classId</code> and <code>factoryId</code>. The Factory object creates the Portable object given the <code>classId</code>.</p>
<p>An example <code>Factory</code> could be implemented as follows:</p>
<pre><code class="lang-java">public class MyPortableFactory implements PortableFactory {

  @Override
  public Portable create( int classId ) {
    if ( Foo.ID == classId )
      return new Foo();
    else
      return null;
  }
}
</code></pre>
<a name="registering-the-portable-factory"></a><h3 id="registering-the-portable-factory">Registering the Portable Factory</h3>
<p>The last step is to register the <code>Factory</code> to the <code>SerializationConfig</code>. Below are the programmatic and declarative configurations for this step.</p>
<pre><code class="lang-java">Config config = new Config();
config.getSerializationConfig().addPortableFactory( 1, new MyPortableFactory() );
</code></pre>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;serialization&gt;
    &lt;portable-version&gt;0&lt;/portable-version&gt;
    &lt;portable-factories&gt;
      &lt;portable-factory factory-id=&quot;1&quot;&gt;
          com.hazelcast.nio.serialization.MyPortableFactory
      &lt;/portable-factory&gt;
    &lt;/portable-factories&gt;
  &lt;/serialization&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>Note that the <code>id</code> that is passed to the <code>SerializationConfig</code> is the same as the <code>factoryId</code> that the <code>Foo</code> class returns.</p>
<a name="versioning-for-portable-serialization"></a><h3 id="versioning-for-portable-serialization">Versioning for Portable Serialization</h3>
<p>More than one version of the same class may need to be serialized and deserialized. For example, a client may have an older version of a class, and the member to which it is connected may have a newer version of the same class. </p>
<p>Portable serialization supports versioning. It is a global versioning, meaning that all portable classes that are serialized through a member get the globally configured portable version.</p>
<p>You can declare Version in the configuration file <code>hazelcast.xml</code> using the <code>portable-version</code> element, as shown below.</p>
<pre><code class="lang-xml">&lt;serialization&gt;
  &lt;portable-version&gt;1&lt;/portable-version&gt;
  &lt;portable-factories&gt;
    &lt;portable-factory factory-id=&quot;1&quot;&gt;
        PortableFactoryImpl
    &lt;/portable-factory&gt;
  &lt;/portable-factories&gt;
&lt;/serialization&gt;
</code></pre>
<p>You can also use the interface <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/nio/serialization/VersionedPortable.java" target="_blank">VersionedPortable</a> which enables to upgrade the version per class, instead of global versioning. If you need to update only one class, you can use this interface. In this case, your class should implement <code>VersionedPortable</code> instead of <code>Portable</code>, and you can give the desired version using the method <code>VersionedPortable.getClassVersion()</code>.</p>
<p>You should consider the following when you perform versioning.</p>
<ul>
<li>It is important to change the version whenever an update is performed in the serialized fields of a class (e.g. increment the version).</li>
<li>If a client performs a Portable deserialization on a field, and then that Portable is updated by removing that field on the cluster side, this may lead to a problem.</li>
<li>Portable serialization does not use reflection and hence, fields in the class and in the serialized content are not automatically mapped. Field renaming is a simpler process. Also, since the class ID is stored, renaming the Portable does not lead to problems.</li>
<li>Types of fields need to be updated carefully. Hazelcast performs basic type upgradings (e.g. <code>int</code> to <code>float</code>).</li>
</ul>
<a name="example-portable-versioning-scenarios"></a><h4 id="example-portable-versioning-scenarios">Example Portable Versioning Scenarios</h4>
<p>Assume that a new member joins to the cluster with a class that has been modified and class&#39; version has been upgraded due to this modification.</p>
<ul>
<li>If you modified the class by adding a new field, the new member&#39;s <code>put</code> operations will include that new field. If this new member tries to get an object that was put from the older members, it will get <code>null</code> for the newly added field.</li>
<li>If you modified the class by removing a field, the old members get <code>null</code> for the objects that are put by the new member.</li>
<li>If you modified the class by changing the type of a field, the error <code>IncompatibleClassChangeError</code> is generated unless the change was made on a built-in type or the byte size of the new type is less than or equal to the old one. The following are example allowed type conversions:<ul>
<li><code>long</code> -&gt; <code>int</code>, <code>byte</code>, <code>char</code>, <code>short</code></li>
<li><code>int</code>-&gt; <code>byte</code>, <code>char</code>, <code>short</code> </li>
</ul>
</li>
</ul>
<p>If you have not modify a class at all, it will work as usual.</p>
<a name="null-portable-serialization"></a><h3 id="null-portable-serialization">Null Portable Serialization</h3>
<p>Be careful with serializing null portables. Hazelcast lazily creates a class definition of portable internally
when the user first serializes. This class definition is stored and used later for deserializing that portable class. When
the user tries to serialize a null portable when there is no class definition at the moment, Hazelcast throws an
exception saying that <code>com.hazelcast.nio.serialization.HazelcastSerializationException: Cannot write null portable
without explicitly registering class definition!</code>. </p>
<p>There are two solutions to get rid of this exception. Either put
a non-null portable class of the same type before any other operation, or manually register a class definition in serialization configuration as shown below.</p>
<pre><code class="lang-java">Config config = new Config();
final ClassDefinition classDefinition = new ClassDefinitionBuilder(Foo.factoryId, Foo.getClassId)
                       .addUTFField(&quot;foo&quot;).build();
config.getSerializationConfig().addClassDefinition(classDefinition);
Hazelcast.newHazelcastInstance(config);
</code></pre>
<a name="distributedobject-serialization"></a><h3 id="distributedobject-serialization">DistributedObject Serialization</h3>
<p>Putting a <code>DistributedObject</code> (Hazelcast Semaphore, Queue, etc.) in a cluster member and getting it from another one is not a straightforward operation. Passing the ID and type of the <code>DistributedObject</code> can be a solution. For deserialization, you can get the object from HazelcastInstance. For instance, if your object is an instance of <code>IQueue</code>, you can either use <code>HazelcastInstance.getQueue(id)</code> or <code>Hazelcast.getDistributedObject</code>.</p>
<p>You can use the <code>HazelcastInstanceAware</code> interface in the case of a deserialization of a Portable <code>DistributedObject</code> if it gets an ID to be looked up. HazelcastInstance is set after deserialization, so you first need to store the ID and then retrieve the <code>DistributedObject</code> using the <code>setHazelcastInstance</code> method. </p>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#serialization-configuration-wrap-up">Serialization Configuration Wrap-Up section</a> for a full description of Hazelcast Serialization configuration.</em></p>

<a name="custom-serialization"></a><h2 id="custom-serialization">Custom Serialization</h2>
<p>Hazelcast lets you plug in a custom serializer for serializing your objects. You can use <a href="#implementing-streamserializer">StreamSerializer</a> and <a href="#implementing-bytearrayserializer">ByteArraySerializer</a> interfaces for this purpose.</p>
<a name="implementing-streamserializer"></a><h3 id="implementing-streamserializer">Implementing StreamSerializer</h3>
<p>You can use a stream to serialize and deserialize data by using <code>StreamSerializer</code>. This is a good option for your own implementations. It can also be adapted to external serialization libraries like Kryo, JSON, and protocol buffers.</p>
<a name="streamserializer-example-code-1"></a><h4 id="streamserializer-example-code-1">StreamSerializer Example Code 1</h4>
<p>First, let&#39;s create a simple object.</p>
<pre><code class="lang-java">public class Employee {
  private String surname;

  public Employee( String surname ) {
    this.surname = surname;
  }
}
</code></pre>
<p>Now, let&#39;s implement StreamSerializer for <code>Employee</code> class.</p>
<pre><code class="lang-java">public class EmployeeStreamSerializer
    implements StreamSerializer&lt;Employee&gt; {

  @Override
  public int getTypeId () {
    return 1; 
  }

  @Override
  public void write( ObjectDataOutput out, Employee employee )
      throws IOException { 
    out.writeUTF(employee.getSurname());
  }

  @Override
  public Employee read( ObjectDataInput in ) 
      throws IOException { 
    String surname = in.readUTF();
    return new Employee(surname);
  }

  @Override
  public void destroy () { 
  }
}
</code></pre>
<p>In practice, classes may have many fields. Just make sure the fields are read in the same order as they are written. The type ID must be unique and greater than or equal to <strong>1</strong>. Uniqueness of the type ID enables Hazelcast to determine which serializer will be used during deserialization. </p>
<p>As the last step, let&#39;s register the <code>EmployeeStreamSerializer</code> in the configuration file <code>hazelcast.xml</code>, as shown below.</p>
<pre><code class="lang-xml">&lt;serialization&gt;
  &lt;serializers&gt;
    &lt;serializer type-class=&quot;Employee&quot; class-name=&quot;EmployeeStreamSerializer&quot; /&gt;
  &lt;/serializers&gt;
&lt;/serialization&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>StreamSerializer</code> cannot be created for well-known types (e.g. Long, String) and primitive arrays. Hazelcast already registers these types.</em></p>
<p><br></br></p>
<a name="streamserializer-example-code-2"></a><h4 id="streamserializer-example-code-2">StreamSerializer Example Code 2</h4>
<p>Let&#39;s take a look at another example implementing <code>StreamSerializer</code>.</p>
<pre><code class="lang-java">public class Foo {
  private String foo;

  public String getFoo() {
    return foo;
  }

  public void setFoo( String foo ) {
    this.foo = foo;
  }
}
</code></pre>
<p>Assume that our custom serialization will serialize
Foo into XML. First you need to implement a
<code>com.hazelcast.nio.serialization.StreamSerializer</code>. A very simple one that uses XMLEncoder and XMLDecoder could look like the following:</p>
<pre><code class="lang-java">public static class FooXmlSerializer implements StreamSerializer&lt;Foo&gt; {

  @Override
  public int getTypeId() {
    return 10;
  }

  @Override
  public void write( ObjectDataOutput out, Foo object ) throws IOException {
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    XMLEncoder encoder = new XMLEncoder( bos );
    encoder.writeObject( object );
    encoder.close();
    out.write( bos.toByteArray() );
  }

  @Override
  public Foo read( ObjectDataInput in ) throws IOException {
    InputStream inputStream = (InputStream) in;
    XMLDecoder decoder = new XMLDecoder( inputStream );
    return (Foo) decoder.readObject();
  }

  @Override
  public void destroy() {
  }
}
</code></pre>
<a name="configuring-streamserializer"></a><h4 id="configuring-streamserializer">Configuring StreamSerializer</h4>
<p>Note that <code>typeId</code> must be unique because Hazelcast will use it to look up the <code>StreamSerializer</code> while it deserializes the object. The last required step is to register the <code>StreamSerializer</code> in your Hazelcast configuration. Below are the programmatic and declarative configurations for this step.</p>
<pre><code class="lang-java">SerializerConfig sc = new SerializerConfig()
    .setImplementation(new FooXmlSerializer())
    .setTypeClass(Foo.class);
Config config = new Config();
config.getSerializationConfig().addSerializerConfig(sc);
</code></pre>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;serialization&gt;
    &lt;serializers&gt;
      &lt;serializer type-class=&quot;com.www.Foo&quot; class-name=&quot;com.www.FooXmlSerializer&quot; /&gt;
    &lt;/serializers&gt;
  &lt;/serialization&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>From now on, this Hazelcast example will use <code>FooXmlSerializer</code>
to serialize Foo objects. In this way, you can write an adapter (StreamSerializer) for any Serialization framework and plug it into Hazelcast.</p>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#serialization-configuration-wrap-up">Serialization Configuration Wrap-Up section</a> for a full description of Hazelcast Serialization configuration.</em></p>

<a name="implementing-bytearrayserializer"></a><h3 id="implementing-bytearrayserializer">Implementing ByteArraySerializer</h3>
<p><code>ByteArraySerializer</code> exposes the raw ByteArray used internally by Hazelcast. It is a good option if the serialization library you are using deals with ByteArrays instead of streams.</p>
<p>Let&#39;s implement <code>ByteArraySerializer</code> for the <code>Employee</code> class mentioned in <a href="#implementing-streamserializer">Implementing StreamSerializer</a>.</p>
<pre><code class="lang-java">public class EmployeeByteArraySerializer
    implements ByteArraySerializer&lt;Employee&gt; {

  @Override
  public void destroy () { 
  }

  @Override
  public int getTypeId () {
    return 1; 
  }

  @Override
  public byte[] write( Employee object )
      throws IOException { 
    return object.getName().getBytes();
  }

  @Override
  public Employee read( byte[] buffer ) 
      throws IOException { 
    String surname = new String( buffer );
    return new Employee( surname );
  }
}
</code></pre>
<a name="configuring-bytearrayserializer"></a><h4 id="configuring-bytearrayserializer">Configuring ByteArraySerializer</h4>
<p>As usual, let&#39;s register the <code>EmployeeByteArraySerializer</code> in the configuration file <code>hazelcast.xml</code>, as shown below.</p>
<pre><code class="lang-xml">&lt;serialization&gt;
  &lt;serializers&gt;
    &lt;serializer type-class=&quot;Employee&quot;&gt;EmployeeByteArraySerializer&lt;/serializer&gt;
  &lt;/serializers&gt;
&lt;/serialization&gt;
</code></pre>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#serialization-configuration-wrap-up">Serialization Configuration Wrap-Up section</a> for a full description of Hazelcast Serialization configuration.</em></p>

<a name="global-serializer"></a><h2 id="global-serializer">Global Serializer</h2>
<p>The global serializer is identical to <a href="#custom-serialization">custom serializers</a> from the implementation perspective. The global serializer is registered as a fallback serializer to handle all other objects if a serializer cannot be located for them.</p>
<p>By default, the global serializer does not handle <code>java.io.Serializable</code> and <code>java.io.Externalizable</code> instances. However, you can configure it to be responsible for those instances.</p>
<p>A custom serializer should be registered for a specific class type. The global serializer will handle all class types if all the steps in searching for a serializer fail as described in <a href="#serialization-interface-types">Serialization Interface Types</a>.</p>
<p><strong>Use cases</strong></p>
<ul>
<li><p>Third party serialization frameworks can be integrated using the global serializer.</p>
</li>
<li><p>For your custom objects, you can implement a single serializer to handle all of them.</p>
</li>
<li><p>You can replace the internal Java serialization by enabling the <code>overrideJavaSerialization</code> option of the global serializer configuration.</p>
</li>
</ul>
<p>Any custom serializer can be used as the global serializer. Please refer to the <a href="#custom-serialization">Custom Serialization section</a> for implementation details.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>To function properly, Hazelcast needs the Java serializable objects to be handled correctly. If the global serializer is configured to handle the Java serialization, the global serializer must properly serialize/deserialize the <code>java.io.Serializable</code> instances. Otherwise, it causes Hazelcast to malfunction.</em></p>
<a name="sample-global-serializer"></a><h3 id="sample-global-serializer">Sample Global Serializer</h3>
<p>A sample global serializer that integrates with a third party serializer is shown below.</p>
<pre><code class="lang-java">public class GlobalStreamSerializer
    implements StreamSerializer&lt;Object&gt; {

  private SomeThirdPartySerializer someThirdPartySerializer;

  private init() {
    //someThirdPartySerializer  = ... 
  }

  @Override
  public int getTypeId () {
    return 123; 
  }

  @Override
  public void write( ObjectDataOutput out, Object object ) throws IOException { 
     byte[] bytes = someThirdPartySerializer.encode(object);
     out.writeByteArray(bytes);
  }

  @Override
  public Object read( ObjectDataInput in ) throws IOException { 
    byte[] bytes = in.readByteArray();
    return someThirdPartySerializer.decode(bytes);
  }

  @Override
  public void destroy () {
     someThirdPartySerializer.destroy();
  }
}
</code></pre>
<p>Now, we can register the global serializer in the configuration file <code>hazelcast.xml</code>, as shown below.</p>
<pre><code class="lang-xml">&lt;serialization&gt;
  &lt;serializers&gt;
    &lt;global-serializer override-java-serialization=&quot;true&quot;&gt;GlobalStreamSerializer&lt;/global-serializer&gt;
  &lt;/serializers&gt;
&lt;/serialization&gt;
</code></pre>

<a name="implementing-hazelcastinstanceaware"></a><h2 id="implementing-hazelcastinstanceaware">Implementing HazelcastInstanceAware</h2>
<p>You can implement the <code>HazelcastInstanceAware</code> interface to access distributed objects for cases where an object is deserialized and needs access to HazelcastInstance.</p>
<p>Let&#39;s implement it for the <code>Employee</code> class mentioned in the <a href="#custom-serialization">Custom Serialization section</a>.</p>
<pre><code class="lang-java">public class Employee
    implements Serializable, HazelcastInstanceAware { 

  private static final long serialVersionUID = 1L;
  private String surname;
  private transient HazelcastInstance hazelcastInstance;

  public Person( String surname ) { 
    this.surname = surname;
  }

  @Override
  public void setHazelcastInstance( HazelcastInstance hazelcastInstance ) {
    this.hazelcastInstance = hazelcastInstance;
    System.out.println( &quot;HazelcastInstance set&quot; ); 
  }

  @Override
  public String toString() {
    return String.format( &quot;Person(surname=%s)&quot;, surname ); 
  }
}
</code></pre>
<p>After deserialization, the object is checked to see if it implements <code>HazelcastInstanceAware</code> and the method <code>setHazelcastInstance</code> is called. Notice the <code>hazelcastInstance</code> is <code>transient</code>. This is because this field should not be serialized.</p>
<p>It may be a good practice to inject a HazelcastInstance into a domain object (e.g. <code>Employee</code> in the above sample) when used together with <code>Runnable</code>/<code>Callable</code> implementations. These runnables/callables are executed by <code>IExecutorService</code> which sends them to another machine. And after a task is deserialized, run/call method implementations need to access HazelcastInstance.</p>
<p>We recommend you only set the HazelcastInstance field while using <code>setHazelcastInstance</code> method and you not execute operations on the HazelcastInstance. The reason is that when HazelcastInstance is injected for a <code>HazelcastInstanceAware</code> implementation, it may not be up and running at the injection time.</p>
<p><br></br></p>

<a name="serialization-configuration-wrap-up"></a><h2 id="serialization-configuration-wrap-up">Serialization Configuration Wrap-Up</h2>
<p>This section summarizes the configuration of serialization options, explained in the above sections, into all-in-one examples. The following are example serialization configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;serialization&gt;
   &lt;portable-version&gt;2&lt;/portable-version&gt;
   &lt;use-native-byte-order&gt;true&lt;/use-native-byte-order&gt;
   &lt;byte-order&gt;BIG_ENDIAN&lt;/byte-order&gt;
   &lt;enable-compression&gt;true&lt;/enable-compression&gt;
   &lt;enable-shared-object&gt;false&lt;/enable-shared-object&gt;
   &lt;allow-unsafe&gt;true&lt;/allow-unsafe&gt;
   &lt;data-serializable-factories&gt;
      &lt;data-serializable-factory factory-id=&quot;1001&quot;&gt;
          abc.xyz.Class
      &lt;/data-serializable-factory&gt;
   &lt;/data-serializable-factories&gt;
   &lt;portable-factories&gt;
      &lt;portable-factory factory-id=&quot;9001&quot;&gt;
         xyz.abc.Class
      &lt;/portable-factory&gt;
   &lt;/portable-factories&gt;
   &lt;serializers&gt;
      &lt;global-serializer&gt;abc.Class&lt;/global-serializer&gt;
      &lt;serializer type-class=&quot;Employee&quot; class-name=&quot;com.EmployeeSerializer&quot;&gt;
      &lt;/serializer&gt;
   &lt;/serializers&gt;
   &lt;check-class-def-errors&gt;true&lt;/check-class-def-errors&gt;
&lt;/serialization&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">Config config = new Config();
SerializationConfig srzConfig = config.getSerializationConfig();
srzConfig.setPortableVersion( &quot;2&quot; ).setUseNativeByteOrder( true );
srzConfig.setAllowUnsafe( true ).setEnableCompression( true );
srzConfig.setCheckClassDefErrors( true );

GlobalSerializerConfig globSrzConfig = srzConfig.getGlobalSerializerConfig();
globSrzConfig.setClassName( &quot;abc.Class&quot; );

SerializerConfig serializerConfig = srzConfig.getSerializerConfig();
serializerConfig.setTypeClass( &quot;Employee&quot; )
                .setClassName( &quot;com.EmployeeSerializer&quot; );
</code></pre>
<p>Serialization configuration has the following elements.</p>
<ul>
<li><code>portable-version</code>: Defines versioning of the portable serialization. Portable version differentiates two of the same classes that have changes, such as adding/removing field or changing a type of a field.</li>
<li><code>use-native-byte-order</code>: Set to <code>true</code> to use native byte order for the underlying platform. </li>
<li><code>byte-order</code>: Defines the byte order that the serialization will use: <code>BIG_ENDIAN</code> or <code>LITTLE_ENDIAN</code>. The default value is <code>BIG_ENDIAN</code>.</li>
<li><code>enable-compression</code>: Enables compression if default Java serialization is used. </li>
<li><code>enable-shared-object</code>: Enables shared object if default Java serialization is used. </li>
<li><code>allow-unsafe</code>: Set to <code>true</code> to allow <code>unsafe</code> to be used. </li>
<li><code>data-serializable-factory</code>: The DataSerializableFactory class to be registered.</li>
<li><code>portable-factory</code>: The PortableFactory class to be registered.</li>
<li><code>global-serializer</code>: The global serializer class to be registered if no other serializer is applicable.</li>
<li><code>serializer</code>: The class name of the serializer implementation.</li>
<li><code>check-class-def-errors</code>: When set to <code>true</code>, the serialization system will check for class definitions error at start and will throw a Serialization Exception with an error definition.</li>
</ul>

<a name="management"></a><h1 id="management">Management</h1>
<p>This chapter provides information on managing and monitoring your Hazelcast cluster. It gives detailed instructions related to gathering statistics, monitoring via JMX protocol, and managing the cluster with useful utilities. It also explains how to use Hazelcast Management Center.</p>
<a name="getting-member-statistics"></a><h2 id="getting-member-statistics">Getting Member Statistics</h2>
<p>You can get various statistics from your distributed data structures via the Statistics API.
Since the data structures are distributed in the cluster, the Statistics API provides
statistics for the local portion (1/Number of Members in the Cluster) of data on each member. </p>
<a name="map-statistics"></a><h3 id="map-statistics">Map Statistics</h3>
<p>To get local map statistics, use the <code>getLocalMapStats()</code> method from the <code>IMap</code> interface. This method returns a
<code>LocalMapStats</code> object that holds local map statistics.</p>
<p>Below is example code where the <code>getLocalMapStats()</code> method and the <code>getOwnedEntryCount()</code> method get the number of entries owned by this member.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
IMap&lt;String, Customer&gt; customers = hazelcastInstance.getMap( &quot;customers&quot; );
LocalMapStats mapStatistics = customers.getLocalMapStats;
System.out.println( &quot;number of entries owned on this member = &quot;
    + mapStatistics.getOwnedEntryCount() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>LocalMapStats</code> object.</p>
<pre><code class="lang-java">/**
 * Returns the number of entries owned by this member.
 */
long getOwnedEntryCount();

/**
 * Returns the number of backup entries hold by this member.
 */
long getBackupEntryCount();

/**
 * Returns the number of backups per entry.
 */
int getBackupCount();

/**
 * Returns memory cost (number of bytes) of owned entries in this member.
 */
long getOwnedEntryMemoryCost();

/**
 * Returns memory cost (number of bytes) of backup entries in this member.
 */
long getBackupEntryMemoryCost();

/**
 * Returns the creation time of this map on this member.
 */
long getCreationTime();

/**
 * Returns the last access (read) time of the locally owned entries.
 */
long getLastAccessTime();

/**
 * Returns the last update time of the locally owned entries.
 */
long getLastUpdateTime();

/**
 * Returns the number of hits (reads) of the locally owned entries.
 */
long getHits();

/**
 * Returns the number of currently locked locally owned keys.
 */
long getLockedEntryCount();

/**
 * Returns the number of entries that the member owns and are dirty (updated
 * but not persisted yet).
 * dirty entry count is meaningful when there is a persistence defined.
 */
long getDirtyEntryCount();

/**
 * Returns the number of put operations.
 */
long getPutOperationCount();

/**
 * Returns the number of get operations.
 */
long getGetOperationCount();

/**
 * Returns the number of Remove operations.
 */
long getRemoveOperationCount();

/**
 * Returns the total latency of put operations. To get the average latency,
 * divide by number of puts
 */
long getTotalPutLatency();

/**
 * Returns the total latency of get operations. To get the average latency,
 * divide by the number of gets.
 */
long getTotalGetLatency();

/**
 * Returns the total latency of remove operations. To get the average latency,
 * divide by the number of gets.
 */
long getTotalRemoveLatency();

/**
 * Returns the maximum latency of put operations. To get the average latency,
 * divide by the number of puts.
 */
long getMaxPutLatency();

/**
 * Returns the maximum latency of get operations. To get the average latency,
 * divide by the number of gets.
 */
long getMaxGetLatency();

/**
 * Returns the maximum latency of remove operations. To get the average latency,
 * divide by the number of gets.
 */
long getMaxRemoveLatency();

/**
 * Returns the number of Events Received.
 */
long getEventOperationCount();

/**
 * Returns the total number of Other Operations.
 */
long getOtherOperationCount();

/**
 * Returns the total number of total operations.
 */
long total();

/**
 * Cost of map &amp; near cache &amp; backup in bytes.
 * todo: in object mode, object size is zero.
 */
long getHeapCost();

/**
 * Returns statistics related to the Near Cache.
 */
NearCacheStats getNearCacheStats();
</code></pre>
<a name="near-cache-statistics"></a><h4 id="near-cache-statistics">Near Cache Statistics</h4>
<p>To get Near Cache statistics, use the <code>getNearCacheStats()</code> method from the <code>LocalMapStats</code> object.
This method returns a <code>NearCacheStats</code> object that holds Near Cache statistics.</p>
<p>Below is example code where the <code>getNearCacheStats()</code> method and the <code>getRatio</code> method from <code>NearCacheStats</code> get a Near Cache hit/miss ratio. </p>
<pre><code class="lang-java">HazelcastInstance node = Hazelcast.newHazelcastInstance();
IMap&lt;String, Customer&gt; customers = node.getMap( &quot;customers&quot; );
LocalMapStats mapStatistics = customers.getLocalMapStats();
NearCacheStats nearCacheStatistics = mapStatistics.getNearCacheStats();
System.out.println( &quot;near cache hit/miss ratio= &quot;
    + nearCacheStatistics.getRatio() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>NearCacheStats</code> object.
This behavior applies to both client and member near caches.</p>
<pre><code class="lang-java">/**
 * Returns the creation time of this NearCache on this member
 */
long getCreationTime();

/**
 * Returns the number of entries owned by this member.
 */
long getOwnedEntryCount();

/**
 * Returns memory cost (number of bytes) of entries in this cache.
 */
long getOwnedEntryMemoryCost();

/**
 * Returns the number of hits (reads) of the locally owned entries.
 */
long getHits();

/**
 * Returns the number of misses  of the locally owned entries.
 */
long getMisses();

/**
 * Returns the hit/miss ratio  of the locally owned entries.
 */
double getRatio();
</code></pre>
<a name="multimap-statistics"></a><h3 id="multimap-statistics">Multimap Statistics</h3>
<p>To get MultiMap statistics, use the <code>getLocalMultiMapStats()</code> method from the <code>MultiMap</code> interface.
This method returns a <code>LocalMultiMapStats</code> object that holds local MultiMap statistics.</p>
<p>Below is example code where the <code>getLocalMultiMapStats()</code> method and the <code>getLastUpdateTime</code> method from <code>LocalMultiMapStats</code> get the last update time.</p>
<pre><code class="lang-java">HazelcastInstance node = Hazelcast.newHazelcastInstance();
MultiMap&lt;String, Customer&gt; customers = node.getMultiMap( &quot;customers&quot; );
LocalMultiMapStats multiMapStatistics = customers.getLocalMultiMapStats();
System.out.println( &quot;last update time =  &quot;
    + multiMapStatistics.getLastUpdateTime() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>LocalMultiMapStats</code> object.</p>
<pre><code class="lang-java">/**
 * Returns the number of entries owned by this member.
 */
long getOwnedEntryCount();

/**
 * Returns the number of backup entries hold by this member.
 */
long getBackupEntryCount();

/**
 * Returns the number of backups per entry.
 */
int getBackupCount();

/**
 * Returns memory cost (number of bytes) of owned entries in this member.
 */
long getOwnedEntryMemoryCost();

/**
 * Returns memory cost (number of bytes) of backup entries in this member.
 */
long getBackupEntryMemoryCost();

/**
 * Returns the creation time of this map on this member.
 */
long getCreationTime();

/**
 * Returns the last access (read) time of the locally owned entries.
 */
long getLastAccessTime();

/**
 * Returns the last update time of the locally owned entries.
 */
long getLastUpdateTime();

/**
 * Returns the number of hits (reads) of the locally owned entries.
 */
long getHits();

/**
 * Returns the number of currently locked locally owned keys.
 */
long getLockedEntryCount();

/**
 * Returns the number of entries that the member owns and are dirty (updated
 * but not persisted yet).
 * Dirty entry count is meaningful when a persistence is defined.
 */
long getDirtyEntryCount();

/**
 * Returns the number of put operations.
 */
long getPutOperationCount();

/**
 * Returns the number of get operations.
 */
long getGetOperationCount();

/**
 * Returns the number of Remove operations.
 */
long getRemoveOperationCount();

/**
 * Returns the total latency of put operations. To get the average latency,
 * divide by the number of puts.
 */
long getTotalPutLatency();

/**
 * Returns the total latency of get operations. To get the average latency,
 * divide by the number of gets.
 */
long getTotalGetLatency();

/**
 * Returns the total latency of remove operations. To get the average latency,
 * divide by the number of gets.
 */
long getTotalRemoveLatency();

/**
 * Returns the maximum latency of put operations. To get the average latency,
 * divide by the number of puts.
 */
long getMaxPutLatency();

/**
 * Returns the maximum latency of get operations. To get the average latency,
 * divide by the number of gets.
 */
long getMaxGetLatency();

/**
 * Returns the maximum latency of remove operations. To get the average latency,
 * divide by the number of gets.
 */
long getMaxRemoveLatency();

/**
 * Returns the number of Events Received.
 */
long getEventOperationCount();

/**
 * Returns the total number of Other Operations.
 */
long getOtherOperationCount();

/**
 * Returns the total number of total operations.
 */
long total();

/**
 * Cost of map &amp; near cache  &amp; backup in bytes.
 * todo: in object mode, object size is zero.
 */
long getHeapCost();
</code></pre>
<a name="queue-statistics"></a><h3 id="queue-statistics">Queue Statistics</h3>
<p>To get local queue statistics, use the <code>getLocalQueueStats()</code> method from the <code>IQueue</code> interface.
This method returns a <code>LocalQueueStats</code> object that holds local queue statistics.</p>
<p>Below is example code where the <code>getLocalQueueStats()</code> method and the <code>getAvgAge</code> method from <code>LocalQueueStats</code> get the average age of items.</p>
<pre><code class="lang-java">HazelcastInstance node = Hazelcast.newHazelcastInstance();
IQueue&lt;Order&gt; orders = node.getQueue( &quot;orders&quot; );
LocalQueueStats queueStatistics = orders.getLocalQueueStats();
System.out.println( &quot;average age of items = &quot; 
    + queueStatistics.getAvgAge() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>LocalQueueStats</code> object.</p>
<pre><code class="lang-java">/**
 * Returns the number of owned items in this member.
 */
long getOwnedItemCount();

/**
 * Returns the number of backup items in this member.
 */
long getBackupItemCount();

/**
 * Returns the min age of the items in this member.
 */
long getMinAge();

/**
 * Returns the max age of the items in this member.
 */
long getMaxAge();

/**
 * Returns the average age of the items in this member.
 */
long getAvgAge();

/**
 * Returns the number of offer/put/add operations.
 * Offers returning false will be included.
 * #getRejectedOfferOperationCount can be used
 * to get the rejected offers.
 */
long getOfferOperationCount();

/**
 * Returns the number of rejected offers. Offer
 * can be rejected because of max-size limit
 * on the queue.
 */
long getRejectedOfferOperationCount();

/**
 * Returns the number of poll/take/remove operations.
 * Polls returning null (empty) will be included.
 * #getEmptyPollOperationCount can be used to get the
 * number of polls returned null.
 */
long getPollOperationCount();

/**
 * Returns the number of null returning poll operations.
 * Poll operation might return null if the queue is empty.
 */
long getEmptyPollOperationCount();

/**
 * Returns the number of other operations.
 */
long getOtherOperationsCount();

/**
 * Returns the number of event operations.
 */
long getEventOperationCount();
</code></pre>
<a name="topic-statistics"></a><h3 id="topic-statistics">Topic Statistics</h3>
<p>To get local topic statistics, use the <code>getLocalTopicStats()</code> method from the <code>ITopic</code> interface.
This method returns a <code>LocalTopicStats</code> object that holds local topic statistics.</p>
<p>Below is example code where the <code>getLocalTopicStats()</code> method and the <code>getPublishOperationCount</code> method from <code>LocalTopicStats</code> get the number of publish operations.</p>
<pre><code class="lang-java">HazelcastInstance node = Hazelcast.newHazelcastInstance();
ITopic&lt;Object&gt; news = node.getTopic( &quot;news&quot; );
LocalTopicStats topicStatistics = news.getLocalTopicStats();
System.out.println( &quot;number of publish operations = &quot; 
    + topicStatistics.getPublishOperationCount() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>LocalTopicStats</code> object.</p>
<pre><code class="lang-java">/**
 * Returns the creation time of this topic on this member.
 */
long getCreationTime();

/**
 * Returns the total number of published messages of this topic on this member.
 */
long getPublishOperationCount();

/**
 * Returns the total number of received messages of this topic on this member.
 */
long getReceiveOperationCount();
</code></pre>
<a name="executor-statistics"></a><h3 id="executor-statistics">Executor Statistics</h3>
<p>To get local executor statistics, use the <code>getLocalExecutorStats()</code> method from the <code>IExecutorService</code> interface.
This method returns a <code>LocalExecutorStats</code> object that holds local executor statistics.</p>
<p>Below is example code where the <code>getLocalExecutorStats()</code> method and the <code>getCompletedTaskCount</code> method from <code>LocalExecutorStats</code> get the number of completed operations of the executor service.</p>
<pre><code class="lang-java">HazelcastInstance node = Hazelcast.newHazelcastInstance();
IExecutorService orderProcessor = node.getExecutorService( &quot;orderProcessor&quot; );
LocalExecutorStats executorStatistics = orderProcessor.getLocalExecutorStats();
System.out.println( &quot;completed task count = &quot; 
    + executorStatistics.getCompletedTaskCount() );
</code></pre>
<p>Below is the list of metrics that you can access via the <code>LocalExecutorStats</code> object.</p>
<pre><code class="lang-java">/**
 * Returns the number of pending operations of the executor service.
 */
long getPendingTaskCount();

/**
 * Returns the number of started operations of the executor service.
 */
long getStartedTaskCount();

/**
 * Returns the number of completed operations of the executor service.
 */
long getCompletedTaskCount();

/**
 * Returns the number of cancelled operations of the executor service.
 */
long getCancelledTaskCount();

/**
 * Returns the total start latency of operations started.
 */
long getTotalStartLatency();

/**
 * Returns the total execution time of operations finished.
 */
long getTotalExecutionLatency();
</code></pre>

<a name="jmx-api-per-member"></a><h2 id="jmx-api-per-member">JMX API per Member</h2>
<p>Hazelcast members expose various management beans which include statistics about distributed data structures and the states of Hazelcast member internals.</p>
<p>The metrics are local to the members, i.e. they do not reflect cluster wide values.</p>
<p>You can find the JMX API definition below with descriptions and the API methods in parenthesis.</p>
<p><strong>Atomic Long (<code>IAtomicLong</code>)</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Current Value ( <code>currentValue</code> )</li>
<li>Set Value ( <code>set(v)</code> )</li>
<li>Add value and Get ( <code>addAndGet(v)</code> )</li>
<li>Compare and Set ( <code>compareAndSet(e,v)</code> )</li>
<li>Decrement and Get ( <code>decrementAndGet()</code> )</li>
<li>Get and Add ( <code>getAndAdd(v)</code> )</li>
<li>Get and Increment ( <code>getAndIncrement()</code> )</li>
<li>Get and Set ( <code>getAndSet(v)</code> )</li>
<li>Increment and Get ( <code>incrementAndGet()</code> )</li>
<li>Partition key ( <code>partitionKey</code> )</li>
</ul>
<p><strong>Atomic Reference ( <code>IAtomicReference</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Partition key  ( <code>partitionKey</code>)</li>
</ul>
<p><strong>Countdown Latch ( <code>ICountDownLatch</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Current count ( <code>count</code>)</li>
<li>Countdown ( <code>countDown()</code> )</li>
<li>Partition key  ( <code>partitionKey</code>)</li>
</ul>
<p><strong>Executor Service ( <code>IExecutorService</code> )</strong></p>
<ul>
<li>Local pending operation count ( <code>localPendingTaskCount</code> )</li>
<li>Local started operation count ( <code>localStartedTaskCount</code> )</li>
<li>Local completed operation count ( <code>localCompletedTaskCount</code> )</li>
<li>Local cancelled operation count ( <code>localCancelledTaskCount</code> )</li>
<li>Local total start latency ( <code>localTotalStartLatency</code> )</li>
<li>Local total execution latency ( <code>localTotalExecutionLatency</code> )</li>
</ul>
<p><strong>List ( <code>IList</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Clear list ( <code>clear</code> )</li>
</ul>
<p><strong>Lock ( <code>ILock</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Lock Object ( <code>lockObject</code> )</li>
<li>Partition key ( <code>partitionKey</code> )</li>
</ul>
<p><strong>Map ( <code>IMap</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Size ( <code>size</code> )</li>
<li>Config ( <code>config</code> )</li>
<li>Owned entry count ( <code>localOwnedEntryCount</code> )</li>
<li>Owned entry memory cost ( <code>localOwnedEntryMemoryCost</code> )</li>
<li>Backup entry count ( <code>localBackupEntryCount</code> )</li>
<li>Backup entry cost ( <code>localBackupEntryMemoryCost</code> )</li>
<li>Backup count ( <code>localBackupCount</code> )</li>
<li>Creation time ( <code>localCreationTime</code> )</li>
<li>Last access time ( <code>localLastAccessTime</code> )</li>
<li>Last update time ( <code>localLastUpdateTime</code> )</li>
<li>Hits ( <code>localHits</code> )</li>
<li>Locked entry count ( <code>localLockedEntryCount</code> )</li>
<li>Dirty entry count ( <code>localDirtyEntryCount</code> )</li>
<li>Put operation count ( <code>localPutOperationCount</code> )</li>
<li>Get operation count ( <code>localGetOperationCount</code> )</li>
<li>Remove operation count ( <code>localRemoveOperationCount</code> )</li>
<li>Total put latency ( <code>localTotalPutLatency</code> )</li>
<li>Total get latency ( <code>localTotalGetLatency</code> )</li>
<li>Total remove latency ( <code>localTotalRemoveLatency</code> )</li>
<li>Max put latency ( <code>localMaxPutLatency</code> )</li>
<li>Max get latency ( <code>localMaxGetLatency</code> )</li>
<li>Max remove latency ( <code>localMaxRemoveLatency</code> )</li>
<li>Event count ( <code>localEventOperationCount</code> )</li>
<li>Other (keySet,entrySet etc..) operation count ( <code>localOtherOperationCount</code> )</li>
<li>Total operation count ( <code>localTotal</code> )</li>
<li>Heap Cost ( <code>localHeapCost</code> )</li>
<li>Clear ( <code>clear()</code> )</li>
<li>Values ( <code>values(p)</code>)</li>
<li>Entry Set ( <code>entrySet(p)</code> )</li>
</ul>
<p><strong>MultiMap ( <code>MultiMap</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Size ( <code>size</code> )</li>
<li>Owned entry count ( <code>localOwnedEntryCount</code> )</li>
<li>Owned entry memory cost ( <code>localOwnedEntryMemoryCost</code> )</li>
<li>Backup entry count ( <code>localBackupEntryCount</code> )</li>
<li>Backup entry cost ( <code>localBackupEntryMemoryCost</code> )</li>
<li>Backup count ( <code>localBackupCount</code> )</li>
<li>Creation time ( <code>localCreationTime</code> )</li>
<li>Last access time ( <code>localLastAccessTime</code> )</li>
<li>Last update time ( <code>localLastUpdateTime</code> )</li>
<li>Hits ( <code>localHits</code> )</li>
<li>Locked entry count ( <code>localLockedEntryCount</code> )</li>
<li>Put operation count ( <code>localPutOperationCount</code> )</li>
<li>Get operation count ( <code>localGetOperationCount</code> )</li>
<li>Remove operation count ( <code>localRemoveOperationCount</code> )</li>
<li>Total put latency ( <code>localTotalPutLatency</code> )</li>
<li>Total get latency ( <code>localTotalGetLatency</code> )</li>
<li>Total remove latency ( <code>localTotalRemoveLatency</code> )</li>
<li>Max put latency ( <code>localMaxPutLatency</code> )</li>
<li>Max get latency ( <code>localMaxGetLatency</code> )</li>
<li>Max remove latency ( <code>localMaxRemoveLatency</code> )</li>
<li>Event count ( <code>localEventOperationCount</code> )</li>
<li>Other (keySet,entrySet etc..) operation count ( <code>localOtherOperationCount</code> )</li>
<li>Total operation count ( <code>localTotal</code> )</li>
<li>Clear ( <code>clear()</code> )</li>
</ul>
<p><strong>Replicated Map ( <code>ReplicatedMap</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Size ( <code>size</code> )</li>
<li>Config ( <code>config</code> )</li>
<li>Owned entry count ( <code>localOwnedEntryCount</code> )</li>
<li>Creation time ( <code>localCreationTime</code> )</li>
<li>Last access time ( <code>localLastAccessTime</code> )</li>
<li>Last update time ( <code>localLastUpdateTime</code> )</li>
<li>Hits ( <code>localHits</code> )</li>
<li>Put operation count ( <code>localPutOperationCount</code> )</li>
<li>Get operation count ( <code>localGetOperationCount</code> )</li>
<li>Remove operation count ( <code>localRemoveOperationCount</code> )</li>
<li>Total put latency ( <code>localTotalPutLatency</code> )</li>
<li>Total get latency ( <code>localTotalGetLatency</code> )</li>
<li>Total remove latency ( <code>localTotalRemoveLatency</code> )</li>
<li>Max put latency ( <code>localMaxPutLatency</code> )</li>
<li>Max get latency ( <code>localMaxGetLatency</code> )</li>
<li>Max remove latency ( <code>localMaxRemoveLatency</code> )</li>
<li>Event count ( <code>localEventOperationCount</code> )</li>
<li>Replication event count ( <code>localReplicationEventCount</code> )</li>
<li>Other (keySet,entrySet etc..) operation count ( <code>localOtherOperationCount</code> )</li>
<li>Total operation count ( <code>localTotal</code> )</li>
<li>Clear ( <code>clear()</code> )</li>
<li>Values ( <code>values()</code>)</li>
<li>Entry Set ( <code>entrySet()</code> )</li>
</ul>
<p><strong>Queue ( <code>IQueue</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Config ( <code>QueueConfig</code> )</li>
<li>Partition key ( <code>partitionKey</code> )</li>
<li>Owned item count ( <code>localOwnedItemCount</code> )</li>
<li>Backup item count ( <code>localBackupItemCount</code> )</li>
<li>Minimum age ( <code>localMinAge</code> )</li>
<li>Maximum age ( <code>localMaxAge</code> )</li>
<li>Average age ( <code>localAveAge</code> )</li>
<li>Offer operation count ( <code>localOfferOperationCount</code> )</li>
<li>Rejected offer operation count ( <code>localRejectedOfferOperationCount</code> )</li>
<li>Poll operation count ( <code>localPollOperationCount</code> )</li>
<li>Empty poll operation count ( <code>localEmptyPollOperationCount</code> )</li>
<li>Other operation count ( <code>localOtherOperationsCount</code> )</li>
<li>Event operation count ( <code>localEventOperationCount</code> )</li>
<li>Clear ( <code>clear()</code> )</li>
</ul>
<p><strong>Semaphore ( <code>ISemaphore</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Available permits ( <code>available</code> )</li>
<li>Partition key ( <code>partitionKey</code> )</li>
<li>Drain ( <code>drain()</code>)</li>
<li>Shrink available permits by given number ( <code>reduce(v)</code> )</li>
<li>Release given number of permits ( <code>release(v)</code> )</li>
</ul>
<p><strong>Set ( <code>ISet</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Partition key ( <code>partitionKey</code> )</li>
<li>Clear ( <code>clear()</code> )</li>
</ul>
<p><strong>Topic ( <code>ITopic</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Config ( <code>config</code> )</li>
<li>Creation time ( <code>localCreationTime</code> )</li>
<li>Publish operation count ( <code>localPublishOperationCount</code> )</li>
<li>Receive operation count ( <code>localReceiveOperationCount</code> )</li>
</ul>
<p><strong>Hazelcast Instance ( <code>HazelcastInstance</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Version ( <code>version</code> )</li>
<li>Build ( <code>build</code> )</li>
<li>Configuration ( <code>config</code> )</li>
<li>Configuration source ( <code>configSource</code> )</li>
<li>Group name ( <code>groupName</code> )</li>
<li>Network Port ( <code>port</code> )</li>
<li>Cluster-wide Time ( <code>clusterTime</code> )</li>
<li>Size of the cluster ( <code>memberCount</code> )</li>
<li>List of members ( <code>Members</code> )</li>
<li>Running state ( <code>running</code> )</li>
<li>Shutdown the member ( <code>shutdown()</code> )</li>
</ul>
<ul>
<li><strong>Node ( <code>HazelcastInstance.Node</code> )</strong><ul>
<li>Address ( <code>address</code> )</li>
<li>Master address ( <code>masterAddress</code> )</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Event Service ( <code>HazelcastInstance.EventService</code> )</strong><ul>
<li>Event thread count  ( <code>eventThreadCount</code> )</li>
<li>Event queue size ( <code>eventQueueSize</code> )</li>
<li>Event queue capacity ( <code>eventQueueCapacity</code> )</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Operation Service ( <code>HazelcastInstance.OperationService</code> )</strong></p>
<ul>
<li>Response queue size  ( <code>responseQueueSize</code> )</li>
<li>Operation executor queue size ( <code>operationExecutorQueueSize</code> )</li>
<li>Running operation count ( <code>runningOperationsCount</code> )</li>
<li>Remote operation count ( <code>remoteOperationCount</code> )</li>
<li>Executed operation count ( <code>executedOperationCount</code> )</li>
<li>Operation thread count ( <code>operationThreadCount</code> )</li>
</ul>
</li>
<li><p><strong>Proxy Service ( <code>HazelcastInstance.ProxyService</code> )</strong></p>
<ul>
<li>Proxy count ( <code>proxyCount</code> )</li>
</ul>
</li>
<li><p><strong>Partition Service ( <code>HazelcastInstance.PartitionService</code> )</strong></p>
<ul>
<li>Partition count ( <code>partitionCount</code> )</li>
<li>Active partition count ( <code>activePartitionCount</code> )</li>
<li>Cluster Safe State ( <code>isClusterSafe</code> )</li>
<li>LocalMember Safe State ( <code>isLocalMemberSafe</code> )</li>
</ul>
</li>
<li><p><strong>Connection Manager ( <code>HazelcastInstance.ConnectionManager</code> )</strong></p>
<ul>
<li>Client connection count ( <code>clientConnectionCount</code> )</li>
<li>Active connection count ( <code>activeConnectionCount</code> )</li>
<li>Connection count ( <code>connectionCount</code> )</li>
</ul>
</li>
<li><p><strong>Client Engine ( <code>HazelcastInstance.ClientEngine</code> )</strong></p>
<ul>
<li>Client endpoint count ( <code>clientEndpointCount</code> )</li>
</ul>
</li>
<li><p><strong>System Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )   </li>
</ul>
</li>
<li><p><strong>Operation Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
<li><p><strong>Async Executor (<code>HazelcastInstance.ManagedExecutorService</code>)</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
<li><p><strong>Scheduled Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
<li><p><strong>Client Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
<li><p><strong>Query Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
<li><p><strong>IO Executor ( <code>HazelcastInstance.ManagedExecutorService</code> )</strong></p>
<ul>
<li>Name ( <code>name</code> )</li>
<li>Work queue size ( <code>queueSize</code> )</li>
<li>Thread count of the pool ( <code>poolSize</code> )</li>
<li>Maximum thread count of the pool ( <code>maximumPoolSize</code> )</li>
<li>Remaining capacity of the work queue ( <code>remainingQueueCapacity</code> )</li>
<li>Is shutdown ( <code>isShutdown</code> )</li>
<li>Is terminated ( <code>isTerminated</code> )</li>
<li>Completed task count ( <code>completedTaskCount</code> )</li>
</ul>
</li>
</ul>

<a name="monitoring-with-jmx"></a><h2 id="monitoring-with-jmx">Monitoring with JMX</h2>
<p>You can monitor your Hazelcast members via the JMX protocol.</p>
<p>To achieve this, first add the following system properties to enable <a href="http://download.oracle.com/javase/1.5.0/docs/guide/management/agent.html" target="_blank">JMX agent</a>:</p>
<ul>
<li><code>-Dcom.sun.management.jmxremote</code></li>
<li><code>-Dcom.sun.management.jmxremote.port=\_portNo\_</code> (to specify JMX port, the default is <code>1099</code>) (<em>optional</em>)</li>
<li><code>-Dcom.sun.management.jmxremote.authenticate=false</code> (to disable JMX auth) (<em>optional</em>)</li>
</ul>
<p>Then enable the Hazelcast property <code>hazelcast.jmx</code> (please refer to the <a href="#system-properties">System Properties section</a>) using one of the following ways:</p>
<ul>
<li>By declarative configuration:</li>
</ul>
<pre><code>&lt;properties&gt;
   &lt;property name=&quot;hazelcast.jmx&quot;&gt;true&lt;/property&gt;
&lt;/properties&gt;
</code></pre><ul>
<li>By programmatic configuration:</li>
</ul>
<p><code>config.setProperty(&quot;hazelcast.jmx&quot;, &quot;true&quot;);</code></p>
<ul>
<li>By Spring XML configuration:</li>
</ul>
<pre><code>&lt;hz:properties&gt;
  &lt;hz: property name=&quot;hazelcast.jmx&quot;&gt;true&lt;/hz:property&gt;
&lt;/hz:properties&gt;
</code></pre><ul>
<li>By setting the system property <code>-Dhazelcast.jmx=true</code></li>
</ul>
<a name="mbean-naming-for-hazelcast-data-structures"></a><h3 id="mbean-naming-for-hazelcast-data-structures">MBean Naming for Hazelcast Data Structures</h3>
<p>Hazelcast set the naming convention for MBeans as follows:</p>
<pre><code>final ObjectName mapMBeanName = new ObjectName(&quot;com.hazelcast:instance=_hzInstance_1_dev,type=IMap,name=trial&quot;);
</code></pre><p>The MBeans name consists of the Hazelcast instance name, the type of the data structure, and that data structure&#39;s name. In the above sample, <code>_hzInstance_1_dev</code> is the instance name, we connect to an IMap with the name <code>trial</code>. </p>
<a name="connecting-to-jmx-agent"></a><h3 id="connecting-to-jmx-agent">Connecting to JMX Agent</h3>
<p>One of the ways you can connect to JMX agent is using jconsole, jvisualvm (with MBean plugin) or another JMX compliant monitoring tool.</p>
<p>The other way to connect is to use a custom JMX client. </p>
<p>First, you need to specify the URL where the Hazelcast JMX service is running. Please see the following sample code snippet. The <code>port</code> in this sample should be the one that you define while setting the JMX remote port number (if different than the default port <code>1099</code>).</p>
<pre><code class="lang-java">// Parameters for connecting to the JMX Service
int port = 1099;
String hostname = InetAddress.getLocalHost().getHostName();
JMXServiceURL url = new JMXServiceURL(&quot;service:jmx:rmi://&quot; + hostname + &quot;:&quot; + port + &quot;/jndi/rmi://&quot; + hostname + &quot;:&quot; + port + &quot;/jmxrmi&quot;);
</code></pre>
<p>Then use the URL you acquired to connect to the JMX service and get the <code>JMXConnector</code> object. Using this object, get the <code>MBeanServerConnection</code> object. The <code>MBeanServerConnection</code> object will enable you to use the MBean methods. Please see the example code below.</p>
<pre><code class="lang-java">// Connect to the JMX Service
JMXConnector jmxc = JMXConnectorFactory.connect(url, null);
MBeanServerConnection mbsc = jmxc.getMBeanServerConnection();
</code></pre>
<p>Once you get the <code>MBeanServerConnection</code> object, you can call the getter methods of MBeans as follows:</p>
<pre><code class="lang-java">System.out.println(&quot;\nTotal entries on map &quot; + mbsc.getAttribute(mapMBeanName, &quot;name&quot;) + &quot; : &quot;
                + mbsc.getAttribute(mapMBeanName, &quot;localOwnedEntryCount&quot;));
</code></pre>

<a name="cluster-utilities"></a><h2 id="cluster-utilities">Cluster Utilities</h2>
<p>This section provides information on programmatic utilities you can use to listen to the cluster events, to change the state of your cluster, to check whether the cluster and/or members are safe before shutting down a member, and to define the minimum number of cluster members required for the cluster to remain up and running. It also gives information on the Hazelcast Lite Member.</p>
<a name="getting-member-events-and-member-sets"></a><h3 id="getting-member-events-and-member-sets">Getting Member Events and Member Sets</h3>
<p>Hazelcast allows you to register for membership events so you will be notified when members are added or removed. You can also get the set of cluster members.</p>
<p>The following example code does the above: registers for member events, notified when members are added or removed, and gets the set of cluster members.</p>
<pre><code class="lang-java">import com.hazelcast.core.*;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Cluster cluster = hazelcastInstance.getCluster();
cluster.addMembershipListener( new MembershipListener() {
  public void memberAdded( MembershipEvent membershipEvent ) {
    System.out.println( &quot;MemberAdded &quot; + membershipEvent );
  }

  public void memberRemoved( MembershipEvent membershipEvent ) {
    System.out.println( &quot;MemberRemoved &quot; + membershipEvent );
  }
} );

Member localMember  = cluster.getLocalMember();
System.out.println ( &quot;my inetAddress= &quot; + localMember.getInetAddress() );

Set setMembers  = cluster.getMembers();
for ( Member member : setMembers ) {
  System.out.println( &quot;isLocalMember &quot; + member.localMember() );
  System.out.println( &quot;member.inetaddress &quot; + member.getInetAddress() );
  System.out.println( &quot;member.port &quot; + member.getPort() );
}
</code></pre>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#listening-for-member-events">Membership Listener section</a> for more information on membership events.</em></p>

<a name="managing-cluster-and-member-states"></a><h3 id="managing-cluster-and-member-states">Managing Cluster and Member States</h3>
<p>Starting with the release of 3.6, Hazelcast introduces cluster and member states in addition to the default <code>ACTIVE</code> state. This section explains these states of Hazelcast clusters and members which you can use to allow or restrict the designated cluster/member operations.</p>
<a name="cluster-states"></a><h4 id="cluster-states">Cluster States</h4>
<p>By changing the state of your cluster, you can allow/restrict several cluster operations or change the behavior of those operations. You can use the methods <code>changeClusterState()</code> and <code>shutdown()</code> which are in the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/Cluster.java" target="_blank">Cluster interface</a> to change your cluster&#39;s state.</p>
<p> Hazelcast clusters have the following states:</p>
<ul>
<li><strong><code>ACTIVE</code></strong>: This is the default cluster state. Cluster continues to operate without restrictions.
<br></br></li>
<li><strong><code>FROZEN</code></strong>: <ul>
<li>In this state, the partition table is frozen and partition assignments are not performed. </li>
<li>Your cluster does not accept new members. </li>
<li>If a member leaves, it can join back. Its partition assignments (both primary and replica) remain the same until either it joins back or the cluster state is changed to <code>ACTIVE</code>. When it joins back to the cluster, it will own all previous partition assignments as it was. On the other hand, when the cluster state changes to <code>ACTIVE</code>, re-partitioning starts and unassigned partitions are assigned to the active members.</li>
<li>All other operations in the cluster, except migration, continue without restrictions.</li>
<li>You cannot change the state of a cluster to <code>FROZEN</code> when migration/replication tasks are being performed.
<br></br></li>
</ul>
</li>
<li><strong><code>PASSIVE</code></strong>:<ul>
<li>In this state, the partition table is frozen and partition assignments are not performed. </li>
<li>Your cluster does not accept new members.</li>
<li>If a member leaves while the cluster is in this state, the member will be removed from the partition table if cluster state moves back to <code>ACTIVE</code>. </li>
<li>This state rejects ALL operations immediately EXCEPT the read-only operations like <code>map.get()</code> and <code>cache.get()</code>, replication and cluster heartbeat tasks. </li>
<li>You cannot change the state of a cluster to <code>PASSIVE</code> when migration/replication tasks are being performed.
<br></br></li>
</ul>
</li>
<li><strong><code>IN_TRANSITION</code></strong>: <ul>
<li>This state shows that the state of the cluster is in transition. </li>
<li>You cannot set your cluster&#39;s state as <code>IN_TRANSITION</code> explicitly. </li>
<li>It is a temporary and intermediate state. </li>
<li>During this state, your cluster does not accept new members and migration/replication tasks are paused.</li>
</ul>
</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>All in-cluster methods are fail-fast, i.e. when a method fails in the cluster, it throws an exception immediately (it will not be retried).</em></p>
<p>The following snippet is from the <code>Cluster</code> interface showing the new methods used to manage your cluster&#39;s states.</p>
<pre><code class="lang-java">public interface Cluster {
...
...
    ClusterState getClusterState();
    void changeClusterState(ClusterState newState);
    void changeClusterState(ClusterState newState, TransactionOptions transactionOptions);
    void shutdown();
    void shutdown(TransactionOptions transactionOptions);
</code></pre>
<p>Please refer to the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/Cluster.java" target="_blank">Cluster interface</a> for information on these methods.</p>
<a name="cluster-member-states"></a><h4 id="cluster-member-states">Cluster Member States</h4>
<p>Hazelcast cluster members have the following states:</p>
<ul>
<li><strong><code>ACTIVE</code></strong>: This is the initial member state. The member can execute and process all operations. When the state of the cluster is <code>ACTIVE</code> or <code>FROZEN</code>, the members are in the <code>ACTIVE</code> state. 
<br></br></li>
<li><strong><code>PASSIVE</code></strong>: In this state, member rejects all operations EXCEPT the read-only ones, replication and migration operations, heartbeat operations, and the join operations as explained in the <a href="#cluster-states">Cluster States section</a> above. A member can go into this state when either of the following happens:<ol>
<li>Until the member&#39;s shutdown process is completed after the method <code>Node.shutdown(boolean)</code> is called. Note that, when the shutdown process is completed, member&#39;s state changes to <code>SHUT_DOWN</code>. </li>
<li>Cluster&#39;s state is changed to <code>PASSIVE</code> using the method <code>changeClusterState()</code>. 
<br></br></li>
</ol>
</li>
<li><strong><code>SHUT_DOWN</code></strong>: A member goes into this state when the member&#39;s shutdown process is completed. The member in this state rejects all operations and invocations. A member in this state cannot be restarted.
<br></br></li>
</ul>

<a name="using-the-script-clustersh"></a><h3 id="using-the-script-cluster-sh">Using the Script cluster.sh</h3>
<p>You can use the script <code>cluster.sh</code>, which comes with the Hazelcast package, to get/change the state of your cluster, to shutdown your cluster and to force your cluster to clean its persisted data and make a fresh start. The latter is the Force Start operation of Hazelcast&#39;s Hot Restart Persistence feature. Please refer to the <a href="#force-start">Force Start section</a>.
<br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The script <code>cluster.sh</code> uses <code>curl</code> command and <code>curl</code> must be installed to be able to use the script.</em>
<br></br></p>
<p>The script <code>cluster.sh</code> needs the following parameters to operate according to your needs. If these parameters are not provided, the default values are used.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Parameter</th>
<th style="text-align:left">Default Value</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>-o</code> or <code>--operation</code></td>
<td style="text-align:left"><code>get-state</code></td>
<td style="text-align:left">Executes a cluster-wide operation. Operation can be <code>get-state</code>, <code>change-state</code>, <code>shutdown</code> and <code>force-start</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>-s</code> or <code>--state</code></td>
<td style="text-align:left">None</td>
<td style="text-align:left">Updates the state of the cluster to a new state. New state can be <code>active</code>, <code>frozen</code>, <code>passive</code>. This is used with the operation <code>change-state</code>. This parameter has no default value; when you use this, you should provide a valid state.</td>
</tr>
<tr>
<td style="text-align:left"><code>-a</code> or <code>--address</code></td>
<td style="text-align:left"><code>127.0.0.1</code></td>
<td style="text-align:left">Defines the IP address of a cluster member. If you want to manage your cluster remotely, you should use this parameter to provide the IP address of a member to this script.</td>
</tr>
<tr>
<td style="text-align:left"><code>-p</code> or <code>--port</code></td>
<td style="text-align:left"><code>5701</code></td>
<td style="text-align:left">Defines on which port Hazelcast is running on the local or remote machine. The default value is <code>5701</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>-g</code> or <code>--groupname</code></td>
<td style="text-align:left"><code>dev</code></td>
<td style="text-align:left">Defines the name of a cluster group which is used for a simple authentication. Please see the <a href="#creating-cluster-groups">Creating Cluster Groups section</a>.</td>
</tr>
<tr>
<td style="text-align:left"><code>-P</code> or <code>--password</code></td>
<td style="text-align:left"><code>dev-pass</code></td>
<td style="text-align:left">Defines the password of a cluster group. Please see the <a href="#creating-cluster-groups">Creating Cluster Groups section</a>.</td>
</tr>
</tbody>
</table>
<p>The script <code>cluster.sh</code> is self-documented; you can see the parameter descriptions using the command <code>sh cluster.sh -h</code> or <code>sh cluster.sh --help</code>.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can perform the above operations using the Hot Restart tab of Hazelcast Management Center or using the REST API. Please see the <a href="#hot-restart">Hot Restart section</a> and <a href="#using-rest-api-for-cluster-management">Using REST API for Cluster Management section</a>.</em>
<br></br></p>
<a name="example-usages-for-clustersh"></a><h4 id="example-usages-for-cluster-sh">Example Usages for cluster.sh</h4>
<p>Let&#39;s say you have a cluster running on remote machines and one Hazelcast member is running on the IP  <code>172.16.254.1</code> and on the port
<code>5702</code>. The group name and password of the cluster are <code>test</code> and <code>test</code>.</p>
<p><br></br>
<strong>Getting the cluster state:</strong></p>
<p>To get the state of the cluster, use the following command:</p>
<p><code>sh cluster.sh -o get-state -a 172.16.254.1 -p 5702 -g test -P test</code></p>
<p>The following also gets the cluster state, using the alternative parameter names (e.g. <code>--port</code> instead of <code>-p</code>):</p>
<p><code>sh cluster.sh --operation get-state --address 172.16.254.1 --port 5702 --groupname test --password test</code></p>
<p><br></br>
<strong>Changing the cluster state:</strong></p>
<p>To change the state of the cluster to <code>frozen</code>, use the following command:</p>
<p><code>sh cluster.sh -o change-state -s frozen -a 172.16.254.1 -p 5702 -g test -P test</code></p>
<p>Similarly, you can use the following command for the same purpose:</p>
<p><code>sh cluster.sh --operation change-state --state frozen --address 172.16.254.1 --port 5702 --groupname test --password test</code></p>
<p><br></br>
<strong>Shutting down the cluster:</strong></p>
<p>To shutdown the cluster, use the following command:</p>
<p><code>sh cluster.sh -o shutdown -a 172.16.254.1 -p 5702 -g test -P test</code></p>
<p>Similarly, you can use the following command for the same purpose:</p>
<p><code>sh cluster.sh --operation shutdown --address 172.16.254.1 --port 5702 --groupname test --password test</code></p>
<p><br></br>
<strong>Force starting the cluster:</strong></p>
<p>To force start the cluster, use the following command:</p>
<p><code>sh cluster.sh -o force-start -a 172.16.254.1 -p 5702 -g test -P test</code></p>
<p>Similarly, you can use the following command for the same purpose:</p>
<p><code>sh cluster.sh --operation force-start --address 172.16.254.1 --port 5702 --groupname test --password test</code></p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Currently, this script is not supported on the Windows platforms.</em>
<br></br></p>

<a name="using-rest-api-for-cluster-management"></a><h3 id="using-rest-api-for-cluster-management">Using REST API for Cluster Management</h3>
<p>Besides the Management Center&#39;s Hot Restart tab and the script <code>cluster.sh</code>, you can also use REST API to manage your cluster&#39;s state. The following are the commands you can use.</p>
<p><br></br>
<strong>Getting the cluster state:</strong></p>
<p>To get the state of the cluster, use the following command:</p>
<pre><code>curl --data &quot;${GROUPNAME}&amp;${PASSWORD}&quot; http://127.0.0.1:5701/hazelcast/rest/management/cluster/state
</code></pre><p><br></br>
<strong>Changing the cluster state:</strong></p>
<p>To change the state of the cluster to <code>frozen</code>, use the following command:</p>
<pre><code>curl --data &quot;${GROUPNAME}&amp;${PASSWORD}&amp;${STATE}&quot; http://127.0.0.1:${PORT}/hazelcast/rest/management/cluster/changeState
</code></pre><p><br></br>
<strong>Shutting down the cluster:</strong></p>
<p>To shutdown the cluster, use the following command:</p>
<pre><code>curl --data &quot;${GROUPNAME}&amp;${PASSWORD}&quot;  http://127.0.0.1:${PORT}/hazelcast/rest/management/cluster/shutdown
</code></pre><p><br></br>
<strong>Force starting the cluster:</strong></p>
<p>To force start the cluster, use the following command:</p>
<pre><code>curl --data &quot;${GROUPNAME}&amp;${PASSWORD}&quot; http://127.0.0.1:${PORT}/hazelcast/rest/management/cluster/forceStart/
</code></pre><p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can also perform the above operations using the Hot Restart tab of Hazelcast Management Center or using the script <code>cluster.sh</code>. Please see the <a href="#hot-restart">Hot Restart section</a> and <a href="#using-the-script-cluster-sh">Using the Script cluster.sh section</a>.</em>
<br></br></p>

<a name="enabling-lite-members"></a><h3 id="enabling-lite-members">Enabling Lite Members</h3>
<p>Lite members are the Hazelcast cluster members that do not store data. These members are used mainly to execute tasks and register listeners, and they do not have partitions.</p>
<p>You can form your cluster to include the regular Hazelcast members to store data and Hazelcast lite members to run heavy computations. The presence of the lite members do not affect the operations performed on the other members in the cluster. You can directly submit your tasks to the lite members, register listeners on them and invoke operations for the Hazelcast data structures on them (e.g. <code>map.put()</code> and <code>map.get()</code>).</p>
<a name="configuring-lite-members"></a><h4 id="configuring-lite-members">Configuring Lite Members</h4>
<p>You can enable a cluster member to be a lite member using declarative or programmatic configuration. </p>
<a name="declarative-configuration"></a><h5 id="declarative-configuration">Declarative Configuration</h5>
<pre><code class="lang-xml">&lt;hazelcast&gt;
    &lt;lite-member enabled=&quot;true&quot;&gt;
&lt;/hazelcast&gt;
</code></pre>
<a name="programmatic-configuration"></a><h5 id="programmatic-configuration">Programmatic Configuration</h5>
<pre><code class="lang-java">Config config = new Config();
config.setLiteMember(true);
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Note that you cannot change a member&#39;s role at runtime.</em></p>

<a name="defining-member-attributes"></a><h3 id="defining-member-attributes">Defining Member Attributes</h3>
<p>You can define various member attributes on your Hazelcast members. You can use these member attributes to tag your members as your business logic requirements.</p>
<p>To define member attribute on a member, you can either:</p>
<ul>
<li><p>provide <code>MemberAttributeConfig</code> to your <code>Config</code> object,</p>
</li>
<li><p>or provide member attributes at runtime via attribute setter methods on the <code>Member</code> interface.</p>
</li>
</ul>
<p>For example, you can tag your members with their CPU characteristics and you can route CPU intensive tasks to those CPU-rich members.</p>
<pre><code class="lang-java">MemberAttributeConfig fourCore = new MemberAttributeConfig();
memberAttributeConfig.setIntAttribute( &quot;CPU_CORE_COUNT&quot;, 4 );
MemberAttributeConfig twelveCore = new MemberAttributeConfig();
memberAttributeConfig.setIntAttribute( &quot;CPU_CORE_COUNT&quot;, 12 );
MemberAttributeConfig twentyFourCore = new MemberAttributeConfig();
memberAttributeConfig.setIntAttribute( &quot;CPU_CORE_COUNT&quot;, 24 );

Config member1Config = new Config();
config.setMemberAttributeConfig( fourCore );
Config member2Config = new Config();
config.setMemberAttributeConfig( twelveCore );
Config member3Config = new Config();
config.setMemberAttributeConfig( twentyFourCore );

HazelcastInstance member1 = Hazelcast.newHazelcastInstance( member1Config );
HazelcastInstance member2 = Hazelcast.newHazelcastInstance( member2Config );
HazelcastInstance member3 = Hazelcast.newHazelcastInstance( member3Config );

IExecutorService executorService = member1.getExecutorService( &quot;processor&quot; );

executorService.execute( new CPUIntensiveTask(), new MemberSelector() {
  @Override
  public boolean select(Member member) {
    int coreCount = (int) member.getIntAttribute( &quot;CPU_CORE_COUNT&quot; );
    // Task will be executed at either member2 or member3
    if ( coreCount &gt; 8 ) { 
      return true;
    }
    return false;
  }
} );

HazelcastInstance member4 = Hazelcast.newHazelcastInstance();
// We can also set member attributes at runtime.
member4.setIntAttribute( &quot;CPU_CORE_COUNT&quot;, 2 );
</code></pre>

<a name="safety-checking-cluster-members"></a><h3 id="safety-checking-cluster-members">Safety Checking Cluster Members</h3>
<p>To prevent data loss when shutting down a cluster member, Hazelcast provides a graceful shutdown feature. You perform this shutdown by calling the method <code>HazelcastInstance.shutdown()</code>. </p>
<p>Starting with Hazelcast 3.7, the master member migrates all of the replicas owned by the shutdown-requesting member to the other running (not initiated shutdown) cluster members. After these migrations are completed, the shutting down member will not be the owner or a backup of any partition anymore. It means that you can shutdown any number of Hazelcast members in a cluster concurrently with no data loss.</p>
<p>Please note that the process of shutting down members waits for a predefined amount of time for the master to migrate their partition replicas. You can specify this graceful shutdown timeout duration using the property <code>hazelcast.graceful.shutdown.max.wait</code>. Its default value is 10 minutes. If migrations are not completed within this duration, shutdown may continue non-gracefully and lead to data loss. Therefore, you should choose your own timeout duration considering the size of data in your cluster.</p>
<a name="ensuring-safe-state-with-partitionservice"></a><h4 id="ensuring-safe-state-with-partitionservice">Ensuring Safe State with PartitionService</h4>
<p>With the improvements in graceful shutdown procedure in Hazelcast 3.7, the following methods are not needed to perform graceful shutdown. Nevertheless, you can use them to check the current safety status of the partitions in your cluster.</p>
<pre><code class="lang-java">public interface PartitionService {
   ...
   ...
    boolean isClusterSafe();
    boolean isMemberSafe(Member member);
    boolean isLocalMemberSafe();
    boolean forceLocalMemberToBeSafe(long timeout, TimeUnit unit);
}
</code></pre>
<p>The method <code>isClusterSafe</code> checks whether the cluster is in a safe state. It returns <code>true</code> if there are no active partition migrations and all backups are in sync for each partition.</p>
<p>The method <code>isMemberSafe</code> checks whether a specific member is in a safe state. It checks if all backups of partitions of the given member are in sync with the primary ones. Once it returns <code>true</code>, the given member is safe and it can be shut down without data loss.</p>
<p>Similarly, the method <code>isLocalMemberSafe</code> does the same check for the local member. The method <code>forceLocalMemberToBeSafe</code> forces the owned and backup partitions to be synchronized, making the local member safe.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you want to use the above methods, please note that they are available starting with Hazelcast 3.3.</em></p>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>For more code samples please refer to <a href="https://github.com/hazelcast/hazelcast-code-samples/tree/master/monitoring/cluster/src/main/java" target="_blank">PartitionService Code Samples</a></em>.
<br></br></p>

<a name="defining-a-cluster-quorum"></a><h3 id="defining-a-cluster-quorum">Defining a Cluster Quorum</h3>
<p>Hazelcast Cluster Quorum enables you to define the minimum number of machines required in a cluster for the cluster to remain in an operational state. If the number of machines is below the defined minimum at any time, the operations are rejected and the rejected operations return a <code>QuorumException</code> to their callers.</p>
<p>When a network partitioning happens, by default Hazelcast chooses to be available. With Cluster Quorum, you can tune your Hazelcast instance towards achieving better consistency by rejecting updates that do not pass a minimum threshold. This reduces the chance of concurrent updates to an entry from two partitioned clusters. Note that the consistency defined here is the best effort, it is not full or strong consistency. To prevent mutative operations in case of a split brain syndrome, you can define a minimum quorum that must be present in the cluster.</p>
<p>Hazelcast initiates a quorum when a change happens on the member list.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Currently, cluster quorum only applies to the Map, Transactional Map and Cache; support for other data structures will be added soon. Also, lock methods in the IMap interface do not participate in a quorum.</em></p>
<a name="configuring-a-cluster-quorum"></a><h4 id="configuring-a-cluster-quorum">Configuring a Cluster Quorum</h4>
<p>You can set up Cluster Quorum using either declarative or programmatic configuration.</p>
<p>Assume that you have a 5-member Hazelcast Cluster and you want to set the minimum number of 3 members for the cluster to continue operating. The following examples are configurations for this scenario.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
....
&lt;quorum name=&quot;quorumRuleWithThreeMembers&quot; enabled=&quot;true&quot;&gt;
  &lt;quorum-size&gt;3&lt;/quorum-size&gt;
&lt;/quorum&gt;

&lt;map name=&quot;default&quot;&gt;
&lt;quorum-ref&gt;quorumRuleWithThreeMembers&lt;/quorum-ref&gt;
&lt;/map&gt;
....
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">QuorumConfig quorumConfig = new QuorumConfig();
quorumConfig.setName(&quot;quorumRuleWithThreeMembers&quot;)
quorumConfig.setEnabled(true);
quorumConfig.setSize(3);

MapConfig mapConfig = new MapConfig();
mapConfig.setQuorumName(&quot;quorumRuleWithThreeMembers&quot;);

Config config = new Config();
config.addQuorumConfig(quorumConfig);
config.addMapConfig(mapConfig);
</code></pre>
<p>Quorum configuration has the following elements.</p>
<ul>
<li><code>quorum-size</code>: Minimum number of members required in a cluster for the cluster to remain in an operational state. If the number of members is below the defined minimum at any time, the operations are rejected and the rejected operations return a QuorumException to their callers.</li>
<li><code>quorum-type</code>: Type of the cluster quorum. Available values are READ, WRITE and READ_WRITE.</li>
</ul>
<a name="configuring-quorum-listeners"></a><h4 id="configuring-quorum-listeners">Configuring Quorum Listeners</h4>
<p>You can register quorum listeners to be notified about quorum results. Quorum listeners are local to the member where they are registered, so they receive only events that occurred on that local member.</p>
<p>Quorum listeners can be configured via declarative or programmatic configuration. The following examples are such configurations.</p>
<p><strong>Declarative:</strong></p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
....
&lt;quorum name=&quot;quorumRuleWithThreeMembers&quot; enabled=&quot;true&quot;&gt;
  &lt;quorum-size&gt;3&lt;/quorum-size&gt;
  &lt;quorum-listeners&gt;
    &lt;quorum-listener&gt;com.company.quorum.ThreeMemberQuorumListener&lt;/quorum-listener&gt;
  &lt;/quorum-listeners&gt;
&lt;/quorum&gt;

&lt;map name=&quot;default&quot;&gt;
  &lt;quorum-ref&gt;quorumRuleWithThreeMembers&lt;/quorum-ref&gt;
&lt;/map&gt;
....
&lt;/hazelcast&gt;
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><code class="lang-java">QuorumListenerConfig listenerConfig = new QuorumListenerConfig();
// You can either directly set quorum listener implementation of your own
listenerConfig.setImplementation(new QuorumListener() {
            @Override
            public void onChange(QuorumEvent quorumEvent) {
              if (QuorumResult.PRESENT.equals(quorumEvent.getType())) {
                // handle quorum presence
              } else if (QuorumResult.ABSENT.equals(quorumEvent.getType())) {
                // handle quorum absence
              }
            }
        });
// Or you can give the name of the class that implements QuorumListener interface.
listenerConfig.setClassName(&quot;com.company.quorum.ThreeMemberQuorumListener&quot;);

QuorumConfig quorumConfig = new QuorumConfig();
quorumConfig.setName(&quot;quorumRuleWithThreeMembers&quot;)
quorumConfig.setEnabled(true);
quorumConfig.setSize(3);
quorumConfig.addListenerConfig(listenerConfig);


MapConfig mapConfig = new MapConfig();
mapConfig.setQuorumName(&quot;quorumRuleWithThreeMembers&quot;);

Config config = new Config();
config.addQuorumConfig(quorumConfig);
config.addMapConfig(mapConfig);
</code></pre>
<a name="querying-quorum-results"></a><h4 id="querying-quorum-results">Querying Quorum Results</h4>
<p>Quorum service gives you the ability to query quorum results over the <code>Quorum</code> instances. Quorum instances let you query the quorum result of a particular quorum.</p>
<p>Here is a Quorum interface that you can interact with.</p>
<pre><code class="lang-java">/**
 * {@link Quorum} provides access to the current status of a quorum.
 */
public interface Quorum {
    /**
     * Returns true if quorum is present, false if absent.
     *
     * @return boolean presence of the quorum
     */
    boolean isPresent();
}
</code></pre>
<p>You can retrieve the quorum instance for a particular quorum over the quorum service, as in the following example.</p>
<pre><code class="lang-java">String quorumName = &quot;at-least-one-storage-member&quot;;
QuorumConfig quorumConfig = new QuorumConfig();
quorumConfig.setName(quorumName)
quorumConfig.setEnabled(true);

MapConfig mapConfig = new MapConfig();
mapConfig.setQuorumName(quorumName);

Config config = new Config();
config.addQuorumConfig(quorumConfig);
config.addMapConfig(mapConfig);

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(config);
QuorumService quorumService = hazelcastInstance.getQuorumService();
Quorum quorum = quorumService.getQuorum(quorumName);

boolean quorumPresence = quorum.isPresent();
</code></pre>

<a name="hazelcast-cli"></a><h2 id="hazelcast-cli">Hazelcast CLI</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>Hazelcast CLI is a command line management tool where you can manage your cluster and lifecycle of Hazelcast members. Hazelcast CLI is provided as a Hazelcast plugin. Please see its own GitHub repo at <a href="https://github.com/hazelcast/hazelcast-cli" target="_blank">Hazelcast CLI</a> for information on installing and using it.</p>

<a name="management-center"></a><h2 id="management-center">Management Center</h2>
<p>Hazelcast Management Center enables you to monitor and manage your cluster members running Hazelcast. In addition to monitoring the overall state of your clusters, you can also analyze and browse your data structures in detail, update map configurations and take thread dumps from members. Uou can run scripts (JavaScript, Groovy, etc.) and commands on your members with its scripting and console modules.</p>
<a name="installing-management-center"></a><h3 id="installing-management-center">Installing Management Center</h3>
<p>You have two options for installing Hazelcast Management Center:</p>
<ol>
<li>Deploy the file <code>mancenter</code>-<em>version</em><code>.war</code> on your Java application server/container.</li>
<li>Start Hazelcast Management Center from the command line and then have the Hazelcast cluster members communicate with it. This means that your members should know the URL of the <code>mancenter</code> application before they start.</li>
</ol>
<p>Here are the steps.</p>
<ul>
<li>Download the latest Hazelcast ZIP from <a href="http://www.hazelcast.org/download/" target="_blank">hazelcast.org</a>. The ZIP contains the <code>mancenter</code>-<em>version</em><code>.war</code> file under the directory <code>mancenter</code>.</li>
<li>You can directly start <code>mancenter</code>-<em>version</em><code>.war</code> file from the command line. The following command will start Hazelcast Management Center on port 8080 with context root &#39;mancenter&#39; (<code>http://localhost:8080/mancenter</code>).</li>
</ul>
<pre><code class="lang-java">java -jar mancenter-*version*.war 8080 mancenter
</code></pre>
<ul>
<li>You can also start it using the scripts <code>startManCenter.bat</code> or <code>startManCenter.sh</code> located in the directory <code>mancenter</code>.</li>
<li>Or, instead of starting at the command line, you can deploy it to your web server (Tomcat, Jetty, etc.). Let us say it is running at <code>http://localhost:8080/mancenter</code>.</li>
<li>After you perform the above steps, make sure that <code>http://localhost:8080/mancenter</code> is up.</li>
<li>Configure your Hazelcast members by adding the URL of your web application to your <code>hazelcast.xml</code>. Hazelcast members will send their states to this URL.</li>
</ul>
<pre><code class="lang-xml">&lt;management-center enabled=&quot;true&quot;&gt;
    http://localhost:8080/mancenter
&lt;/management-center&gt;
</code></pre>
<ul>
<li>If you have deployed <code>mancenter-*version*.war</code> in your already-SSL-enabled web container, configure <code>hazelcast.xml</code> as follows.</li>
</ul>
<pre><code class="lang-xml">&lt;management-center enabled=&quot;true&quot;&gt;
    https://localhost:sslPortNumber/mancenter
&lt;/management-center&gt;
</code></pre>
<p>If you are using an untrusted certificate for your container, which you created yourself, you need to add that certificate to your JVM first. Download the certificate from the browser, after this you can add it to JVM as follows.</p>
<p><code>keytool -import -noprompt -trustcacerts -alias &lt;AliasName&gt; -file &lt;certificateFile&gt; -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass &lt;Password&gt;</code></p>
<ul>
<li>You can also set a frequency (in seconds) for which Management Center will take information from the Hazelcast cluster, using the element <code>update-interval</code> as shown below. <code>update-interval</code> is optional and its default value is 3 seconds.</li>
</ul>
<pre><code class="lang-xml">&lt;management-center enabled=&quot;true&quot; update-interval=&quot;3&quot;&gt;http://localhost:8080/
mancenter&lt;/management-center&gt;
</code></pre>
<ul>
<li>Start your Hazelcast cluster.</li>
<li>Browse to <code>http://localhost:8080/mancenter</code> and setup your <a href="#getting-started-to-management-center">administrator account</a> explained in the next section.</li>
</ul>
<a name="getting-started-to-management-center"></a><h3 id="getting-started-to-management-center">Getting Started to Management Center</h3>
<p>If you have the open source edition of Hazelcast, Management Center can be used for at most 2 members in the cluster. To use it for more members, you need to have either a Management Center license, Hazelcast Enterprise license or Hazelcast Enterprise HD license. This license should be entered within the Management Center as described in the following paragraphs.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Even if you have a Hazelcast Enterprise or Enterprise HD license key and you set it as explained in the <a href="#setting-the-license-key">Setting the License Key</a> section, you still need to enter this same license within the Management Center. Please see the following paragraphs to learn how you can enter your license.</em>
<br></br></p>
<p>Once you browse to <code>http://localhost:8080/mancenter</code> and since you are going to use Management Center for the first time, the following dialog box appears.</p>
<p><img src="images/Signup.png" alt="Signing Up"></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>If you already created an administrator account before, a login dialog box appears instead.</em></p>
<p>It asks you to create a username and password and give a valid e-mail address of yours. Once you press the <strong>Sign Up</strong> button, your administrator account credentials are created and the following dialog box appears.</p>
<p><img src="images/ConnectCluster.png" alt="Selecting Cluster to Connect"></p>
<p>&quot;Select Cluster to Connect&quot; dialog box lists the clusters that send statistics to Management Center. You can either select a cluster to connect using the <strong>Connect</strong> button or enter your Management Center license key using the <strong>Enter License</strong> button. Management Center can be used without a license if the cluster that you want to monitor has at most 2 members.</p>
<p>If you have a Management Center license or Hazelcast Enterprise license, you can enter it in the dialog box that appears once you press the <strong>Enter License</strong> button, as shown below.</p>
<p><img src="images/EnterLicense.png" alt="Providing License for Management Center"></p>
<p>When you try to connect to a cluster that has more than 2 members without entering a license key or if your license key is expired, the following dialog box appears.</p>
<p><img src="images/ExpiredLicense.png" alt="Management Center License Warning"></p>
<p>Here, you can either choose to connect to a cluster without providing a license key or to enter your license key. If you choose to continue without a license, Management Center still continues to function but you will only be able to monitor up to 2 members of your cluster.</p>
<p>Management Center creates a folder with the name <code>mancenter</code> under your <code>user/home</code> folder to save data files and above settings/license information. You can change the data folder by setting the <code>hazelcast.mancenter.home</code> system property. Please see the <a href="#system-properties">System Properties section</a> to see the description of this property and to learn how to set a system property.</p>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#management-center-configuration">Management Center Configuration section</a> for a full description of Hazelcast Management Center configuration.</em></p>
<a name="management-center-tools"></a><h3 id="management-center-tools">Management Center Tools</h3>
<p>Once the page is loaded after selecting a cluster, the tool&#39;s home page appears as shown below.</p>
<p><img src="images/ManagementCenterHomePage.png" alt="Management Center Home Page"></p>
<p>This page provides the fundamental properties of the selected cluster which are explained in the <a href="#management-center-home-page">Home Page</a> section. The page has a toolbar on the top and a menu on the left.</p>
<a name="toolbar"></a><h4 id="toolbar">Toolbar</h4>
<p><img src="images/Toolbar.png" alt="Management Center Toolbar"></p>
<p>The toolbar has the following buttons:</p>
<ul>
<li><strong>Home</strong>: Loads the home page shown above. Please see the <a href="#management-center-home-page">Management Center Home Page section</a>.</li>
<li><strong>Scripting</strong>: Loads the page used to write and execute the user`s own scripts on the cluster. Please see the <a href="#scripting">Scripting section</a>.</li>
<li><strong>Console</strong>: Loads the page used to execute commands on the cluster. Please see the <a href="#executing-console-commands">Console section</a>.</li>
<li><strong>Alerts</strong>: Creates alerts by specifying filters. Please see the <a href="#creating-alerts">Setting Alerts section</a>.</li>
<li><strong>Documentation</strong>: Opens the Management Center documentation in a window inside the tool. Please see the <a href="#management-center-documentation">Documentation section</a>.</li>
<li><strong>Administration</strong>: Used by the admin users to manage users in the system. Please see the <a href="#administering-management-center">Administering Management Center section</a>.</li>
<li><strong>Logout</strong>: Closes the current user&#39;s session.</li>
<li><strong>Hot Restart</strong>: Used by the admin users to manage cluster state. Please see the <a href="#hot-restart">Hot Restart section</a>.</li>
<li><strong>Time Travel</strong>: Sees the cluster&#39;s situation at a time in the past. Please see the <a href="#checking-past-status-with-time-travel">Time Travel section</a>.</li>
<li><p><strong>Cluster Selector</strong>: Switches between clusters. When the mouse is moved onto this item, a drop down list of clusters appears.</p>
<p><img src="images/ChangingCluster.jpg" alt="Changing Cluster"></p>
<p>The user can select any cluster and once selected, the page immediately loads with the selected cluster&#39;s information.</p>
</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Some of the above listed toolbar items are not visible to users who are not admin or who have <strong>read-only</strong> permission. Also, some of the operations explained in the later sections cannot be performed by users with read-only permission. Please see the <a href="#administering-management-center">Administering Management Center section</a> for details.</em></p>
<a name="menu"></a><h4 id="menu">Menu</h4>
<p>The Home Page includes a menu on the left which lists the distributed data structures in the cluster and all the cluster members, as shown below.</p>
<p><img src="images/Menu.png" alt="Management Center Menu"></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Distributed data structures will be shown there when the proxies are created for them.</em></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>WAN Replication tab is only visible with <font color="#3981DB"><strong>Hazelcast Enterprise</strong></font> license.</em></p>
<p>You can expand and collapse menu items by clicking on them. Below is the list of menu items with links to their explanations.</p>
<ul>
<li><a href="#monitoring-caches">Caches</a></li>
<li><a href="#managing-maps">Maps</a></li>
<li><a href="#monitoring-replicated-maps">Replicated Maps</a></li>
<li><a href="#monitoring-queues">Queues</a></li>
<li><a href="#monitoring-topics">Topics</a></li>
<li><a href="#monitoring-multimaps">MultiMaps</a></li>
<li><a href="#monitoring-executors">Executors</a></li>
<li><a href="#monitoring-wan-replication">WAN</a></li>
<li><a href="#monitoring-members">Members</a></li>
</ul>
<a name="tabbed-view"></a><h4 id="tabbed-view">Tabbed View</h4>
<p>Each time you select an item from the toolbar or menu, the item is added to the main view as a tab, as shown below.</p>
<p><img src="images/TabbedView.jpg" alt="Tabbed View"></p>
<p>In the above example, <em>Home</em>, <em>Scripting</em>, <em>Console</em>, <em>queue1</em> and <em>map1</em> windows can be seen as tabs. Windows can be closed using the <img src="images/CloseIcon.jpg" alt=""> icon on each tab (except the Home Page; it cannot be closed).</p>
<a name="management-center-home-page"></a><h3 id="management-center-home-page">Management Center Home Page</h3>
<p>This is the first page appearing after logging in. It gives an overview of the connected cluster. The following subsections describe each portion of the page.</p>
<a name="cpu-utilization"></a><h4 id="cpu-utilization">CPU Utilization</h4>
<p>This part of the page provides load and utilization information for the CPUs for each cluster member, as shown below.</p>
<p><img src="images/CPUUtilization.jpg" alt="CPU Utilization"></p>
<p>The first column lists the members with their IPs and ports. The next columns list the system load averages on each member for the last 1, 5 and 15 minutes. These average values are calculated as the sum of the count of runnable entities running on and queued to the available CPUs averaged over the last 1, 5 and 15 minutes. This calculation is operating system specific, typically a damped time-dependent average. If system load average is not available, these columns show negative values.</p>
<p>The last column (<strong>Chart</strong>) graphically shows the recent load on the CPUs. When you move the mouse cursor on a chart, you can see the CPU load at the time where the cursor is placed. Charts under this column shows the CPU loads approximately for the last 2 minutes. If recent CPU load is not available, you will see a negative value.</p>
<a name="memory-utilization"></a><h4 id="memory-utilization">Memory Utilization</h4>
<p>This part of the page provides information related to memory usages for each member, as shown below.</p>
<p><img src="images/MemoryUtilization.jpg" alt="Memory Utilization"></p>
<p>The first column lists the members with their IPs and ports. The next columns show the used and free memories out of the total memory reserved for Hazelcast usage, in real-time. The <strong>Max</strong> column lists the maximum memory capacity of each member and the <strong>Percent</strong> column lists the percentage value of used memory out of the maximum memory. The last column (<strong>Chart</strong>) shows the memory usage of members graphically. When you move the mouse cursor on a desired graph, you can see the memory usage at the time where the cursor is placed. Graphs under this column shows the memory usages approximately for the last 2 minutes.</p>
<a name="memory-distribution"></a><h4 id="memory-distribution">Memory Distribution</h4>
<p>This part of the page graphically provides the cluster wise breakdown of memory, as shown below. The blue area is the memory used by maps. The dark yellow area is the memory used by both non-Hazelcast entities and all Hazelcast entities except the map (i.e. the memory used by all entities subtracted by the memory used by map). The green area is the free memory out of the whole cluster`s memory capacity.</p>
<p><img src="images//MemoryDistribution.jpg" alt="Memory Distribution of Cluster"></p>
<p>In the above example, you can see 0.32% of the total memory is used by Hazelcast maps (it can be seen by placing the mouse cursor on it), 58.75% is used by non-Hazelcast entities and 40.85% of the total memory is free.</p>
<a name="map-memory-distribution"></a><h4 id="map-memory-distribution">Map Memory Distribution</h4>
<p>This part is the breakdown of the blue area shown in the <strong>Memory Distribution</strong> graph explained above. It provides the percentage values of the memories used by each map, out of the total cluster memory reserved for all Hazelcast maps.</p>
<p><img src="images/MapMemoryDistribution.jpg" alt="Memory Distribution of Map"></p>
<p>In the above example, you can see 49.55% of the total map memory is used by <strong>map1</strong> and 49.55% is used by <strong>map2</strong>.</p>
<a name="partition-distribution"></a><h4 id="partition-distribution">Partition Distribution</h4>
<p>This pie chart shows what percentage of partitions each cluster member has, as shown below.</p>
<p><img src="images/PartitionDistribution.jpg" alt="Partition Distribution per Member"></p>
<p>You can see each member&#39;s partition percentages by placing the mouse cursor on the chart. In the above example, you can see the member &quot;127.0.0.1:5708&quot; has 5.64% of the total partition count (which is 271 by default and configurable, please see the <code>hazelcast.partition.count</code> property explained in the <a href="#system-properties">System Properties section</a>).</p>
<a name="monitoring-caches"></a><h3 id="monitoring-caches">Monitoring Caches</h3>
<p>You can monitor your caches&#39; metrics by clicking the cache name listed on the left panel under <strong>Caches</strong> menu item. A new tab for monitoring that cache instance is opened on the right, as shown below.</p>
<p><img src="images/MonitoringCaches.png" alt="Monitoring Caches"></p>
<p>On top of the page, four charts monitor the <strong>Gets</strong>, <strong>Puts</strong>, <strong>Removals</strong> and <strong>Evictions</strong> in real-time. The X-axis of all the charts show the current system time. To open a chart as a separate dialog, click on the <img src="images/MaximizeChart.jpg" alt=""> button placed at the top right of each chart.</p>
<p>Under these charts is the Cache Statistics Data Table. From left to right, this table lists the IP addresses and ports of each member, and the entry, get, put, removal, eviction, and hit and miss counts per second in real-time.</p>
<p>You can navigate through the pages using the buttons at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). You can ascend or descend the order of the listings in each column by clicking on column headings.</p>
<p>Under the Cache Statistics Data Table, there is Cache Throughput Data Table.</p>
<p>From left to right, this table lists:</p>
<ul>
<li>the IP address and port of each member,</li>
<li>the put/s, get/s and remove/s operation rates on each member.</li>
</ul>
<p>You can select the period in the combo box placed at the top right corner of the window, for which the table data will be shown. Available values are <strong>Since Beginning</strong>, <strong>Last Minute</strong>, <strong>Last 10 Minutes</strong> and <strong>Last 1 Hour</strong>.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You need to enable the statistics for caches to monitor them in the Management Center. Use the <code>&lt;statistics-enabled&gt;</code> element or <code>setStatisticsEnabled()</code> method in declarative or programmatic configuration, respectively, to enable the statistics. Please refer to the <a href="#jcache-declarative-configuration">JCache Declarative Configuration section</a> for more information.</em></p>
<a name="managing-maps"></a><h3 id="managing-maps">Managing Maps</h3>
<p>Map instances are listed under the <strong>Maps</strong> menu item on the left. When you click on a map, a new tab for monitoring that map instance opens on the right, as shown below. In this tab, you can monitor metrics and also re-configure the selected map.</p>
<p><img src="images/MonitoringMaps.jpg" alt="Monitoring Maps"></p>
<p>The below subsections explain the portions of this window.</p>
<a name="map-browser"></a><h4 id="map-browser">Map Browser</h4>
<p>Use the Map Browser tool to retrieve properties of the entries stored in the selected map. To open the Map Browser tool, click on the <strong>Map Browser</strong> button, located at the top right of the window. Once opened, the tool appears as a dialog, as shown below.</p>
<p><img src="images/MapBrowser.jpg" alt="Map Browser"></p>
<p>Once the key and the key&#39;s type are specified and the <strong>Browse</strong> button is clicked, the key&#39;s properties along with its value are listed.</p>
<a name="map-config"></a><h4 id="map-config">Map Config</h4>
<p>Use the Map Config tool to set the selected map&#39;s attributes, such as the backup count, TTL, and eviction policy. To open the Map Config tool, click on the <strong>Map Config</strong> button, located at the top right of the window. Once opened, the tool appears as a dialog, as shown below.</p>
<p><img src="images/MapConfig.jpg" alt="Map Config Tool"></p>
<p>You can change any attribute and click the <strong>Update</strong> button to save your changes.</p>
<a name="map-monitoring"></a><h4 id="map-monitoring">Map Monitoring</h4>
<p>Besides the Map Browser and Map Config tools, the map monitoring page has monitoring options that are explained below. All of these options perform real-time monitoring.</p>
<p>On top of the page, small charts monitor the size, throughput, memory usage, backup size, etc. of the selected map in real-time. The X-axis of all the charts show the current system time. You can select other small monitoring charts using the <img src="images/ChangeWindowIcon.jpg" alt=""> button at the top right of each chart. When you click the button, the monitoring options are listed, as shown below.</p>
<p><img src="images/MonitoringOptionsMap.jpg" alt="Monitoring Options for Map"></p>
<p>When you click on a desired monitoring, the chart is loaded with the selected option. To open a chart as a separate dialog, click on the <img src="images/MaximizeChart.jpg" alt=""> button placed at the top right of each chart. The monitoring charts below are available:</p>
<ul>
<li><strong>Size</strong>: Monitors the size of the map. Y-axis is the entry count (should be multiplied by 1000).</li>
<li><strong>Throughput</strong>: Monitors get, put and remove operations performed on the map. Y-axis is the operation count.</li>
<li><strong>Memory</strong>: Monitors the memory usage on the map. Y-axis is the memory count.</li>
<li><strong>Backups</strong>: Chart loaded when &quot;Backup Size&quot; is selected. Monitors the size of the backups in the map. Y-axis is the backup entry count (should be multiplied by 1000).</li>
<li><strong>Backup Memory</strong>: Chart loaded when &quot;Backup Mem.&quot; is selected. Monitors the memory usage of the backups. Y-axis is the memory count.</li>
<li><strong>Hits</strong>: Monitors the hit count of the map.</li>
<li><strong>Puts/s, Gets/s, Removes/s</strong>: These three charts monitor the put, get and remove operations (per second) performed on the selected map.</li>
</ul>
<p>Under these charts are <strong>Map Memory</strong> and <strong>Map Throughput</strong> data tables. The Map Memory data table provides memory metrics distributed over members, as shown below.</p>
<p><img src="images/MemoryDataTable.jpg" alt="Map Memory Data Table"></p>
<p>From left to right, this table lists the IP address and port, entry counts, memory used by entries, backup entry counts, memory used by backup entries, events, hits, locks and dirty entries (in the cases where <em>MapStore</em> is enabled, these are the entries that are put to/removed from the map but not written to/removed from a database yet) of each entry in the map. You can navigate through the pages using the buttons at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). You can ascend or descend the order of the listings by clicking on the column headings.</p>
<p>Map Throughput data table provides information about the operations (get, put, remove) performed on each member in the map, as shown below.</p>
<p><img src="images/MapThroughputDataTable.jpg" alt="Map Throughput Data Table"></p>
<p>From left to right, this table lists:</p>
<ul>
<li>the IP address and port of each member,</li>
<li>the put, get and remove operations on each member,</li>
<li>the average put, get, remove latencies,</li>
<li>and the maximum put, get, remove latencies on each member.</li>
</ul>
<p>You can select the period in the combo box placed at the top right corner of the window, for which the table data will be shown. Available values are <strong>Since Beginning</strong>, <strong>Last Minute</strong>, <strong>Last 10 Minutes</strong> and <strong>Last 1 Hour</strong>.</p>
<p>You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). To ascend or descent the order of the listings, click on the column headings.</p>
<a name="monitoring-replicated-maps"></a><h3 id="monitoring-replicated-maps">Monitoring Replicated Maps</h3>
<p>Replicated Map instances are shown under the <strong>Replicated Maps</strong> menu item on the left. When you click on a Replicated Map, a new tab for monitoring that instance opens on the right, as shown below.</p>
<p><img src="images/MonitoringReplicatedMaps.png" alt="Monitoring Replicated Maps"></p>
<p>In this tab, you can monitor metrics and also re-configure the selected Replicated Map. All of the statistics are real-time monitoring statistics.</p>
<p>When you click on a desired monitoring, the chart is loaded with the selected option. Also you can open the chart in new window.</p>
<ul>
<li><strong>Size</strong>: Monitors the size of the Replicated Map. Y-axis is the entry count (should be multiplied by 1000).</li>
<li><strong>Throughput</strong>: Monitors get, put and remove operations performed on the Replicated Map. Y-axis is the operation count.</li>
<li><strong>Memory</strong>: Monitors the memory usage on the Replicated Map. Y-axis is the memory count.</li>
<li><strong>Hits</strong>: Monitors the hit count of the Replicated Map.</li>
<li><strong>Puts/s, Gets/s, Removes/s</strong>: These three charts monitor the put, get and remove operations (per second) performed on the selected Replicated Map, the average put, get, remove latencies, and the maximum put, get, remove latencies on each member.</li>
</ul>
<p>The Replicated Map Throughput Data Table provides information about operations (get, put, remove) performed on each member in the selected Replicated Map.</p>
<p><img src="images/ReplicatedMapThroughput.png" alt="Replicated Map Throughput Data Table"></p>
<p>From left to right, this table lists:</p>
<ul>
<li>the IP address and port of each member,</li>
<li>the put, get, and remove operations on each member,</li>
<li>the average put, get, and remove latencies,</li>
<li>and the maximum put, get, and remove latencies on each member.</li>
</ul>
<p>You can select the period from the combo box placed at the top right corner of the window, in which the table data is shown. Available values are <strong>Since Beginning</strong>, <strong>Last Minute</strong>, <strong>Last 10 Minutes</strong> and <strong>Last 1 Hour</strong>.</p>
<p>You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). To ascend or descent the order of the listings, click on the column headings.</p>
<a name="monitoring-queues"></a><h3 id="monitoring-queues">Monitoring Queues</h3>
<p>Using the menu item <strong>Queues</strong>, you can monitor your queues data structure. When you expand this menu item and click on a queue, a new tab for monitoring that queue instance is opened on the right, as shown below.</p>
<p><img src="images/MonitoringQueues.jpg" alt="Monitoring Queues"></p>
<p>On top of the page, small charts monitor the size, offers and polls of the selected queue in real-time. The X-axis of all the charts shows the current system time. To open a chart as a separate dialog, click on the <img src="images/MaximizeChart.jpg" alt=""> button placed at the top right of each chart. The monitoring charts below are available:</p>
<ul>
<li><strong>Size</strong>: Monitors the size of the queue. Y-axis is the entry count (should be multiplied by 1000).</li>
<li><strong>Offers</strong>: Monitors the offers sent to the selected queue. Y-axis is the offer count.</li>
<li><strong>Polls</strong>: Monitors the polls sent to the selected queue. Y-axis is the poll count.</li>
</ul>
<p>Under these charts are <strong>Queue Statistics</strong> and <strong>Queue Operation Statistics</strong> tables. The Queue Statistics table provides item and backup item counts in the queue and age statistics of items and backup items at each member, as shown below.</p>
<p><img src="images/QueueStatistics.jpg" alt="Queue Statistics"></p>
<p>From left to right, this table lists the IP address and port, items and backup items on the queue of each member, and maximum, minimum and average age of items in the queue. You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). The order of the listings in each column can be ascended or descended by clicking on column headings.</p>
<p>Queue Operations Statistics table provides information about the operations (offers, polls, events) performed on the queues, as shown below.</p>
<p><img src="images/QueueOperationStatistics.jpg" alt="Queue Operation Statistics"></p>
<p>From left to right, this table lists the IP address and port of each member, and counts of offers, rejected offers, polls, poll misses and events.</p>
<p>You can select the period in the combo box placed at the top right corner of the window to show the table data. Available values are <strong>Since Beginning</strong>, <strong>Last Minute</strong>, <strong>Last 10 Minutes</strong> and <strong>Last 1 Hour</strong>.</p>
<p>You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). Click on the column headings to ascend or descend the order of the listings.</p>
<a name="monitoring-topics"></a><h3 id="monitoring-topics">Monitoring Topics</h3>
<p>To monitor your topics&#39; metrics, click the topic name listed on the left panel under the <strong>Topics</strong> menu item. A new tab for monitoring that topic instance opens on the right, as shown below.</p>
<p><img src="images/MonitoringTopics.jpg" alt="Monitoring Topics"></p>
<p>On top of the page, two charts monitor the <strong>Publishes</strong> and <strong>Receives</strong> in real-time. They show the published and received message counts of the cluster, the members of which are subscribed to the selected topic. The X-axis of both charts show the current system time. To open a chart as a separate dialog, click on the <img src="images/MaximizeChart.jpg" alt=""> button placed at the top right of each chart.</p>
<p>Under these charts is the Topic Operation Statistics table. From left to right, this table lists the IP addresses and ports of each member, and counts of message published and receives per second in real-time. You can select the period in the combo box placed at top right corner of the table to show the table data. The available values are <strong>Since Beginning</strong>, <strong>Last Minute</strong>, <strong>Last 10 Minutes</strong> and <strong>Last 1 Hour</strong>.</p>
<p>You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). Click on the column heading to ascend or descend the order of the listings.</p>
<a name="monitoring-multimaps"></a><h3 id="monitoring-multimaps">Monitoring MultiMaps</h3>
<p>MultiMap is a specialized map where you can associate a key with multiple values. This monitoring option is similar to the <strong>Maps</strong> option: the same monitoring charts and data tables monitor MultiMaps. The differences are that you cannot browse the MultiMaps and re-configure it. Please see <a href="#managing-maps">Managing Maps</a>.</p>
<a name="monitoring-executors"></a><h3 id="monitoring-executors">Monitoring Executors</h3>
<p>Executor instances are listed under the <strong>Executors</strong> menu item on the left. When you click on a executor, a new tab for monitoring that executor instance opens on the right, as shown below.</p>
<p><img src="images/MonitoringExecutors.jpg" alt="Monitoring Executors"></p>
<p>On top of the page, small charts monitor the pending, started, completed, etc. executors in real-time. The X-axis of all the charts shows the current system time. You can select other small monitoring charts using the <img src="images/ChangeWindowIcon.jpg" alt=""> button placed at the top right of each chart. Click the button to list the monitoring options, as shown below.</p>
<p><img src="images/MonitoringOptionsExecutor.jpg" alt="Monitoring Options for Executor"></p>
<p>When you click on a desired monitoring, the chart loads with the selected option. To open a chart as a separate dialog, click on the <img src="images/MaximizeChart.jpg" alt=""> button placed at top right of each chart. The below monitoring charts are available:</p>
<ul>
<li><strong>Pending</strong>: Monitors the pending executors. Y-axis is the executor count.</li>
<li><strong>Started</strong>: Monitors the started executors. Y-axis is the executor count.</li>
<li><strong>Start Lat. (msec.)</strong>: Shows the latency when executors are started. Y-axis is the duration in milliseconds.</li>
<li><strong>Completed</strong>: Monitors the completed executors. Y-axis is the executor count.</li>
<li><strong>Comp. Time (msec.)</strong>: Shows the completion period of executors. Y-axis is the duration in milliseconds.</li>
</ul>
<p>Under these charts is the <strong>Executor Operation Statistics</strong> table, as shown below.</p>
<p><img src="images/ExecutorOperationStats.jpg" alt="Executor Operation Statistics"></p>
<p>From left to right, this table lists the IP address and port of members, the counts of pending, started and completed executors per second, and the execution time and average start latency of executors on each member. You can navigate through the pages using the buttons placed at the bottom right of the table (<strong>First, Previous, Next, Last</strong>). Click on the column heading to ascend or descend the order of the listings.</p>
<a name="monitoring-wan-replication"></a><h3 id="monitoring-wan-replication">Monitoring WAN Replication</h3>
<p>WAN Replication schemes are listed under the <strong>WAN</strong> menu item on the left. When you click on a scheme, a new tab for monitoring the targets which that scheme has appears on the right, as shown below.</p>
<p><img src="images/WanPublisherStats.png" alt="Monitoring WAN Replication"></p>
<p>In this tab, you see <strong>WAN Replication Operations Table</strong> for each target which belongs to this scheme. One of the example tables is shown below.</p>
<p><img src="images/WanTargetTable.png" alt="WAN Replication Operations Table"></p>
<ul>
<li><strong>Connected</strong>: Status of the member connection to the target.</li>
<li><strong>Outbound Recs (sec)</strong>: Average number of records sent to target per second from this member.</li>
<li><strong>Outbound Lat (ms)</strong>: Average latency of sending a record to the target from this member.</li>
<li><strong>Outbound Queue</strong>: Number of records waiting in the queue to be sent to the target.</li>
<li><strong>Action</strong>: Stops/Resumes replication of this member&#39;s records.</li>
</ul>
<a name="monitoring-members"></a><h3 id="monitoring-members">Monitoring Members</h3>
<p>Use this menu item to monitor each cluster member and perform operations like running garbage collection (GC) and taking a thread dump. Once you select a member from the menu, a new tab for monitoring that member opens on the right, as shown below.</p>
<p><img src="images/MonitoringMembers.png" alt="Monitoring Members"></p>
<p>The <strong>CPU Utilization</strong> chart shows the percentage of CPU usage on the selected member. The <strong>Memory Utilization</strong> chart shows the memory usage on the selected member with three different metrics (maximum, used and total memory). You can open both of these charts as separate windows using the <img src="images/ChangeWindowIcon.jpg" alt=""> button placed at top right of each chart; this gives you a clearer view of the chart.</p>
<p>The window titled <strong>Partitions</strong> shows which partitions are assigned to the selected member. <strong>Runtime</strong> is a dynamically updated window tab showing the processor number, the start and up times, and the maximum, total and free memory sizes of the selected member. These values are collected from the default MXBeans provided by the Java Virtual Machine (JVM). Descriptions from the Javadocs and some explanations are below:</p>
<ul>
<li><p><strong>Number of Processors</strong>: Number of processors available to the member (JVM).</p>
</li>
<li><p><strong>Start Time</strong>: Start time of the member (JVM) in milliseconds.</p>
</li>
<li><p><strong>Up Time</strong>: Uptime of the member (JVM) in milliseconds</p>
</li>
<li><p><strong>Maximum Memory</strong>: Maximum amount of memory that the member (JVM) will attempt to use.</p>
</li>
<li><p><strong>Free Memory</strong>: Amount of free memory in the member (JVM).</p>
</li>
<li><p><strong>Used Heap Memory</strong>: Amount of used memory in bytes.</p>
</li>
<li><p><strong>Max Heap Memory</strong>: Maximum amount of memory in bytes that can be used for memory management.</p>
</li>
<li><p><strong>Used Non-Heap Memory</strong>: Amount of used memory in bytes.</p>
</li>
<li><p><strong>Max Non-Heap Memory</strong>: Maximum amount of memory in bytes that can be used for memory management.</p>
</li>
<li><p><strong>Total Loaded Classes</strong>: Total number of classes that have been loaded since the member (JVM) has started execution.</p>
</li>
<li><p><strong>Current Loaded Classes</strong>: Number of classes that are currently loaded in the member (JVM).</p>
</li>
<li><p><strong>Total Unloaded Classes</strong>: Total number of classes unloaded since the member (JVM) has started execution.</p>
</li>
<li><p><strong>Total Thread Count</strong>: Total number of threads created and also started since the member (JVM) started.</p>
</li>
<li><p><strong>Active Thread Count</strong>: Current number of live threads including both daemon and non-daemon threads.</p>
</li>
<li><p><strong>Peak Thread Count</strong>: Peak live thread count since the member (JVM) started or peak was reset.</p>
</li>
<li><p><strong>Daemon Thread Count</strong>: Current number of live daemon threads.</p>
</li>
<li><p><strong>OS: Free Physical Memory</strong>: Amount of free physical memory in bytes.</p>
</li>
<li><p><strong>OS: Committed Virtual Memory</strong>: Amount of virtual memory that is guaranteed to be available to the running process in bytes.</p>
</li>
<li><p><strong>OS: Total Physical Memory</strong>: Total amount of physical memory in bytes.</p>
</li>
<li><p><strong>OS: Free Swap Space</strong>: Amount of free swap space in bytes. Swap space is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space.</p>
</li>
<li><p><strong>OS: Total Swap Space</strong>: Total amount of swap space in bytes.</p>
</li>
<li><p><strong>OS: Maximum File Descriptor Count</strong>: Maximum number of file descriptors. File descriptor is an integer number that uniquely represents an opened file in the operating system.</p>
</li>
<li><p><strong>OS: Open File Descriptor Count</strong>: Number of open file descriptors.</p>
</li>
<li><p><strong>OS: Process CPU Time</strong>: CPU time used by the process on which the member (JVM) is running in nanoseconds.</p>
</li>
<li><p><strong>OS: Process CPU Load</strong>: Recent CPU usage for the member (JVM) process. This is a double with a value from 0.0 to 1.0. A value of 0.0 means that none of the CPUs were running threads from the member (JVM) process during the recent period of time observed, while a value of 1.0 means that all CPUs were actively running threads from the member (JVM) 100% of the time during the recent period being observed. Threads from the member (JVM) include the application threads as well as the member (JVM) internal threads.</p>
</li>
<li><p><strong>OS: System Load Average</strong>: System load average for the last minute. The system load average is the average over a period of time of this sum: (the number of runnable entities queued to the available processors) + (the number of runnable entities running on the available processors). The way in which the load average is calculated is operating system specific but it is typically a damped time-dependent average.</p>
</li>
<li><p><strong>OS: System CPU Load</strong>:
Recent CPU usage for the whole system. This is a double with a value from 0.0 to 1.0. A value of 0.0 means that all CPUs were idle during the recent period of time observed, while a value of 1.0 means that all CPUs were actively running 100% of the time during the recent period being observed.</p>
</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>These descriptions may vary according to the JVM version or vendor.</em></p>
<p>Next to the <strong>Runtime</strong> tab, the <strong>Properties</strong> tab shows the system properties. The <strong>Member Configuration</strong> window shows the XML configuration of the connected Hazelcast cluster.</p>
<p>The <strong>List of Slow Operations</strong> gives an overview of detected slow operations which occurred on that member. The data is collected by the <a href="#slowoperationdetector">SlowOperationDetector</a>.</p>
<p><img src="images/ListOfSlowOperations.png" alt="List of Slow Operations"></p>
<p>Click on an entry to open a dialog which shows the stacktrace and detailed information about each slow invocation of this operation.</p>
<p><img src="images/SlowOperationDetail.png" alt="Slow Operations Details"></p>
<p>Besides the aforementioned monitoring charts and windows, you can also perform operations on the selected member through this page. The operation buttons are located at the top right of the page, as explained below:</p>
<ul>
<li><strong>Run GC</strong>: Press this button to execute garbage collection on the selected member. A notification stating that the GC execution was successful will be shown.</li>
<li><strong>Thread Dump</strong>: Press this button to take a thread dump of the selected member and show it as a separate dialog to the user.</li>
<li><strong>Shutdown Node</strong>: Press this button to shutdown the selected member.</li>
</ul>
<a name="scripting"></a><h3 id="scripting">Scripting</h3>
<p>You can use the scripting feature of this tool to execute codes on the cluster. To open this feature as a tab, select <strong>Scripting</strong> located at the toolbar on top. Once selected, the scripting feature opens as shown below.</p>
<p><img src="images/Scripting.jpg" alt="Scripting"></p>
<p>In this window, the <strong>Scripting</strong> part is the actual coding editor. You can select the members on which the code will execute from the <strong>Members</strong> list shown at the right side of the window. Below the members list, a combo box enables you to select a scripting language: currently, JavaScript, Ruby, Groovy and Python languages are supported. After you write your script and press the <strong>Execute</strong> button, you can see the execution result in the <strong>Result</strong> part of the window.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>To use the scripting languages other than JavaScript on a member, the libraries for those languages should be placed in the classpath of that member.</em></p>
<p>There are <strong>Save</strong> and <strong>Delete</strong> buttons on the top right of the scripting editor. To save your scripts, press the <strong>Save</strong> button after you type a name for your script into the field next to this button. The scripts you saved are listed in the <strong>Saved Scripts</strong> part of the window, located at the bottom right of the page. Click on a saved script from this list to execute or edit it. If you want to remove a script that you wrote and saved before, select it from this list and press the <strong>Delete</strong> button.</p>
<p>In the scripting engine you have a <code>HazelcastInstance</code> bonded to a variable named <code>hazelcast</code>. You can invoke any method that <code>HazelcastInstance</code> has via the <code>hazelcast</code> variable. You can see example usage for JavaScript below.</p>
<pre><code class="lang-javascript">var name = hazelcast.getName();
var node = hazelcast.getCluster().getLocalMember();
var employees = hazelcast.getMap(&quot;employees&quot;);
employees.put(&quot;1&quot;,&quot;John Doe&quot;);
employees.get(&quot;1&quot;); // will return &quot;John Doe&quot;
</code></pre>
<a name="executing-console-commands"></a><h3 id="executing-console-commands">Executing Console Commands</h3>
<p>The Management Center has a console feature that enables you to execute commands on the cluster. For example, you can perform <code>put</code>s and <code>get</code>s on a map, after you set the namespace with the command <code>ns &lt;name of your map&gt;</code>. The same is valid for queues, topics, etc. To execute your command, type it into the field below the console and press <strong>Enter</strong>. Type <code>help</code> to see all the commands that you can use.</p>
<p>Open a console window by clicking on the <strong>Console</strong> button located on the toolbar. Below is a sample view with some executed commands.</p>
<p><img src="images/Console.jpg" alt="Console"></p>
<a name="creating-alerts"></a><h3 id="creating-alerts">Creating Alerts</h3>
<p>You can use the alerts feature of this tool to receive alerts and/or e-mail notifications by creating filters. In these filters, you can specify criteria for cluster members or data structures. When the specified criteria are met for a filter, the related alert is shown as a pop-up message on the top right of the page or sent as an e-mail.</p>
<p>Once you click the <strong>Alerts</strong> button located on the toolbar, the page shown below appears.</p>
<p><img src="images/Alerts.png" alt="Creating Alerts"></p>
<p>If you want to enable the Management Center to send e-mail notifications to the Management Center Admin users, you need to configure the SMTP server. To do this, click on the <strong>Create STMP Config</strong> shown above. The form shown below appears.</p>
<p><img src="images/CreateSMTPConfig.png" alt="Create SMTP Configuration"></p>
<p>In this form, specify the e-mail address from which the notifications will be sent and also its password. Then, provide the SMTP server host address and port. Finally, check the <strong>TLS Connection</strong> checkbox if the connection is secured by TLS (Transport Layer Security).</p>
<p>After you provide the required information, click on the <strong>Save Config</strong> button. After a processing period (for a couple of seconds), the form will be closed if the configuration is created successfully. In this case, an e-mail will be sent to the e-mail address you provided in the form stating that the SMTP configuration is successful and e-mail alert system is created.</p>
<p>If not, you will see an error message at the bottom of this form as shown below.   </p>
<p><img src="images/SMTPConfigFormWithError.png" alt="SMTP Configuration Error"></p>
<p>As you can see, the reasons can be wrong SMTP configuration or connectivity problems. In this case, please check the form fields and check for any causes for the connections issues with your server.</p>
<p><strong>Creating Filters for Cluster Members</strong></p>
<p>Select <strong>Member Alerts</strong> check box to create filters for some or all members in the cluster. Once selected, the next screen asks for which members the alert will be created. Select the desired members and click on the <strong>Next</strong> button. On the next page (shown below), specify the criteria.</p>
<p><img src="images/MemberAlert.jpg" alt="Filter for Member"></p>
<p>You can create alerts when:</p>
<ul>
<li>free memory on the selected members is less than the specified number.</li>
<li>used heap memory is larger than the specified number.</li>
<li>the number of active threads are less than the specified count.</li>
<li>the number of daemon threads are larger than the specified count.</li>
</ul>
<p>When two or more criteria is specified they will be bound with the logical operator <strong>AND</strong>.</p>
<p>On the next page, give a name for the filter. Then, select whether notification e-mails will be sent to the Management Center Admins using the <strong>Send Email Alert</strong> checkbox. Then, provide a time interval (in seconds) for which the e-mails with the <strong>same notification content</strong> will be sent using the <strong>Email Interval (secs)</strong> field.  Finally, select whether the alert data will be written to the disk (if checked, you can see the alert log at the folder <em>/users/<your user>/mancenter<version></em>).</p>
<p>Click on the <strong>Save</strong> button; your filter will be saved and put into the <strong>Filters</strong> part of the page. To edit the filter, click on the <img src="images/EditIcon.jpg" alt=""> icon. To delete it, click on the <img src="images/DeleteIcon.jpg" alt=""> icon.</p>
<p><strong>Creating Filters for Data Types</strong></p>
<p>Select the <strong>Data Type Alerts</strong> check box to create filters for data structures. The next screen asks for which data structure (maps, queues, multimaps, executors) the alert will be created. Once a structure is selected, the next screen immediately loads and you then select the data structure instances (i.e. if you selected <em>Maps</em>, it will list all the maps defined in the cluster, you can select one map or more). Select as desired, click on the <strong>Next</strong> button, and select the members on which the selected data structure instances will run.</p>
<p>The next screen, as shown below, is the one where you specify the criteria for the selected data structure.</p>
<p><img src="images/DataAlert.jpg" alt="Filter for Data Types"></p>
<p>As the screen shown above shows, you will select an item from the left combo box, select the operator in the middle one, specify a value in the input field, and click on the <strong>Add</strong> button. You can create more than one criteria in this page; those will be bound by the logical operator <strong>AND</strong>.</p>
<p>After you specify the criteria, click the <strong>Next</strong> button. On the next page, give a name for the filter. Then, select whether notification e-mails will be sent to the Management Center Admins using the <strong>Send Email Alert</strong> checkbox. Then, provide a time interval (in seconds) for which the e-mails with the <strong>same notification content</strong> will be sent using the <strong>Email Interval (secs)</strong> field.  Finally, select whether the alert data will be written to the disk (if checked, you can see the alert log at the folder <em>/users/<your user>/mancenter<version></em>).</p>
<p>Click on the <strong>Save</strong> button; your filter will be saved and put into the <strong>Filters</strong> part of the page. To edit the filter, click on the <img src="images/EditIcon.jpg" alt=""> icon. To delete it, click on the <img src="images/DeleteIcon.jpg" alt=""> icon.</p>
<a name="administering-management-center"></a><h3 id="administering-management-center">Administering Management Center</h3>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>This toolbar item is available only to admin users.</em></p>
<p>The <strong>Admin</strong> user can add, edit, and remove users and specify the permissions for the users of Management Center. To perform these operations, click on the <strong>Administration</strong> button located on the toolbar. The page below appears.</p>
<p><img src="images/Administration.png" alt="Administration"></p>
<a name="users"></a><h5 id="users">Users</h5>
<p>To add a user to the system, specify the username, e-mail and password in the <strong>Add/Edit User</strong> part of the page. If the user to be added will have administrator privileges, select <strong>isAdmin</strong> checkbox. <strong>Permissions</strong> checkboxes have two values:</p>
<ul>
<li><strong>Read Only</strong>: If this permission is given to the user, only <em>Home</em>, <em>Documentation</em> and <em>Time Travel</em> items will be visible at the toolbar at that user&#39;s session. Also, users with this permission cannot update a <a href="#map-config">map configuration</a>, run a garbage collection and take a thread dump on a cluster member, or shutdown a member (please see <a href="#monitoring-members">Monitoring Members</a>).</li>
<li><strong>Read/Write</strong>: If this permission is given to the user, <em>Home</em>, <em>Scripting</em>, <em>Console</em>, <em>Documentation</em> and <em>Time Travel</em> items will be visible. The users with this permission can update a map configuration and perform operations on the members.</li>
</ul>
<p>After you enter/select all fields, click <strong>Save</strong> button to create the user. You will see the newly created user&#39;s username on the left side, in the <strong>Users</strong> part of the page.</p>
<p>To edit or delete a user, select a username listed in the <strong>Users</strong>. Selected user information appears on the right side of the page. To update the user information, change the fields as desired and click the <strong>Save</strong> button. To delete the user from the system, click the <strong>Delete</strong> button.</p>
<a name="license"></a><h5 id="license">License</h5>
<p>To update the management center license, you can click on the <strong>Update License</strong> button and enter the new license code. You will see the expiration date of your current license on the screen.</p>
<p><img src="images/License.png" alt="License"></p>
<a name="hot-restart"></a><h3 id="hot-restart">Hot Restart</h3>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>This toolbar item is available only to admin users.</em></p>
<p>The admin user can see and change the cluster state, shut down the cluster, and force start the cluster using the operations listed in this screen as shown below.</p>
<p><img src="images/HotRestart.png" alt="Hot Restart Operations"></p>
<p><strong>Cluster States</strong></p>
<ul>
<li><p><strong>Active</strong>: Cluster will continue to operate without any restriction. All operations are allowed. This is the default state of a cluster.</p>
</li>
<li><p><strong>Frozen</strong>: New members are not allowed to join, except the members left in <strong>this</strong> state or <strong>Passive</strong> state. All other operations except migrations are allowed and will operate without any restriction.</p>
</li>
<li><p><strong>Passive</strong>: New members are not allowed to join, except the members left in <strong>this</strong> state or <strong>Frozen</strong> state. All operations, except the ones marked with <code>AllowedDuringPassiveState</code>, will be rejected immediately.</p>
</li>
<li><p><strong>In Transition</strong>: Shows that the cluster state is in transition. This is a temporary and intermediate state. It is not allowed to set it explicitly.</p>
</li>
</ul>
<p><strong>Changing Cluster State</strong></p>
<p><img src="images/ChangeClusterState.png" alt="Changing Cluster state"></p>
<ul>
<li>Click the dropdown menu and choose the state to which you want your cluster to change. A pop-up will appear and stay on the screen until the state is successfully changed.</li>
</ul>
<p><img src="images/ChangeClusterState-wait.png" alt="Waiting the State Change"></p>
<p><strong>Shutting Down the Cluster</strong></p>
<ul>
<li>Click the <strong>Shutdown</strong> button. A pop-up will appear and stay on screen until the cluster is successfully shutdown.</li>
</ul>
<p><img src="images/ShutdownCluster.png" alt="Shutdown Cluster"></p>
<p>If an exception occurs during the state change or shutdown operation on the cluster, this exception message will be shown on the screen as a notification.</p>
<p><strong>Force Start the Cluster</strong></p>
<p>Restart process cannot be completed if a member crashes permanently and cannot recover from the failure since it cannot start or it fails to load its own data. In that case, you can force the cluster to clean its persisted data and make a fresh start. This process is called <strong>force start</strong>.</p>
<p><img src="images/ForceStart.png" alt="Force Start"></p>
<p>Click the <strong>Force Start</strong> button. A pop-up will appear and stay on screen until the operation is triggered.</p>
<p>If an exception occurs, this exception message will be showed on the screen as a notification.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The operations explained in this section (Hot Restart) can also be performed using REST API and the script <code>cluster.sh</code>. Please refer to the <a href="#using-rest-api-for-cluster-management">Using REST API for Cluster Management section</a> and <a href="#using-the-script-cluster-sh">Using the Script cluster.sh section</a>.</em></p>
<a name="checking-past-status-with-time-travel"></a><h3 id="checking-past-status-with-time-travel">Checking Past Status with Time Travel</h3>
<p>Use the Time Travel toolbar item to check the status of the cluster at a time in the past. When you select it on the toolbar, a small window appears on top of the page, as shown below.</p>
<p><img src="images/TimeTravel.jpg" alt=""></p>
<p>To see the cluster status in a past time, you should first enable the Time Travel. Click on the area where it says <strong>OFF</strong> (on the right of Time Travel window). It will turn to <strong>ON</strong> after it asks whether to enable the Time Travel with a dialog: click on <strong>Enable</strong> in the dialog to enable the Time Travel.</p>
<p>Once it is <strong>ON</strong>, the status of your cluster will be stored on your disk as long as your web server is alive.</p>
<p>You can go back in time using the slider and/or calendar and check your cluster&#39;s situation at the selected time. All data structures and members can be monitored as if you are using the management center normally (charts and data tables for each data structure and members). Using the arrow buttons placed at both sides of the slider, you can go back or further with steps of 5 seconds. It will show status if Time Travel has been <strong>ON</strong> at the selected time in past; otherwise, all the charts and tables will be shown as empty.</p>
<p>The historical data collected with Time Travel feature are stored in a file database on the disk. These files can be found in the folder <code>&lt;Users Home Directory&gt;/mancenter&lt;Hazelcast version&gt;</code>, e.g. <code>/home/mancenter3.5</code>. This folder can be changed using the <code>hazelcast.mancenter.home</code> property on the server where Management Center is running.</p>
<p>Time travel data files are created monthly. Their file name format is <code>[group-name]-[year][month].db</code> and
 <code>[group-name]-[year][month].lg</code>. Time travel data is kept in the <code>*.db</code> files. The files with the extension <code>lg</code> are temporary files created internally and you do not have to worry about them.</p>
<p>Management Center has no automatic way of removing or archiving old time travel data files. They remain in the aforementioned folder until you delete or archive them.</p>
<a name="management-center-documentation"></a><h3 id="management-center-documentation">Management Center Documentation</h3>
<p>To see the documentation, click on the <strong>Documentation</strong> button located at the toolbar. Management Center manual will appear as a tab.</p>
<a name="suggested-heap-size"></a><h3 id="suggested-heap-size">Suggested Heap Size</h3>
<p><strong>For 2 Cluster Members</strong></p>
<table>
<thead>
<tr>
<th>Mancenter Heap Size</th>
<th># of Maps</th>
<th># of Queues</th>
<th># of Topics</th>
</tr>
</thead>
<tbody>
<tr>
<td>256m</td>
<td>3k</td>
<td>1k</td>
<td>1k</td>
</tr>
<tr>
<td>1024m</td>
<td>10k</td>
<td>1k</td>
<td>1k</td>
</tr>
</tbody>
</table>
<p><strong>For 10 Members</strong></p>
<table>
<thead>
<tr>
<th>Mancenter Heap Size</th>
<th># of Maps</th>
<th># of Queues</th>
<th># of Topics</th>
</tr>
</thead>
<tbody>
<tr>
<td>256m</td>
<td>50</td>
<td>30</td>
<td>30</td>
</tr>
<tr>
<td>1024m</td>
<td>2k</td>
<td>1k</td>
<td>1k</td>
</tr>
</tbody>
</table>
<p><strong>For 20 Members</strong></p>
<table>
<thead>
<tr>
<th>Mancenter Heap Size</th>
<th># of Maps</th>
<th># of Queues</th>
<th># of Topics</th>
</tr>
</thead>
<tbody>
<tr>
<td>256m*</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>1024m</td>
<td>1k</td>
<td>1k</td>
<td>1k</td>
</tr>
</tbody>
</table>
<p>* With 256m heap, management center is unable to collect statistics.</p>

<a name="clustered-jmx-via-management-center"></a><h2 id="clustered-jmx-via-management-center">Clustered JMX via Management Center</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Clustered JMX via Management Center allows you to monitor clustered statistics of distributed objects from a JMX interface.</p>
<a name="configuring-clustered-jmx"></a><h3 id="configuring-clustered-jmx">Configuring Clustered JMX</h3>
<p>In order to configure Clustered JMX, use the following command line parameters for your Management Center deployment.</p>
<ul>
<li><code>-Dhazelcast.mc.jmx.enabled=true</code> (default is false)</li>
<li><code>-Dhazelcast.mc.jmx.port=9000</code> (optional, default is 9999)</li>
<li><code>-Dcom.sun.management.jmxremote.ssl=false</code></li>
</ul>
<p>With embedded Jetty, you do not need to deploy your Management Center application to any container or application server.</p>
<p>You can start Management Center application with Clustered JMX enabled as shown below.</p>
<pre><code>java -Dhazelcast.mc.jmx.enabled=true -Dhazelcast.mc.jmx.port=9999 -Dcom.sun.management.jmxremote.ssl=false -jar mancenter-3.3.jar
</code></pre><p>Once Management Center starts, you should see a log similar to below.</p>
<pre><code>INFO: Management Center 3.3
Jun 05, 2014 11:55:32 AM com.hazelcast.webmonitor.service.jmx.impl.JMXService
INFO: Starting Management Center JMX Service on port :9999
</code></pre><p>You should be able to connect to Clustered JMX interface from the address <code>localhost:9999</code>.</p>
<p>You can use <code>jconsole</code> or any other JMX client to monitor your Hazelcast Cluster. As a sample, below is the <code>jconsole</code> screenshot of the Clustered JMX hierarchy.</p>
<p><img src="images/ClusteredJMX.png" alt=""></p>
<a name="clustered-jmx-api"></a><h3 id="clustered-jmx-api">Clustered JMX API</h3>
<p>The management beans are exposed with the following object name format.</p>
<p><code>ManagementCenter[</code><em>cluster name</em><code>]:type=&lt;</code><em>object type</em><code>&gt;,name=&lt;</code><em>object name</em><code>&gt;,member=&quot;&lt;</code><em>cluster member IP address</em><code>&gt;&quot;</code></p>
<p>Object name starts with <code>ManagementCenter</code> prefix. Then it has the cluster name in brackets followed by a colon. After that, <code>type</code>,<code>name</code> and <code>member</code> attributes follows, each separated with a comma.</p>
<ul>
<li><p><code>type</code> is the type of object. Values are <code>Clients</code>, <code>Executors</code>, <code>Maps</code>, <code>Members</code>, <code>MultiMaps</code>, <code>Queues</code>, <code>Services</code>, and <code>Topics</code>.</p>
</li>
<li><p><code>name</code> is the name of object.</p>
</li>
<li><p><code>member</code> is the member address of object (only required if the statistics are local to the member).</p>
</li>
</ul>
<p>A sample bean is shown below.</p>
<pre><code>ManagementCenter[dev]:type=Services,name=OperationService,member=&quot;192.168.2.79:5701&quot;
</code></pre><p>Here is the list of attributes that are exposed from the Clustered JMX interface.</p>
<ul>
<li><strong>ManagementCenter[ClusterName]</strong></li>
<li>Clients<ul>
<li>Address</li>
<li>ClientType</li>
<li>Uuid</li>
</ul>
</li>
<li>Executors<ul>
<li>Cluster</li>
<li>Name</li>
<li>StartedTaskCount</li>
<li>CompletedTaskCount</li>
<li>CancelledTaskCount</li>
<li>PendingTaskCount</li>
</ul>
</li>
<li>Maps<ul>
<li>Cluster</li>
<li>Name</li>
<li>BackupEntryCount</li>
<li>BackupEntryMemoryCost</li>
<li>CreationTime</li>
<li>DirtyEntryCount</li>
<li>Events</li>
<li>GetOperationCount</li>
<li>HeapCost</li>
<li>Hits</li>
<li>LastAccessTime</li>
<li>LastUpdateTime</li>
<li>LockedEntryCount</li>
<li>MaxGetLatency</li>
<li>MaxPutLatency</li>
<li>MaxRemoveLatency</li>
<li>OtherOperationCount</li>
<li>OwnedEntryCount</li>
<li>PutOperationCount</li>
<li>RemoveOperationCount</li>
</ul>
</li>
<li>ReplicatedMaps<ul>
<li>Cluster</li>
<li>Name</li>
<li>BackupEntryCount</li>
<li>BackupEntryMemoryCost</li>
<li>CreationTime</li>
<li>DirtyEntryCount</li>
<li>Events</li>
<li>GetOperationCount</li>
<li>HeapCost</li>
<li>Hits</li>
<li>LastAccessTime</li>
<li>LastUpdateTime</li>
<li>LockedEntryCount</li>
<li>MaxGetLatency</li>
<li>MaxPutLatency</li>
<li>MaxRemoveLatency</li>
<li>OtherOperationCount</li>
<li>OwnedEntryCount</li>
<li>PutOperationCount</li>
<li>RemoveOperationCount</li>
</ul>
</li>
<li>Members<ul>
<li>ConnectedClientCount</li>
<li>HeapFreeMemory</li>
<li>HeapMaxMemory</li>
<li>HeapTotalMemory</li>
<li>HeapUsedMemory</li>
<li>IsMaster</li>
<li>OwnedPartitionCount</li>
</ul>
</li>
<li>MultiMaps<ul>
<li>Cluster</li>
<li>Name</li>
<li>BackupEntryCount</li>
<li>BackupEntryMemoryCost</li>
<li>CreationTime</li>
<li>DirtyEntryCount</li>
<li>Events</li>
<li>GetOperationCount</li>
<li>HeapCost</li>
<li>Hits</li>
<li>LastAccessTime</li>
<li>LastUpdateTime</li>
<li>LockedEntryCount</li>
<li>MaxGetLatency</li>
<li>MaxPutLatency</li>
<li>MaxRemoveLatency</li>
<li>OtherOperationCount</li>
<li>OwnedEntryCount</li>
<li>PutOperationCount</li>
<li>RemoveOperationCount</li>
</ul>
</li>
<li>Queues<ul>
<li>Cluster</li>
<li>Name</li>
<li>MinAge</li>
<li>MaxAge</li>
<li>AvgAge</li>
<li>OwnedItemCount</li>
<li>BackupItemCount</li>
<li>OfferOperationCount</li>
<li>OtherOperationsCount</li>
<li>PollOperationCount</li>
<li>RejectedOfferOperationCount</li>
<li>EmptyPollOperationCount</li>
<li>EventOperationCount</li>
<li>CreationTime</li>
</ul>
</li>
<li>Services<ul>
<li>ConnectionManager<ul>
<li>ActiveConnectionCount</li>
<li>ClientConnectionCount</li>
<li>ConnectionCount</li>
</ul>
</li>
<li>EventService<ul>
<li>EventQueueCapacity</li>
<li>EventQueueSize</li>
<li>EventThreadCount</li>
</ul>
</li>
<li>OperationService<ul>
<li>ExecutedOperationCount</li>
<li>OperationExecutorQueueSize</li>
<li>OperationThreadCount</li>
<li>RemoteOperationCount</li>
<li>ResponseQueueSize</li>
<li>RunningOperationsCount</li>
</ul>
</li>
<li>PartitionService<ul>
<li>ActivePartitionCount</li>
<li>PartitionCount</li>
</ul>
</li>
<li>ProxyService<ul>
<li>ProxyCount</li>
</ul>
</li>
<li>ManagedExecutor[hz::async]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::client]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::global-operation]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::io]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::query]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::scheduled]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated</li>
</ul>
</li>
<li>ManagedExecutor[hz::system]<ul>
<li>Name</li>
<li>CompletedTaskCount</li>
<li>MaximumPoolSize</li>
<li>PoolSize</li>
<li>QueueSize</li>
<li>RemainingQueueCapacity</li>
<li>Terminated  </li>
</ul>
</li>
</ul>
</li>
<li>Topics<ul>
<li>Cluster</li>
<li>Name</li>
<li>CreationTime</li>
<li>PublishOperationCount</li>
<li>ReceiveOperationCount</li>
</ul>
</li>
</ul>
<a name="integrating-with-new-relic"></a><h3 id="integrating-with-new-relic">Integrating with New Relic</h3>
<p>Use the Clustered JMX interface to integrate Hazelcast Management Center with <em>New Relic</em>. To perform this integration, attach New Relic Java agent and provide an extension file that describes which metrics will be sent to New Relic.</p>
<p>Please see <a href="http://docs.newrelic.com/docs/java/custom-jmx-instrumentation-by-yml" target="_blank">Custom JMX instrumentation by YAML</a> on the New Relic webpage.</p>
<p>Below is an example Map monitoring <code>.yml</code> file for New Relic.</p>
<pre><code class="lang-plain">name: Clustered JMX
version: 1.0
enabled: true

jmx:
  - object_name: ManagementCenter[clustername]:type=Maps,name=mapname
    metrics:
      - attributes: PutOperationCount, GetOperationCount, RemoveOperationCount, Hits,\
            BackupEntryCount, OwnedEntryCount, LastAccessTime, LastUpdateTime
        type: simple
  - object_name: ManagementCenter[clustername]:type=Members,name=&quot;member address in\
        double quotes&quot;
    metrics:
      - attributes: OwnedPartitionCount
        type: simple
</code></pre>
<p>Put the <code>.yml</code> file in the <code>extensions</code> folder in your New Relic installation. If an <code>extensions</code> folder does not exist there, create one.</p>
<p>After you set your extension, attach the New Relic Java agent and start Management Center as shown below.</p>
<pre><code class="lang-plain">java -javaagent:/path/to/newrelic.jar -Dhazelcast.mc.jmx.enabled=true\
    -Dhazelcast.mc.jmx.port=9999 -jar mancenter-3.3.jar
</code></pre>
<p>If your logging level is set as FINER, you should see the log listing in the file <code>newrelic_agent.log</code>, which is located in the <code>logs</code> folder in your New Relic installation. Below is an example log listing.</p>
<pre><code class="lang-plain">Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINE:
    JMX Service : querying MBeans (1)
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    JMX Service : MBeans query ManagementCenter[dev]:type=Members,
    name=&quot;192.168.2.79:5701&quot;, matches 1
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric OwnedPartitionCount : 68
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    JMX Service : MBeans query ManagementCenter[dev]:type=Maps,name=orders,
    matches 1
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric Hits : 46,593
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric BackupEntryCount : 1,100
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric OwnedEntryCount : 1,100
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric RemoveOperationCount : 0
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric PutOperationCount : 118,962
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric GetOperationCount : 0
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric LastUpdateTime : 1,401,962,426,811
Jun 5, 2014 14:18:43 +0300 [72696 62] com.newrelic.agent.jmx.JmxService FINER:
    Recording JMX metric LastAccessTime : 1,401,962,426,811
</code></pre>
<p>Then you can navigate to your New Relic account and create Custom Dashboards. Please see <a href="http://docs.newrelic.com/docs/dashboards-menu/creating-custom-dashboards" target="_blank">Creating custom dashboards</a>.</p>
<p>While you are creating the dashboard, you should see the metrics that you are sending to New Relic from Management Center in the <strong>Metrics</strong> section under the JMX folder.</p>
<a name="integrating-with-appdynamics"></a><h3 id="integrating-with-appdynamics">Integrating with AppDynamics</h3>
<p>Use the Clustered JMX interface to integrate Hazelcast Management Center with <em>AppDynamics</em>. To perform this integration, attach AppDynamics Java agent to the Management Center.</p>
<p>For agent installation, refer to <a href="http://docs.appdynamics.com/display/PRO14S/Install+the+App+Agent+for+Java" target="_blank">Install the App Agent for Java</a> page.</p>
<p>For monitoring on AppDynamics, refer to <a href="http://docs.appdynamics.com/display/PRO14S/Monitor+JMX+MBeans#MonitorJMXMBeans-UsingAppDynamicsforJMXMonitoring" target="_blank">Using AppDynamics for JMX Monitoring</a> page.</p>
<p>After installing AppDynamics agent, you can start Management Center as shown below.</p>
<pre><code class="lang-plain">java -javaagent:/path/to/javaagent.jar -Dhazelcast.mc.jmx.enabled=true\
    -Dhazelcast.mc.jmx.port=9999 -jar mancenter-3.3.jar
</code></pre>
<p>When Management Center starts, you should see the logs below.</p>
<pre><code class="lang-plain">Started AppDynamics Java Agent Successfully.
Hazelcast Management Center starting on port 8080 at path : /mancenter
</code></pre>
<p><br></br></p>

<a name="clustered-rest-via-management-center"></a><h2 id="clustered-rest-via-management-center">Clustered REST via Management Center</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>The Clustered REST API is exposed from Management Center to allow you to monitor clustered statistics of distributed objects.</p>
<a name="enabling-clustered-rest"></a><h3 id="enabling-clustered-rest">Enabling Clustered REST</h3>
<p>To enable Clustered REST on your Management Center, pass the following system property at startup. This property is disabled by default.</p>
<pre><code class="lang-plain">-Dhazelcast.mc.rest.enabled=true
</code></pre>
<a name="clustered-rest-api-root"></a><h3 id="clustered-rest-api-root">Clustered REST API Root</h3>
<p>The entry point for Clustered REST API is <code>/rest/</code>.</p>
<p>This resource does not have any attributes.</p>
<a name="clusters-resource"></a><h3 id="clusters-resource">Clusters Resource</h3>
<p>This resource returns a list of clusters that are connected to the Management Center.</p>
<a name="retrieve-clusters"></a><h5 id="retrieve-clusters">Retrieve Clusters</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;dev&quot;,&quot;qa&quot;]
</code></pre>
</li>
</ul>
<a name="cluster-resource"></a><h3 id="cluster-resource">Cluster Resource</h3>
<p>This resource returns information related to the provided cluster name.</p>
<a name="retrieve-cluster-information"></a><h5 id="retrieve-cluster-information">Retrieve Cluster Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {&quot;masterAddress&quot;:&quot;192.168.2.78:5701&quot;}
</code></pre>
</li>
</ul>
<a name="members-resource"></a><h3 id="members-resource">Members Resource</h3>
<p>This resource returns a list of members belonging to the provided clusters.</p>
<a name="retrieve-members-get-restclustersclusternamemembers"></a><h5 id="retrieve-members-get-rest-clusters-clustername-members-">Retrieve Members [GET] [/rest/clusters/{clustername}/members]</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;192.168.2.78:5701&quot;,&quot;192.168.2.78:5702&quot;,&quot;192.168.2.78:5703&quot;,&quot;192.168.2.78:5704&quot;]
</code></pre>
</li>
</ul>
<a name="member-resource"></a><h3 id="member-resource">Member Resource</h3>
<p>This resource returns information related to the provided member.</p>
<a name="retrieve-member-information"></a><h5 id="retrieve-member-information">Retrieve Member Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;192.168.2.78:5701&quot;,
    &quot;maxMemory&quot;:129957888,
    &quot;ownedPartitionCount&quot;:68,
    &quot;usedMemory&quot;:60688784,
    &quot;freeMemory&quot;:24311408,
    &quot;totalMemory&quot;:85000192,
    &quot;connectedClientCount&quot;:1,
    &quot;master&quot;:true
  }
</code></pre>
</li>
</ul>
<a name="retrieve-connection-manager-information"></a><h5 id="retrieve-connection-manager-information">Retrieve Connection Manager Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/connectionManager</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/connectionManager
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;clientConnectionCount&quot;:2,
    &quot;activeConnectionCount&quot;:5,
    &quot;connectionCount&quot;:5
  }
</code></pre>
</li>
</ul>
<a name="retrieve-operation-service-information"></a><h5 id="retrieve-operation-service-information">Retrieve Operation Service Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/operationService</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/operationService
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;responseQueueSize&quot;:0,
    &quot;operationExecutorQueueSize&quot;:0,
    &quot;runningOperationsCount&quot;:0,
    &quot;remoteOperationCount&quot;:1,
    &quot;executedOperationCount&quot;:461139,
    &quot;operationThreadCount&quot;:8
  }
</code></pre>
</li>
</ul>
<a name="retrieve-event-service-information"></a><h5 id="retrieve-event-service-information">Retrieve Event Service Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/eventService</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/eventService
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;eventThreadCount&quot;:5,
    &quot;eventQueueCapacity&quot;:1000000,
    &quot;eventQueueSize&quot;:0
  }
</code></pre>
</li>
</ul>
<a name="retrieve-partition-service-information"></a><h5 id="retrieve-partition-service-information">Retrieve Partition Service Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/partitionService</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/partitionService
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;partitionCount&quot;:271,
    &quot;activePartitionCount&quot;:68
  }
</code></pre>
</li>
</ul>
<a name="retrieve-proxy-service-information"></a><h5 id="retrieve-proxy-service-information">Retrieve Proxy Service Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/proxyService</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/proxyService
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;proxyCount&quot;:8
  }
</code></pre>
</li>
</ul>
<a name="retrieve-all-managed-executors"></a><h5 id="retrieve-all-managed-executors">Retrieve All Managed Executors</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/managedExecutors</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701/managedExecutors
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;hz:system&quot;,&quot;hz:scheduled&quot;,&quot;hz:client&quot;,&quot;hz:query&quot;,&quot;hz:io&quot;,&quot;hz:async&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-a-managed-executor"></a><h5 id="retrieve-a-managed-executor">Retrieve a Managed Executor</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/members/{member}/managedExecutors/{managedExecutor}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/members/192.168.2.78:5701
    /managedExecutors/hz:system
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;name&quot;:&quot;hz:system&quot;,
    &quot;queueSize&quot;:0,
    &quot;poolSize&quot;:0,
    &quot;remainingQueueCapacity&quot;:2147483647,
    &quot;maximumPoolSize&quot;:4,
    &quot;completedTaskCount&quot;:12,
    &quot;terminated&quot;:false
  }
</code></pre>
</li>
</ul>
<a name="clients-resource"></a><h3 id="clients-resource">Clients Resource</h3>
<p>This resource returns a list of clients belonging to the provided cluster.</p>
<a name="retrieve-list-of-clients"></a><h5 id="retrieve-list-of-clients">Retrieve List of Clients</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/clients</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/clients
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;192.168.2.78:61708&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-client-information"></a><h5 id="retrieve-client-information">Retrieve Client Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/clients/{client}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/clients/192.168.2.78:61708
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;uuid&quot;:&quot;6fae7af6-7a7c-4fa5-b165-cde24cf070f5&quot;,
    &quot;address&quot;:&quot;192.168.2.78:61708&quot;,
    &quot;clientType&quot;:&quot;JAVA&quot;
  }
</code></pre>
</li>
</ul>
<a name="maps-resource"></a><h3 id="maps-resource">Maps Resource</h3>
<p>This resource returns a list of maps belonging to the provided cluster.</p>
<a name="retrieve-list-of-maps"></a><h5 id="retrieve-list-of-maps">Retrieve List of Maps</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/maps</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/maps
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;customers&quot;,&quot;orders&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-map-information"></a><h5 id="retrieve-map-information">Retrieve Map Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/maps/{mapName}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/maps/customers
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;customers&quot;,
    &quot;ownedEntryCount&quot;:1000,
    &quot;backupEntryCount&quot;:1000,
    &quot;ownedEntryMemoryCost&quot;:157890,
    &quot;backupEntryMemoryCost&quot;:113683,
    &quot;heapCost&quot;:297005,
    &quot;lockedEntryCount&quot;:0,
    &quot;dirtyEntryCount&quot;:0,
    &quot;hits&quot;:3001,
    &quot;lastAccessTime&quot;:1403608925777,
    &quot;lastUpdateTime&quot;:1403608925777,
    &quot;creationTime&quot;:1403602693388,
    &quot;putOperationCount&quot;:110630,
    &quot;getOperationCount&quot;:165945,
    &quot;removeOperationCount&quot;:55315,
    &quot;otherOperationCount&quot;:0,
    &quot;events&quot;:0,
    &quot;maxPutLatency&quot;:52,
    &quot;maxGetLatency&quot;:30,
    &quot;maxRemoveLatency&quot;:21
  }
</code></pre>
</li>
</ul>
<a name="multimaps-resource"></a><h3 id="multimaps-resource">MultiMaps Resource</h3>
<p>This resource returns a list of multimaps belonging to the provided cluster.</p>
<a name="retrieve-list-of-multimaps"></a><h5 id="retrieve-list-of-multimaps">Retrieve List of MultiMaps</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/multimaps</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/multimaps
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;customerAddresses&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-multimap-information"></a><h5 id="retrieve-multimap-information">Retrieve MultiMap Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/multimaps/{multimapname}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/multimaps/customerAddresses
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;customerAddresses&quot;,
    &quot;ownedEntryCount&quot;:996,
    &quot;backupEntryCount&quot;:996,
    &quot;ownedEntryMemoryCost&quot;:0,
    &quot;backupEntryMemoryCost&quot;:0,
    &quot;heapCost&quot;:0,
    &quot;lockedEntryCount&quot;:0,
    &quot;dirtyEntryCount&quot;:0,
    &quot;hits&quot;:0,
    &quot;lastAccessTime&quot;:1403603095521,
    &quot;lastUpdateTime&quot;:1403603095521,
    &quot;creationTime&quot;:1403602694158,
    &quot;putOperationCount&quot;:166041,
    &quot;getOperationCount&quot;:110694,
    &quot;removeOperationCount&quot;:55347,
    &quot;otherOperationCount&quot;:0,
    &quot;events&quot;:0,
    &quot;maxPutLatency&quot;:77,
    &quot;maxGetLatency&quot;:69,
    &quot;maxRemoveLatency&quot;:42
  }
</code></pre>
</li>
</ul>
<a name="queues-resource"></a><h3 id="queues-resource">Queues Resource</h3>
<p>This resource returns a list of queues belonging to the provided cluster.</p>
<a name="retrieve-list-of-queues"></a><h5 id="retrieve-list-of-queues">Retrieve List of Queues</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/queues</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/queues
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;messages&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-queue-information"></a><h5 id="retrieve-queue-information">Retrieve Queue Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/queues/{queueName}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/queues/messages
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;messages&quot;,
    &quot;ownedItemCount&quot;:55408,
    &quot;backupItemCount&quot;:55408,
    &quot;minAge&quot;:0,
    &quot;maxAge&quot;:0,
    &quot;aveAge&quot;:0,
    &quot;numberOfOffers&quot;:55408,
    &quot;numberOfRejectedOffers&quot;:0,
    &quot;numberOfPolls&quot;:0,
    &quot;numberOfEmptyPolls&quot;:0,
    &quot;numberOfOtherOperations&quot;:0,
    &quot;numberOfEvents&quot;:0,
    &quot;creationTime&quot;:1403602694196
  }
</code></pre>
</li>
</ul>
<a name="topics-resource"></a><h3 id="topics-resource">Topics Resource</h3>
<p>This resource returns a list of topics belonging to the provided cluster.</p>
<a name="retrieve-list-of-topics"></a><h5 id="retrieve-list-of-topics">Retrieve List of Topics</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/topics</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/topics
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;news&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-topic-information"></a><h5 id="retrieve-topic-information">Retrieve Topic Information</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/topics/{topicName}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/topics/news
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;news&quot;,
    &quot;numberOfPublishes&quot;:56370,
    &quot;totalReceivedMessages&quot;:56370,
    &quot;creationTime&quot;:1403602693411
  }
</code></pre>
</li>
</ul>
<a name="executors-resource"></a><h3 id="executors-resource">Executors Resource</h3>
<p>This resource returns a list of executors belonging to the provided cluster.</p>
<a name="retrieve-list-of-executors"></a><h5 id="retrieve-list-of-executors">Retrieve List of Executors</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/executors</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/executors
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  [&quot;order-executor&quot;]
</code></pre>
</li>
</ul>
<a name="retrieve-executor-information-get-restclustersclusternameexecutorsexecutorname"></a><h5 id="retrieve-executor-information-get-rest-clusters-clustername-executors-executorname-">Retrieve Executor Information [GET] [/rest/clusters/{clustername}/executors/{executorName}]</h5>
<ul>
<li><em>Request Type:</em> GET</li>
<li><em>URL:</em> <code>/rest/clusters/{clustername}/executors/{executorName}</code></li>
<li><p><em>Request:</em></p>
<pre><code class="lang-plain">  curl http://localhost:8083/mancenter/rest/clusters/dev/executors/order-executor
</code></pre>
</li>
<li><em>Response:</em> <code>200</code> (application/json)</li>
<li><p><em>Body:</em></p>
<pre><code class="lang-json">  {
    &quot;cluster&quot;:&quot;dev&quot;,
    &quot;name&quot;:&quot;order-executor&quot;,
    &quot;creationTime&quot;:1403602694196,
    &quot;pendingTaskCount&quot;:0,
    &quot;startedTaskCount&quot;:1241,
    &quot;completedTaskCount&quot;:1241,
    &quot;cancelledTaskCount&quot;:0
  }
</code></pre>
</li>
</ul>

<a name="security"></a><h1 id="security">Security</h1>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>This chapter describes the security features of Hazelcast. These features allow you to perform security activities, such as intercepting socket connections and remote operations executed by the clients, encrypting the communications between the members at socket level, and using SSL socket communication. All of the Security features explained in this chapter are the features of <font color="#3981DB"><strong>Hazelcast Enterprise</strong></font> edition.</p>
<a name="enabling-security-for-hazelcast-enterprise"></a><h2 id="enabling-security-for-hazelcast-enterprise">Enabling Security for Hazelcast Enterprise</h2>
<p>With Hazelcast&#39;s extensible, JAAS based security feature, you can:</p>
<ul>
<li>authenticate both cluster members and clients, </li>
<li>and perform access control checks on client operations. Access control can be done according to endpoint principal and/or endpoint address. </li>
</ul>
<p>You can enable security declaratively or programmatically, as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast xsi:schemaLocation=&quot;http://www.hazelcast.com/schema/config
    http://www.hazelcast.com/schema/config/hazelcast-config-3.3.xsd&quot;
    xmlns=&quot;http://www.hazelcast.com/schema/config&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;

  ...

  &lt;security enabled=&quot;true&quot;&gt;
    ...
  &lt;/security&gt;
&lt;/hazelcast&gt;
</code></pre>
<pre><code class="lang-java">Config cfg = new Config();
SecurityConfig securityCfg = cfg.getSecurityConfig();
securityCfg.setEnabled( true );
</code></pre>
<p>Also, see the <a href="#setting-the-license-key">Setting License Key section</a> for information on how to set your <font color="#3981DB"><strong>Hazelcast Enterprise</strong></font> license.</p>

<a name="socket-interceptor"></a><h2 id="socket-interceptor">Socket Interceptor</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Hazelcast allows you to intercept socket connections before a member joins a cluster or a client connects to a member of a cluster. This allow you to add custom hooks to join and perform connection procedures (like identity checking using Kerberos, etc.). </p>
<p>To use the socket interceptor, implement <code>com.hazelcast.nio.MemberSocketInterceptor</code> for members and <code>com.hazelcast.nio.SocketInterceptor</code> for clients.</p>
<p>The following example code enables the socket interceptor for members.</p>
<pre><code class="lang-java">public class MySocketInterceptor implements MemberSocketInterceptor {
  public void init( SocketInterceptorConfig socketInterceptorConfig ) {
    // initialize interceptor
  }

  void onConnect( Socket connectedSocket ) throws IOException {
    // do something meaningful when a member has connected to the cluster
  }

  public void onAccept( Socket acceptedSocket ) throws IOException {
    // do something meaningful when the cluster is ready to accept the member connection
  }
}
</code></pre>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    ...
    &lt;socket-interceptor enabled=&quot;true&quot;&gt;
      &lt;class-name&gt;com.hazelcast.examples.MySocketInterceptor&lt;/class-name&gt;
      &lt;properties&gt;
        &lt;property name=&quot;kerberos-host&quot;&gt;kerb-host-name&lt;/property&gt;
        &lt;property name=&quot;kerberos-config-file&quot;&gt;kerb.conf&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/socket-interceptor&gt;
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<pre><code class="lang-java">public class MyClientSocketInterceptor implements SocketInterceptor {
  void onConnect( Socket connectedSocket ) throws IOException {
    // do something meaningful when connected
  }
}

ClientConfig clientConfig = new ClientConfig();
clientConfig.setGroupConfig( new GroupConfig( &quot;dev&quot;, &quot;dev-pass&quot; ) )
    .addAddress( &quot;10.10.3.4&quot; );

MyClientSocketInterceptor clientSocketInterceptor = new MyClientSocketInterceptor();
clientConfig.setSocketInterceptor( clientSocketInterceptor );
HazelcastInstance client = HazelcastClient.newHazelcastClient( clientConfig );
</code></pre>

<a name="security-interceptor"></a><h2 id="security-interceptor">Security Interceptor</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Hazelcast allows you to intercept every remote operation executed by the client. This lets you add a very flexible custom security logic. To do this, implement <code>com.hazelcast.security.SecurityInterceptor</code>.</p>
<pre><code class="lang-java">public class MySecurityInterceptor implements SecurityInterceptor {

  public void before( Credentials credentials, String serviceName,
                      String methodName, Parameters parameters )
      throws AccessControlException {
    // credentials: client credentials 
    // serviceName: MapService.SERVICE_NAME, QueueService.SERVICE_NAME, ... etc
    // methodName: put, get, offer, poll, ... etc
    // parameters: holds parameters of the executed method, iterable.
  }

  public void after( Credentials credentials, String serviceName,
                     String methodName, Parameters parameters ) {
    // can be used for logging etc.
  }
}
</code></pre>
<p>The <code>before</code> method will be called before processing the request on the remote server. The <code>after</code> method will be called after the processing. Exceptions thrown while executing the <code>before</code> method will propagate to the client, but exceptions thrown while executing the <code>after</code> method will be suppressed.  </p>

<a name="encryption"></a><h2 id="encryption">Encryption</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Hazelcast allows you to encrypt the entire socket level communication among all Hazelcast members and clients. Encryption is based on <a href="http://java.sun.com/javase/6/docs/technotes/guides/security/crypto/CryptoSpec.html" target="_blank">Java Cryptography Architecture</a>. </p>
<p>Hazelcast provides symmetric and asymmetric encryption. For asymmetric encryption, SSL/TLS cryptographic protocols are used. For symmetric encryption, the following algorithms are used.</p>
<ul>
<li>DES/ECB/PKCS5Padding</li>
<li>PBEWithMD5AndDES</li>
<li>Blowfish</li>
<li>DESede</li>
</ul>
<p>Hazelcast uses MD5 message-digest algorithm as the cryptographic hash function. You can also use the salting process by giving a salt and password which are then concatenated and processed with MD5, and the resulting output is stored with the salt.</p>
<p>In symmetric encryption, each member uses the same key, so the key is shared. Here is an example configuration for symmetric encryption.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    ...
    &lt;symmetric-encryption enabled=&quot;true&quot;&gt;
      &lt;algorithm&gt;PBEWithMD5AndDES&lt;/algorithm&gt;
      &lt;salt&gt;thesalt&lt;/salt&gt;
      &lt;password&gt;thepass&lt;/password&gt;
      &lt;iteration-count&gt;19&lt;/iteration-count&gt;
    &lt;/symmetric-encryption&gt;
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>You set the encryption algorithm, the salt value to use for generating the secret key, the password to use when generating the secret key, and the iteration count to use when generating the secret key. You also need to set <code>enabled</code> to true. Note that all members should have the same encryption configuration.</p>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em>Encryption cannot be used on Hazelcast clients. Moreover, when you enable encryption on your Hazelcast cluster, the clients will not work, i.e., they will not be able to connect to the cluster.</em></p>
<p><br> </br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please see the <a href="#ssl">SSL section</a>.</em></p>

<a name="ssl"></a><h2 id="ssl">SSL</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You cannot use SSL when <a href="#encryption">Hazelcast Encryption</a> is enabled.</em></p>
<a name="ssl-for-hazelcast-members"></a><h3 id="ssl-for-hazelcast-members">SSL for Hazelcast Members</h3>
<p>Hazelcast allows you to encrypt socket level communication between Hazelcast members and between Hazelcast clients and members, for end to end encryption. To use it, you need to implement <code>com.hazelcast.nio.ssl.SSLContextFactory</code> and configure the SSL section in network configuration.</p>
<pre><code class="lang-java">public class MySSLContextFactory implements SSLContextFactory {
  public void init( Properties properties ) throws Exception {
  }

  public SSLContext getSSLContext() {
    ...
    SSLContext sslCtx = SSLContext.getInstance( protocol );
    return sslCtx;
  }
}
</code></pre>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    ...
    &lt;ssl enabled=&quot;true&quot;&gt;
      &lt;factory-class-name&gt;
          com.hazelcast.examples.MySSLContextFactory
      &lt;/factory-class-name&gt;
      &lt;properties&gt;
        &lt;property name=&quot;foo&quot;&gt;bar&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/ssl&gt;
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>Hazelcast provides a default SSLContextFactory, <code>com.hazelcast.nio.ssl.BasicSSLContextFactory</code>, which uses configured keystore to initialize <code>SSLContext</code>. You define <code>keyStore</code> and <code>keyStorePassword</code>, and you can set <code>keyManagerAlgorithm</code> (default <code>SunX509</code>), <code>trustManagerAlgorithm</code> (default <code>SunX509</code>) and <code>protocol</code> (default <code>TLS</code>).</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;network&gt;
    ...
    &lt;ssl enabled=&quot;true&quot;&gt;
      &lt;factory-class-name&gt;
          com.hazelcast.nio.ssl.BasicSSLContextFactory
      &lt;/factory-class-name&gt;
      &lt;properties&gt;
        &lt;property name=&quot;keyStore&quot;&gt;keyStore&lt;/property&gt;
        &lt;property name=&quot;keyStorePassword&quot;&gt;keyStorePassword&lt;/property&gt;
        &lt;property name=&quot;keyManagerAlgorithm&quot;&gt;SunX509&lt;/property&gt;
        &lt;property name=&quot;trustManagerAlgorithm&quot;&gt;SunX509&lt;/property&gt;
        &lt;property name=&quot;protocol&quot;&gt;TLS&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/ssl&gt;
  &lt;/network&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>You can set <code>keyStore</code> and <code>keyStorePassword</code> also using the following system properties.</p>
<ul>
<li><code>javax.net.ssl.keyStore</code></li>
<li><code>javax.net.ssl.keyStorePassword</code> </li>
</ul>
<a name="ssl-for-hazelcast-clients"></a><h3 id="ssl-for-hazelcast-clients">SSL for Hazelcast Clients</h3>
<p>Hazelcast Java and .NET clients also have SSL support. Following is a programmatic configuration for Java client:</p>
<pre><code class="lang-java">System.setProperty(&quot;javax.net.ssl.keyStore&quot;, new File(&quot;hazelcast.ks&quot;).getAbsolutePath());
System.setProperty(&quot;javax.net.ssl.trustStore&quot;, new File(&quot;hazelcast.ts&quot;).getAbsolutePath());
System.setProperty(&quot;javax.net.ssl.keyStorePassword&quot;, &quot;password&quot;);

ClientConfig clientConfig = new ClientConfig();
clientConfig.getNetworkConfig().addAddress(&quot;127.0.0.1&quot;);
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>When you use SSL with the Java client, it will have a throughput that is 50% of a non-SSL Java client with the same configuration. If it is a .NET client, it will have a throughput that is 46% of a non-SSL .NET client.</em></p>

<a name="credentials"></a><h2 id="credentials">Credentials</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>One of the key elements in Hazelcast security is the <code>Credentials</code> object, which carries all credentials of an endpoint (member or client). Credentials is an interface which extends <code>Serializable</code>. You can either implement the three methods in the <code>Credentials</code> interface, or you can extend the <code>AbstractCredentials</code> class, which is an abstract implementation of <code>Credentials</code>.</p>
<p>Hazelcast calls the <code>Credentials.setEndpoint()</code> method when an authentication request arrives at the member before authentication takes place.</p>
<pre><code class="lang-java">package com.hazelcast.security;
public interface Credentials extends Serializable {
  String getEndpoint();
  void setEndpoint( String endpoint ) ;    
  String getPrincipal() ;    
}
</code></pre>
<p>Here is an example of extending the <code>AbstractCredentials</code> class.</p>
<pre><code class="lang-java">package com.hazelcast.security;
...
public abstract class AbstractCredentials implements Credentials, DataSerializable {
  private transient String endpoint;
  private String principal;
  ...
}
</code></pre>
<p><code>UsernamePasswordCredentials</code>, a custom implementation of Credentials, is in the Hazelcast <code>com.hazelcast.security</code> package. <code>UsernamePasswordCredentials</code> is used for default configuration during the authentication process of both members and clients.</p>
<pre><code class="lang-java">package com.hazelcast.security;
...
public class UsernamePasswordCredentials extends Credentials {
  private byte[] password;
  ...
}
</code></pre>

<a name="clusterloginmodule"></a><h2 id="clusterloginmodule">ClusterLoginModule</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>All security attributes are carried in the <code>Credentials</code> object. <code>Credentials</code> is used by <a href="http://docs.oracle.com/javase/7/docs/api/javax/security/auth/spi/LoginModule.html" target="_blank">LoginModule</a> s during the authentication process. User supplied attributes from <code>LoginModule</code>s are accessed by <a href="http://docs.oracle.com/javase/7/docs/api/javax/security/auth/callback/CallbackHandler.html" target="_blank">CallbackHandler</a> s. To access the <code>Credentials</code> object, Hazelcast uses its own specialized <code>CallbackHandler</code>. During initialization of <code>LoginModules</code>, Hazelcast passes this special <code>CallbackHandler</code> into the <code>LoginModule.initialize()</code> method.</p>
<p>Your implementation of <code>LoginModule</code> should create an instance of <code>com.hazelcast.security.CredentialsCallback</code> and call the <code>handle(Callback[] callbacks)</code> method of <code>CallbackHandler</code> during the login process. </p>
<p><code>CredentialsCallback.getCredentials()</code> returns the supplied <code>Credentials</code> object.</p>
<pre><code class="lang-java">public class CustomLoginModule implements LoginModule {
  CallbackHandler callbackHandler;
  Subject subject;

  public void initialize( Subject subject, CallbackHandler callbackHandler,
                          Map&lt;String, ?&gt; sharedState, Map&lt;String, ?&gt; options ) {
    this.subject = subject;
    this.callbackHandler = callbackHandler;
  }

  public final boolean login() throws LoginException {
    CredentialsCallback callback = new CredentialsCallback();
    try {
      callbackHandler.handle( new Callback[] { callback } );
      credentials = cb.getCredentials();
    } catch ( Exception e ) {
      throw new LoginException( e.getMessage() );
    }
    ...
  }
  ...
}
</code></pre>
<p>To use the default Hazelcast permission policy, you must create an instance of <code>com.hazelcast.security.ClusterPrincipal</code> that holds the <code>Credentials</code> object, and you must add it to <code>Subject.principals onLoginModule.commit()</code> as shown below.</p>
<pre><code class="lang-java">public class MyCustomLoginModule implements LoginModule {
  ...
  public boolean commit() throws LoginException {
    ...
    Principal principal = new ClusterPrincipal( credentials );
    subject.getPrincipals().add( principal );

    return true;
  }
  ...
}
</code></pre>
<p>Hazelcast has an abstract implementation of <code>LoginModule</code> that does callback and cleanup operations and holds the resulting <code>Credentials</code> instance. <code>LoginModule</code>s extending <code>ClusterLoginModule</code> can access <code>Credentials</code>, <code>Subject</code>, <code>LoginModule</code> instances and options, and <code>sharedState</code> maps. Extending the <code>ClusterLoginModule</code> is recommended instead of implementing all required stuff.</p>
<pre><code class="lang-java">package com.hazelcast.security;
...
public abstract class ClusterLoginModule implements LoginModule {

  protected abstract boolean onLogin() throws LoginException;
  protected abstract boolean onCommit() throws LoginException;
  protected abstract boolean onAbort() throws LoginException;
  protected abstract boolean onLogout() throws LoginException;
}
</code></pre>
<p><br></br></p>
<a name="enterprise-integration"></a><h3 id="enterprise-integration">Enterprise Integration</h3>
<p>Using the above API, you can implement a <code>LoginModule</code> that performs authentication against the Security System of your choice, such as an LDAP store like <a href="https://directory.apache.org/" target="_blank">Apache Directory</a> or some other corporate standard you might have. For example, you may wish to have your clients send an identification token in the <code>Credentials</code> object. This token can then be sent to your back-end security system via the <code>LoginModule</code> that runs on the cluster side.</p>
<p>Additionally, the same system may authenticate the user and also then return the roles that are attributed to the user. These roles can then be used for data structure authorization. </p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to <a href="http://docs.oracle.com/javase/7/docs/technotes/guides/security/jaas/JAASRefGuide.html" target="_blank">JAAS Reference Guide</a> for further information.</em></p>

<a name="cluster-member-security"></a><h2 id="cluster-member-security">Cluster Member Security</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Hazelcast supports standard Java Security (JAAS) based authentication between cluster members. To implement it, you configure one or more LoginModules and an instance of <code>com.hazelcast.security.ICredentialsFactory</code>. Although Hazelcast has default implementations using cluster group and group-password and UsernamePasswordCredentials on authentication, it is recommended that you implement the LoginModules and an instance of <code>com.hazelcast.security.ICredentialsFactory</code> according to your specific needs and environment.</p>
<pre><code class="lang-xml">&lt;security enabled=&quot;true&quot;&gt;
  &lt;member-credentials-factory 
      class-name=&quot;com.hazelcast.examples.MyCredentialsFactory&quot;&gt;
    &lt;properties&gt;
      &lt;property name=&quot;property1&quot;&gt;value1&lt;/property&gt;
      &lt;property name=&quot;property2&quot;&gt;value2&lt;/property&gt;
    &lt;/properties&gt;
  &lt;/member-credentials-factory&gt;
  &lt;member-login-modules&gt;
    &lt;login-module usage=&quot;required&quot;
        class-name=&quot;com.hazelcast.examples.MyRequiredLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property3&quot;&gt;value3&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
    &lt;login-module usage=&quot;sufficient&quot;
        class-name=&quot;com.hazelcast.examples.MySufficientLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property4&quot;&gt;value4&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
    &lt;login-module usage=&quot;optional&quot;
        class-name=&quot;com.hazelcast.examples.MyOptionalLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property5&quot;&gt;value5&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
  &lt;/member-login-modules&gt;
  ...
&lt;/security&gt;
</code></pre>
<p>You can define as many as LoginModules as you want in configuration. They are executed in the order listed in configuration. The <code>usage</code> attribute has 4 values: &#39;required&#39;, &#39;requisite&#39;, &#39;sufficient&#39; and &#39;optional&#39; as defined in <code>javax.security.auth.login.AppConfigurationEntry.LoginModuleControlFlag</code>.</p>
<pre><code class="lang-java">package com.hazelcast.security;
/**
 * ICredentialsFactory is used to create Credentials objects to be used
 * during member authentication before connection accepted by master member.
 */
public interface ICredentialsFactory {

  void configure( GroupConfig groupConfig, Properties properties );

  Credentials newCredentials();

  void destroy();
}
</code></pre>
<p>Properties defined in the configuration are passed to the <code>ICredentialsFactory.configure()</code> method as <code>java.util.Properties</code> and to the <code>LoginModule.initialize()</code> method as <code>java.util.Map</code>.</p>

<a name="native-client-security"></a><h2 id="native-client-security">Native Client Security</h2>
<p><font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>
<br></br></p>
<p>Hazelcast&#39;s Client security includes both authentication and authorization.</p>
<a name="authentication"></a><h3 id="authentication">Authentication</h3>
<p>The authentication mechanism works the same as cluster member authentication. To implement client authentication, you configure a Credential and one or more LoginModules. The client side does not have and does not need a factory object to create Credentials objects like <code>ICredentialsFactory</code>. You must create the credentials at the client side and send them to the connected member during the connection process.</p>
<pre><code class="lang-xml">&lt;security enabled=&quot;true&quot;&gt;
  &lt;client-login-modules&gt;
    &lt;login-module usage=&quot;required&quot;
        class-name=&quot;com.hazelcast.examples.MyRequiredClientLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property3&quot;&gt;value3&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
    &lt;login-module usage=&quot;sufficient&quot;
        class-name=&quot;com.hazelcast.examples.MySufficientClientLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property4&quot;&gt;value4&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
    &lt;login-module usage=&quot;optional&quot;
        class-name=&quot;com.hazelcast.examples.MyOptionalClientLoginModule&quot;&gt;
      &lt;properties&gt;
        &lt;property name=&quot;property5&quot;&gt;value5&lt;/property&gt;
      &lt;/properties&gt;
    &lt;/login-module&gt;
  &lt;/client-login-modules&gt;
  ...
&lt;/security&gt;
</code></pre>
<p>You can define as many as <code>LoginModules</code> as you want in configuration. Those are executed in the order given in configuration. The <code>usage</code> attribute has 4 values: &#39;required&#39;, &#39;requisite&#39;, &#39;sufficient&#39; and &#39;optional&#39; as defined in <code>javax.security.auth.login.AppConfigurationEntry.LoginModuleControlFlag</code>.</p>
<pre><code class="lang-java">ClientConfig clientConfig = new ClientConfig();
clientConfig.setCredentials( new UsernamePasswordCredentials( &quot;dev&quot;, &quot;dev-pass&quot; ) );
HazelcastInstance client = HazelcastClient.newHazelcastClient( clientConfig );
</code></pre>
<a name="authorization"></a><h3 id="authorization">Authorization</h3>
<p>Hazelcast client authorization is configured by a client permission policy. Hazelcast has a default permission policy implementation that uses permission configurations defined in the Hazelcast security configuration. Default policy permission checks are done against instance types (map, queue, etc.), instance names (map, queue, name, etc.), instance actions (put, read, remove, add, etc.), client endpoint addresses, and client principal defined by the Credentials object. Instance and principal names and endpoint addresses can be defined as wildcards(*). Please see the <a href="#network-configuration">Network Configuration section</a> and <a href="#using-wildcards">Using Wildcards section</a>.</p>
<pre><code class="lang-xml">&lt;security enabled=&quot;true&quot;&gt;
  &lt;client-permissions&gt;
    &lt;!-- Principal &#39;admin&#39; from endpoint &#39;127.0.0.1&#39; has all permissions. --&gt;
    &lt;all-permissions principal=&quot;admin&quot;&gt;
      &lt;endpoints&gt;
        &lt;endpoint&gt;127.0.0.1&lt;/endpoint&gt;
      &lt;/endpoints&gt;
    &lt;/all-permissions&gt;

    &lt;!-- Principals named &#39;dev&#39; from all endpoints have &#39;create&#39;, &#39;destroy&#39;, 
         &#39;put&#39;, &#39;read&#39; permissions for map named &#39;default&#39;. --&gt;
    &lt;map-permission name=&quot;default&quot; principal=&quot;dev&quot;&gt;
      &lt;actions&gt;
        &lt;action&gt;create&lt;/action&gt;
        &lt;action&gt;destroy&lt;/action&gt;
        &lt;action&gt;put&lt;/action&gt;
        &lt;action&gt;read&lt;/action&gt;
      &lt;/actions&gt;
    &lt;/map-permission&gt;

    &lt;!-- All principals from endpoints &#39;127.0.0.1&#39; or matching to &#39;10.10.*.*&#39; 
         have &#39;put&#39;, &#39;read&#39;, &#39;remove&#39; permissions for map
         whose name matches to &#39;com.foo.entity.*&#39;. --&gt;
    &lt;map-permission name=&quot;com.foo.entity.*&quot;&gt;
      &lt;endpoints&gt;
        &lt;endpoint&gt;10.10.*.*&lt;/endpoint&gt;
        &lt;endpoint&gt;127.0.0.1&lt;/endpoint&gt;
      &lt;/endpoints&gt;
      &lt;actions&gt;
        &lt;action&gt;put&lt;/action&gt;
        &lt;action&gt;read&lt;/action&gt;
        &lt;action&gt;remove&lt;/action&gt;
      &lt;/actions&gt;
    &lt;/map-permission&gt;

    &lt;!-- Principals named &#39;dev&#39; from endpoints matching to either 
         &#39;192.168.1.1-100&#39; or &#39;192.168.2.*&#39; 
         have &#39;create&#39;, &#39;add&#39;, &#39;remove&#39; permissions for all queues. --&gt;
    &lt;queue-permission name=&quot;*&quot; principal=&quot;dev&quot;&gt;
      &lt;endpoints&gt;
        &lt;endpoint&gt;192.168.1.1-100&lt;/endpoint&gt;
        &lt;endpoint&gt;192.168.2.*&lt;/endpoint&gt;
      &lt;/endpoints&gt;
      &lt;actions&gt;
        &lt;action&gt;create&lt;/action&gt;
        &lt;action&gt;add&lt;/action&gt;
        &lt;action&gt;remove&lt;/action&gt;
      &lt;/actions&gt;
    &lt;/queue-permission&gt;

    &lt;!-- All principals from all endpoints have transaction permission.--&gt;
    &lt;transaction-permission /&gt;
  &lt;/client-permissions&gt;
&lt;/security&gt;
</code></pre>
<p>You can also define your own policy by implementing <code>com.hazelcast.security.IPermissionPolicy</code>.</p>
<pre><code class="lang-java">package com.hazelcast.security;
/**
 * IPermissionPolicy is used to determine any Subject&#39;s 
 * permissions to perform a security sensitive Hazelcast operation.
 *
 */
public interface IPermissionPolicy {
  void configure( SecurityConfig securityConfig, Properties properties );

  PermissionCollection getPermissions( Subject subject,
                                       Class&lt;? extends Permission&gt; type );

  void destroy();
}
</code></pre>
<p>Permission policy implementations can access client-permissions that are in configuration by using
<code>SecurityConfig.
getClientPermissionConfigs()</code> when Hazelcast calls the method <code>configure(SecurityConfig securityConfig, Properties properties)</code>.</p>
<p>The <code>IPermissionPolicy.getPermissions(Subject subject, Class&lt;? extends Permission&gt; type)</code> method is used to determine a client request that has been granted permission to perform a security-sensitive operation. </p>
<p>Permission policy should return a <code>PermissionCollection</code> containing permissions of the given type for the given <code>Subject</code>. The Hazelcast access controller will call <code>PermissionCollection.implies(Permission)</code> on returning <code>PermissionCollection</code> and it will decide whether or not the current <code>Subject</code> has permission to access the requested resources.</p>
<a name="permissions"></a><h3 id="permissions">Permissions</h3>
<ul>
<li>All Permission</li>
</ul>
<pre><code class="lang-xml">&lt;all-permissions principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
&lt;/all-permissions&gt;
</code></pre>
<ul>
<li>Map Permission</li>
</ul>
<pre><code class="lang-xml">&lt;map-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/map-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, put, read, remove, lock, intercept, index, listen
</code></pre><ul>
<li>Queue Permission</li>
</ul>
<pre><code class="lang-xml">&lt;queue-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/queue-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, add, remove, read, listen
</code></pre><ul>
<li>Multimap Permission</li>
</ul>
<pre><code class="lang-xml">&lt;multimap-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/multimap-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, put, read, remove, listen, lock
</code></pre><ul>
<li>Topic Permission</li>
</ul>
<pre><code class="lang-xml">&lt;topic-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/topic-permission&gt;
</code></pre>
<pre><code>Actions: create, destroy, publish, listen
</code></pre><ul>
<li>List Permission</li>
</ul>
<pre><code class="lang-xml">&lt;list-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/list-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, add, read, remove, listen
</code></pre><ul>
<li>Set Permission</li>
</ul>
<pre><code class="lang-xml">&lt;set-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/set-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, add, read, remove, listen
</code></pre><ul>
<li>Lock Permission</li>
</ul>
<pre><code class="lang-xml">&lt;lock-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/lock-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, lock, read
</code></pre><ul>
<li>AtomicLong Permission</li>
</ul>
<pre><code class="lang-xml">&lt;atomic-long-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
        ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/atomic-long-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, read, modify
</code></pre><ul>
<li>CountDownLatch Permission</li>
</ul>
<pre><code class="lang-xml">&lt;countdown-latch-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/countdown-latch-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, modify, read
</code></pre><ul>
<li>IdGenerator Permission</li>
</ul>
<pre><code class="lang-xml">&lt;id-generator-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/id-generator-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, modify, read
</code></pre><ul>
<li>Semaphore Permission</li>
</ul>
<pre><code class="lang-xml">&lt;semaphore-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/semaphore-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy, acquire, release, read
</code></pre><ul>
<li>Executor Service Permission</li>
</ul>
<pre><code class="lang-xml">&lt;executor-service-permission name=&quot;name&quot; principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
  &lt;actions&gt;
    ...
  &lt;/actions&gt;
&lt;/executor-service-permission&gt;
</code></pre>
<pre><code>Actions: all, create, destroy
</code></pre><ul>
<li>Transaction Permission</li>
</ul>
<pre><code class="lang-xml">&lt;transaction-permission principal=&quot;principal&quot;&gt;
  &lt;endpoints&gt;
    ...
  &lt;/endpoints&gt;
&lt;/transaction-permission&gt;
</code></pre>
<p><br> </br></p>

<a name="performance"></a><h1 id="performance">Performance</h1>
<p>This chapter provides information on the performance features of Hazelcast including slow operations detector, back pressure and data affinity. Moreover, the chapter describes the best performance practices for Hazelcast deployed on Amazon EC2. It also describes the threading models for I/O, events, executors and operations. </p>
<a name="data-affinity"></a><h2 id="data-affinity">Data Affinity</h2>
<p>Data affinity ensures that related entries exist on the same member. If related data is on the same member, operations can be executed without the cost of extra network calls and extra wire data. This feature is provided by using the same partition keys for related data.</p>
<p><strong>Co-location of related data and computation</strong></p>
<p>Hazelcast has a standard way of finding out which member owns/manages each key object. The following operations will be routed to the same member, since all of them are operating based on the same key <code>&quot;key1&quot;</code>.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Map mapA = hazelcastInstance.getMap( &quot;mapA&quot; );
Map mapB = hazelcastInstance.getMap( &quot;mapB&quot; );
Map mapC = hazelcastInstance.getMap( &quot;mapC&quot; );

// since map names are different, operation will be manipulating
// different entries, but the operation will take place on the
// same member since the keys (&quot;key1&quot;) are the same
mapA.put( &quot;key1&quot;, value );
mapB.get( &quot;key1&quot; );
mapC.remove( &quot;key1&quot; );

// lock operation will still execute on the same member
// of the cluster since the key (&quot;key1&quot;) is same
hazelcastInstance.getLock( &quot;key1&quot; ).lock();

// distributed execution will execute the &#39;runnable&#39; on the
// same member since &quot;key1&quot; is passed as the key.   
hazelcastInstance.getExecutorService().executeOnKeyOwner( runnable, &quot;key1&quot; );
</code></pre>
<p>When the keys are the same, entries are stored on the same member. But we sometimes want to have related entries stored on the same member, such as a customer and his/her order entries. We would have a customers map with customerId as the key and an orders map with orderId as the key. Since customerId and orderId are different keys, a customer and his/her orders may fall into different members in your cluster. So how can we have them stored on the same member? We create an affinity between customer and orders. If we make them part of the same partition then these entries will be co-located. We achieve this by making orderIds <code>PartitionAware</code>.</p>
<pre><code class="lang-java">public class OrderKey implements Serializable, PartitionAware {

  private final long customerId;
  private final long orderId;

  public OrderKey( long orderId, long customerId ) {
    this.customerId = customerId;
    this.orderId = orderId;
  }

  public long getCustomerId() {
    return customerId;
  }

  public long getOrderId() {
    return orderId;
  }

  public Object getPartitionKey() {
    return customerId;
  }

  @Override
  public String toString() {
    return &quot;OrderKey{&quot;
      + &quot;customerId=&quot; + customerId
      + &quot;, orderId=&quot; + orderId
      + &#39;}&#39;;
  }
}
</code></pre>
<p>Notice that OrderKey implements <code>PartitionAware</code> and that <code>getPartitionKey()</code> returns the <code>customerId</code>. This will make sure that the <code>Customer</code> entry and its <code>Order</code>s will be stored on the same member.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Map mapCustomers = hazelcastInstance.getMap( &quot;customers&quot; );
Map mapOrders = hazelcastInstance.getMap( &quot;orders&quot; );

// create the customer entry with customer id = 1
mapCustomers.put( 1, customer );

// now create the orders for this customer
mapOrders.put( new OrderKey( 21, 1 ), order );
mapOrders.put( new OrderKey( 22, 1 ), order );
mapOrders.put( new OrderKey( 23, 1 ), order );
</code></pre>
<p>Assume that you have a customers map where <code>customerId</code> is the key and the customer object is the value. You want to remove one of the customer orders and return the number of remaining orders. Here is how you would normally do it.</p>
<pre><code class="lang-java">public static int removeOrder( long customerId, long orderId ) throws Exception {
  IMap&lt;Long, Customer&gt; mapCustomers = instance.getMap( &quot;customers&quot; );
  IMap mapOrders = hazelcastInstance.getMap( &quot;orders&quot; );

  mapCustomers.lock( customerId );
  mapOrders.remove( orderId );
  Set orders = orderMap.keySet(Predicates.equal( &quot;customerId&quot;, customerId ));
  mapCustomers.unlock( customerId );

  return orders.size();
}
</code></pre>
<p>There are couple of things you should consider.</p>
<ol>
<li>There are four distributed operations there: lock, remove, keySet, unlock. Can you reduce 
the number of distributed operations?</li>
<li>The customer object may not be that big, but can you not have to pass that object through the 
wire? Think about a scenario where you set order count to the customer object for fast access, so you 
should do a get and a put, and as a result, the customer object is passed through the wire twice.</li>
</ol>
<p>Instead, why not move the computation over to the member (JVM) where your customer data resides. Here is how you can do this with distributed executor service.</p>
<ol>
<li>Send a <code>PartitionAware</code> <code>Callable</code> task.</li>
<li><code>Callable</code> does the deletion of the order right there and returns with the remaining 
order count.</li>
<li>Upon completion of the <code>Callable</code> task, return the result (remaining order count). You 
do not have to wait until the task is completed; since distributed executions are asynchronous, you can do other things in the meantime.</li>
</ol>
<p>Here is some example code.</p>
<pre><code class="lang-java">HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

public int removeOrder( long customerId, long orderId ) throws Exception {
  IExecutorService executorService
    = hazelcastInstance.getExecutorService( &quot;ExecutorService&quot; );

  OrderDeletionTask task = new OrderDeletionTask( customerId, orderId );
  Future&lt;Integer&gt; future = executorService.submit( task );
  int remainingOrders = future.get();

  return remainingOrders;
}

public static class OrderDeletionTask
    implements Callable&lt;Integer&gt;, PartitionAware, Serializable {

  private long customerId;
  private long orderId;

  public OrderDeletionTask() {
  }

  public OrderDeletionTask(long customerId, long orderId) {
    this.customerId = customerId;
    this.orderId = orderId;
  }

  @Override
  public Integer call() {
    Map&lt;Long, Customer&gt; customerMap = hazelcastInstance.getMap( &quot;customers&quot; );
    IMap&lt;OrderKey, Order&gt; orderMap = hazelcastInstance.getMap( &quot;orders&quot; );

    mapCustomers.lock( customerId );
    Customer customer = mapCustomers.get( customerId );
    Predicate predicate = Predicates.equal( &quot;customerId&quot;, customerId );
    Set&lt;OrderKey&gt; orderKeys = orderMap.localKeySet( predicate );
    int orderCount = orderKeys.size();
    for (OrderKey key : orderKeys) {
      if (key.orderId == orderId) {
        orderCount--;
        orderMap.delete( key );
      }
    }
    mapCustomers.unlock( customerId );

    return orderCount;
  }

  @Override
  public Object getPartitionKey() {
    return customerId;
  }
}
</code></pre>
<p>The benefits of doing the same operation with distributed <code>ExecutorService</code> based on the key are:</p>
<ul>
<li>Only one distributed execution (<code>executorService.submit(task)</code>), instead of four.</li>
<li>Less data is sent over the wire.</li>
<li>Since lock/update/unlock cycle is done locally (local to the customer data), lock duration for the <code>Customer</code> entry is much less, thus enabling higher concurrency.</li>
</ul>

<a name="back-pressure"></a><h2 id="back-pressure">Back Pressure</h2>
<p>Hazelcast uses operations to make remote calls. For example, a <code>map.get</code> is an operation and a <code>map.put</code> is one operation for the primary 
and one operation for each of the backups, i.e. <code>map.put</code> is executed for the primary and also for each backup. In most cases, there will be a natural balance between the number of threads performing operations
and the number of operations being executed. However, there are two situations where this balance and operations 
can pile up and eventually lead to Out of Memory Exception (OOME):</p>
<ul>
<li><p>Asynchronous calls: With async calls, the system may be flooded with the requests.</p>
</li>
<li><p>Asynchronous backups: The asynchronous backups may be piling up.</p>
</li>
</ul>
<p>To prevent the system from crashing, Hazelcast provides back pressure. Back pressure works by:</p>
<ul>
<li><p>limiting the number of concurrent operation invocations, </p>
</li>
<li><p>periodically making an async backup sync.</p>
</li>
</ul>
<p>Back pressure is disabled by default and you can enable it using the following system property:</p>
<p><code>hazelcast.backpressure.enabled</code></p>
<p>To control the number of concurrent invocations, you can configure the number of invocations allowed per partition using the 
following system property:</p>
<p><code>hazelcast.backpressure.max.concurrent.invocations.per.partition</code></p>
<p>The default value of this system property is 100. Using a default configuration a system is allowed to have (271 + 1) * 100 = 27200 concurrent invocations (271 partitions + 1 for generic operations).</p>
<p>Back pressure is only applied to normal operations. System operations like heart beats and partition migration operations 
are not influenced by back pressure. 27200 invocations might seem like a lot, but keep in mind that executing a task on <code>IExecutor</code> 
or acquiring a lock also requires an operation.</p>
<p>If the maximum number of invocations has been reached, Hazelcast will automatically apply an exponential back off policy. This
gives the system some time to deal with the load. Using the following system property, you can configure the maximum time to wait before a <code>HazelcastOverloadException</code> is thrown:</p>
<p><code>hazelcast.backpressure.backoff.timeout.millis</code></p>
<p>This system property&#39;s default value is 60000 ms.</p>
<p>The Health Monitor keeps an eye on the usage of the invocations. If it sees a member has consumed 70% or more of the invocations, it starts to log health messages.</p>
<p>Apart from controlling the number of invocations, you also need to control the number of pending async backups. This is done
by periodically making these backups sync instead of async. This forces all pending backups to get drained. For this, Hazelcast tracks the number of asynchronous backups for each partition. At every <strong>Nth</strong> call, one synchronization is forced. This <strong>N</strong> is 
controlled through the following property:</p>
<p><code>hazelcast.backpressure.syncwindow</code></p>
<p>This system property&#39;s default value is 100. It means, out of 100 <em>asynchronous</em> backups, Hazelcast makes 1 of them a <em>synchronous</em> one. A randomization is added, so the sync window with default configuration will be between 75 and 125 
invocations. </p>
<p><br></br></p>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#system-properties">System Properties section</a> to learn how to configure the system properties.</em></p>

<a name="threading-model"></a><h2 id="threading-model">Threading Model</h2>
<p>Your application server has its own threads. Hazelcast does not use these; it manages its own threads.</p>
<a name="io-threading"></a><h3 id="i-o-threading">I/O Threading</h3>
<p>Hazelcast uses a pool of threads for I/O. A single thread does not perform all the I/O. Instead, multiple threads perform the I/O. On each cluster member, the I/O threading is split up in 3 types of I/O threads:</p>
<ul>
<li>I/O thread for the accept requests.</li>
<li>I/O threads to read data from other members/clients.</li>
<li>I/O threads to write data to other members/clients.</li>
</ul>
<p>You can configure the number of I/O threads using the <code>hazelcast.io.thread.count</code> system property. Its default value is 3 per member. If 3 is used, in total there are 7 I/O threads: 1 accept I/O thread, 3 read I/O threads, and 3 write I/O threads. Each I/O thread has its own Selector instance and waits on the <code>Selector.select</code> if there is nothing to do.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>You can also specify counts for input and output threads separately. There are <code>hazelcast.io.input.thread.count</code> and <code>hazelcast.io.output.thread.count</code> properties for this purpose. Please refer to the <a href="#system-properties">System Properties section</a> for information on these properties and how to set them.</em></p>
<p>Hazelcast periodically scans utilization of each I/O thread and can decide to migrate a connection to a new thread if the existing thread is servicing a disproportionate number of I/O events. You can customize the scanning interval by configuring the <code>hazelcast.io.balancer.interval.seconds</code> system property; its default interval is 20 seconds. You can disable the balancing process by setting this property to a negative value.</p>
<p>In case of the read I/O thread, when sufficient bytes for a packet have been received, the <code>Packet</code> object is created. This <code>Packet</code> object is 
then sent to the system where it is de-multiplexed. If the <code>Packet</code> header signals that it is an operation/response, the <code>Packet</code> is handed 
over to the operation service (please see the <a href="#operation-threading">Operation Threading section</a>). If the <code>Packet</code> is an event, it is handed 
over to the event service (please see the <a href="#event-threading">Event Threading section</a>). </p>

<a name="event-threading"></a><h3 id="event-threading">Event Threading</h3>
<p>Hazelcast uses a shared event system to deal with components that rely on events, such as topic, collections, listeners, and Near Cache. </p>
<p>Each cluster member has an array of event threads and each thread has its own work queue. When an event is produced,
either locally or remotely, an event thread is selected (depending on if there is a message ordering) and the event is placed
in the work queue for that event thread.</p>
<p>The following properties
can be set to alter the behavior of the system.</p>
<ul>
<li><code>hazelcast.event.thread.count</code>: Number of event-threads in this array. Its default value is 5.</li>
<li><code>hazelcast.event.queue.capacity</code>: Capacity of the work queue. Its default value is 1000000.</li>
<li><code>hazelcast.event.queue.timeout.millis</code>: Timeout for placing an item on the work queue. Its default value is 250.</li>
</ul>
<p>If you process a lot of events and have many cores, changing the value of <code>hazelcast.event.thread.count</code> property to
a higher value is a good practice. This way, more events can be processed in parallel.</p>
<p>Multiple components share the same event queues. If there are 2 topics, say A and B, for certain messages
they may share the same queue(s) and hence the same event thread. If there are a lot of pending messages produced by A, then B needs to wait.
Also, when processing a message from A takes a lot of time and the event thread is used for that, B suffers from this. 
That is why it is better to offload processing to a dedicated thread (pool) so that systems are better isolated.</p>
<p>If the events are produced at a higher rate than they are consumed, the queue grows in size. To prevent overloading the system
and running into an <code>OutOfMemoryException</code>, the queue is given a capacity of 1 million items. When the maximum capacity is reached, the items are
dropped. This means that the event system is a &#39;best effort&#39; system. There is no guarantee that you are going to get an
event. Topic A might have a lot of pending messages and therefore B cannot receive messages because the queue
has no capacity and messages for B are dropped.</p>

<a name="iexecutor-threading"></a><h3 id="iexecutor-threading">IExecutor Threading</h3>
<p>Executor threading is straight forward. When a task is received to be executed on Executor E, then E will have its
own <code>ThreadPoolExecutor</code> instance and the work is placed in the work queue of this executor. Thus, Executors are fully isolated, but still share the same underlying hardware - most importantly the CPUs. </p>
<p>You can configure the IExecutor using the <code>ExecutorConfig</code> (programmatic configuration) or using <code>&lt;executor&gt;</code> (declarative configuration). Please also see the <a href="#configuring-executor-service">Configuring Executor Service section</a>.</p>

<a name="operation-threading"></a><h3 id="operation-threading">Operation Threading</h3>
<p>There are 2 types of operations:</p>
<ul>
<li>Operations that are aware of a certain partition, e.g. <code>IMap.get(key)</code>.</li>
<li>Operations that are not partition aware, such as the <code>IExecutorService.executeOnMember(command, member)</code> operation.</li>
</ul>
<p>Each of these operation types has a different threading model explained in the following sections.</p>
<a name="partition-aware-operations"></a><h4 id="partition-aware-operations">Partition-aware Operations</h4>
<p>To execute partition-aware operations, an array of operation threads is created. The default value of this array&#39;s size is the number of cores and it has a minimum value of 2. This value can be changed using the <code>hazelcast.operation.thread.count</code> property.</p>
<p>Each operation thread has its own work queue and it consumes messages from this work queue. If a partition-aware 
operation needs to be scheduled, the right thread is found using the formula below.</p>
<p><code>threadIndex = partitionId % partition thread-count</code></p>
<p>After the <code>threadIndex</code> is determined, the operation is put in the work queue of that operation thread. This means the followings:</p>
<ul>
<li><p>A single operation thread executes operations for multiple partitions; if there are 271 partitions and
10 partition threads, then roughly every operation thread executes operations for 27 partitions. </p>
</li>
<li><p>Each partition belongs to only 1 operation thread. All operations for a partition are always handled by exactly the same operation thread. </p>
</li>
<li><p>Concurrency control is not needed to deal with partition-aware operations because once a partition-aware
operation is put in the work queue of a partition-aware operation thread, only 
1 thread is able to touch that partition.</p>
</li>
</ul>
<p>Because of this threading strategy, there are two forms of false sharing you need to be aware of:</p>
<ul>
<li><p>False sharing of the partition - two completely independent data structures share the same partition. For example, if there
is a map <code>employees</code> and a map <code>orders</code>, the method <code>employees.get(&quot;peter&quot;)</code> running on partition 25 may be blocked
by the method <code>orders.get(1234)</code> also running on partition 25. If independent data structures share the same partition,
a slow operation on one data structure can slow down the other data structures.</p>
</li>
<li><p>False sharing of the partition-aware operation thread - each operation thread is responsible for executing
operations on a number of partitions. For example, <em>thread 1</em> could be responsible for partitions 0, 10, 20, etc. and <em>thread-2</em> could be responsible for partitions
1, 11, 21, etc. If an operation for partition 1 takes a lot of time, it blocks the execution of an operation for partition
11 because both of them are mapped to the same operation thread.</p>
</li>
</ul>
<p>You need to be careful with long running operations because you could starve operations of a thread. 
As a general rule, the partition thread should be released as soon as possible because operations are not designed
as long running operations. That is why, for example, it is very dangerous to execute a long running operation 
using <code>AtomicReference.alter()</code> or an <code>IMap.executeOnKey()</code>, because these operations block other operations to be executed.</p>
<p>Currently, there is no support for work stealing. Different partitions that map to the same thread may need to wait 
till one of the partitions is finished, even though there are other free partition-aware operation threads available.</p>
<p><strong>Example:</strong></p>
<p>Take a cluster with three members. Two members will have 90 primary partitions and one member will have 91 primary partitions. Let&#39;s
say you have one CPU and four cores per CPU. By default, four operation threads will be allocated to serve 90 or 91 partitions.</p>
<a name="operations-that-are-not-partition-aware"></a><h4 id="operations-that-are-not-partition-aware">Operations that are Not Partition-aware</h4>
<p>To execute operations that are not partition-aware, e.g. <code>IExecutorService.executeOnMember(command, member)</code>, generic operation 
threads are used. When the Hazelcast instance is started, an array of operation threads is created. The size of this array 
has a default value of the number of cores divided by two with a minimum value of 2. It can be changed using the 
<code>hazelcast.operation.generic.thread.count</code> property. </p>
<p>A non-partition-aware operation thread does not execute an operation for a specific partition. Only partition-aware
  operation threads execute partition-aware operations. </p>
<p>Unlike the partition-aware operation threads, all the generic operation threads share the same work queue: <code>genericWorkQueue</code>.</p>
<p>If a non-partition-aware operation needs to be executed, it is placed in that work queue and any generic operation 
thread can execute it. The big advantage is that you automatically have work balancing since any generic operation 
thread is allowed to pick up work from this queue.</p>
<p>The disadvantage is that this shared queue can be a point of contention. You may not see this contention in 
production since performance is dominated by I/O and the system does not run many non-partition-aware operations.</p>
<a name="priority-operations"></a><h4 id="priority-operations">Priority Operations</h4>
<p>In some cases, the system needs to run operations with a higher priority, e.g. an important system operation. To support priority operations, Hazelcast has the following features:</p>
<ul>
<li><p>For partition-aware operations: Each partition thread has its own work queue and it also has a priority
work queue. The partition thread always checks the priority queue before it processes work from its normal work queue.</p>
</li>
<li><p>For non-partition-aware operations: Next to the <code>genericWorkQueue</code>, there is also a <code>genericPriorityWorkQueue</code>. When a priority operation
needs to be run, it is put in the <code>genericPriorityWorkQueue</code>. Like the partition-aware operation threads, a generic
operation thread first checks the <code>genericPriorityWorkQueue</code> for work. </p>
</li>
</ul>
<p>Since a worker thread blocks on the normal work queue (either partition specific or generic), a priority operation
may not be picked up because it is not put in the queue where it is blocking. Hazelcast always sends a &#39;kick the worker&#39; operation that<br>only triggers the worker to wake up and check the priority queue. </p>
<a name="operation-response-and-invocation-future"></a><h4 id="operation-response-and-invocation-future">Operation-response and Invocation-future</h4>
<p>When an Operation is invoked, a <code>Future</code> is returned. Please see the example code below.</p>
<pre><code class="lang-java">GetOperation operation = new GetOperation( mapName, key );
Future future = operationService.invoke( operation );
future.get();
</code></pre>
<p>The calling side blocks for a reply. In this case, <code>GetOperation</code> is set in the work queue for the partition of <code>key</code>, where
it eventually is executed. Upon execution, a response is returned and placed on the <code>genericWorkQueue</code> where it is executed by a 
&quot;generic operation thread&quot;. This thread signals the <code>future</code> and notifies the blocked thread that a response is available. 
Hazelcast has a plan of exposing this <code>future</code> to the outside world, and we will provide the ability to register a completion listener so you can perform asynchronous calls. </p>
<a name="local-calls"></a><h4 id="local-calls">Local Calls</h4>
<p>When a local partition-aware call is done, an operation is made and handed over to the work queue of the correct partition operation thread,
and a <code>future</code> is returned. When the calling thread calls <code>get</code> on that <code>future</code>, it acquires a lock and waits for the result 
to become available. When a response is calculated, the <code>future</code> is looked up and the waiting thread is notified.  </p>
<p>In the future, this will be optimized to reduce the amount of expensive systems calls, such as <code>lock.acquire()</code>/<code>notify()</code> and the expensive
interaction with the operation-queue. Probably, we will add support for a caller-runs mode, so that an operation is directly run on
the calling thread.</p>

<a name="slowoperationdetector"></a><h2 id="slowoperationdetector">SlowOperationDetector</h2>
<p>The <code>SlowOperationDetector</code> monitors the operation threads and collects information about all slow operations. An <code>Operation</code> is a task executed by a generic or partition thread (see <a href="#operation-threading">Operation Threading</a>). An operation is considered as slow when it takes more computation time than the configured threshold.</p>
<p>The <code>SlowOperationDetector</code> stores the fully qualified classname of the operation and its stacktrace as well as operation details, start time and duration of each slow invocation. All collected data is available in the <a href="#monitoring-members">Management Center</a>.</p>
<p>The <code>SlowOperationDetector</code> is configured via the following system properties.</p>
<ul>
<li><code>hazelcast.slow.operation.detector.enabled</code></li>
<li><code>hazelcast.slow.operation.detector.log.purge.interval.seconds</code></li>
<li><code>hazelcast.slow.operation.detector.log.retention.seconds</code></li>
<li><code>hazelcast.slow.operation.detector.stacktrace.logging.enabled</code></li>
<li><code>hazelcast.slow.operation.detector.threshold.millis</code></li>
</ul>
<p>Please refer to the <a href="#system-properties">System Properties section</a> for explanations of these properties.</p>
<a name="logging-of-slow-operations"></a><h3 id="logging-of-slow-operations">Logging of Slow Operations</h3>
<p>The detected slow operations are logged as warnings in the Hazelcast log files:</p>
<pre><code>WARN 2015-05-07 11:05:30,890 SlowOperationDetector: [127.0.0.1]:5701
  Slow operation detected: com.hazelcast.map.impl.operation.PutOperation
  Hint: You can enable the logging of stacktraces with the following config
  property: hazelcast.slow.operation.detector.stacktrace.logging.enabled
WARN 2015-05-07 11:05:30,891 SlowOperationDetector: [127.0.0.1]:5701
  Slow operation detected: com.hazelcast.map.impl.operation.PutOperation
  (2 invocations)
WARN 2015-05-07 11:05:30,892 SlowOperationDetector: [127.0.0.1]:5701
  Slow operation detected: com.hazelcast.map.impl.operation.PutOperation
  (3 invocations)
</code></pre><p>Stacktraces are always reported to the Management Center, but by default they are not printed to keep the log size small. If logging of stacktraces is enabled, the full stacktrace is printed every 100 invocations. All other invocations print a shortened version.</p>
<a name="purging-of-slow-operation-logs"></a><h3 id="purging-of-slow-operation-logs">Purging of Slow Operation Logs</h3>
<p>Since a Hazelcast cluster can run for a very long time, Hazelcast purges the slow operation logs periodically to prevent an OOME. You can configure the purge interval and the retention time for each invocation.</p>
<p>The purging removes each invocation whose retention time is exceeded. When all invocations are purged from a slow operation log, the log is deleted.</p>

<a name="hazelcast-simulator"></a><h1 id="hazelcast-simulator">Hazelcast Simulator</h1>
<p>Hazelcast Simulator is a production simulator used to test Hazelcast and Hazelcast-based applications in clustered environments. It also allows you to create your own tests and perform them on your Hazelcast clusters and applications that are deployed to cloud computing environments. In your tests, you can provide any property that can be specified on these environments (Amazon EC2, Google Compute Engine(GCE), or your own environment): properties such as hardware specifications, operating system, Java version, etc.</p>
<p>Please refer to the documentation on its own GitHub repository at <a href="https://github.com/hazelcast/hazelcast-simulator/blob/master/README.md">Hazelcast Simulator</a>.</p>

<a name="wan"></a><h1 id="wan">WAN</h1>
<font color="#3981DB"><strong>Hazelcast Enterprise</strong></font>

<p><br></br></p>
<a name="wan-replication"></a><h2 id="wan-replication">WAN Replication</h2>
<p>There could be cases where you need to synchronize multiple Hazelcast clusters to the same state. Hazelcast WAN Replication allows you to keep multiple Hazelcast clusters in sync by replicating their state over WAN environments such as the Internet.</p>
<p>Imagine you have different data centers in New York, London and Tokyo each running an independent Hazelcast cluster. Every cluster
would be operating at native speed in their own LAN (Local Area Network), but you also want some or all record sets in
these clusters to be replicated to each other: updates in the Tokyo cluster should also replicate to London and New York and updates
in the New York cluster are to be synchronized to the Tokyo and London clusters.</p>
<p>This chapter explains how you can replicate the state of your clusters over Wide Area Network (WAN) through Hazelcast WAN Replication.</p>
<p><br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>You can download the white paper <strong>Hazelcast on AWS: Best Practices for Deployment</strong> from
<a href="http://hazelcast.com/resources/hazelcast-on-aws-best-practices-for-deployment/" target="_blank">Hazelcast.com</a>.</em></p>

<a name="defining-wan-replication"></a><h3 id="defining-wan-replication">Defining WAN Replication</h3>
<p>Hazelcast supports two different operation modes of WAN Replication:</p>
<ul>
<li><p><strong>Active-Passive:</strong> This mode is mostly used for failover scenarios where you want to replicate an active cluster to one
or more passive clusters, for the purpose of maintaining a backup.</p>
</li>
<li><p><strong>Active-Active:</strong> Every cluster is equal, each cluster replicates to all other clusters. This is normally used to connect
different clients to different clusters for the sake of the shortest path between client and server.</p>
</li>
</ul>
<p>Below is an example of declarative configuration of WAN Replication from New York cluster to target the London cluster:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
...
  &lt;wan-replication name=&quot;my-wan-cluster-batch&quot;&gt;
      &lt;wan-publisher group-name=&quot;london&quot;&gt;
          &lt;class-name&gt;com.hazelcast.enterprise.wan.replication.WanBatchReplication&lt;/class-name&gt;
          &lt;queue-full-behavior&gt;THROW_EXCEPTION&lt;/queue-full-behavior&gt;
          &lt;queue-capacity&gt;1000&lt;/queue-capacity&gt;
          &lt;properties&gt;
              &lt;property name=&quot;batch.size&quot;&gt;500&lt;/property&gt;
              &lt;property name=&quot;batch.max.delay.millis&quot;&gt;1000&lt;/property&gt;
              &lt;property name=&quot;snapshot.enabled&quot;&gt;false&lt;/property&gt;
              &lt;property name=&quot;response.timeout.millis&quot;&gt;60000&lt;/property&gt;
              &lt;property name=&quot;ack.type&quot;&gt;ACK_ON_OPERATION_COMPLETE&lt;/property&gt;
              &lt;property name=&quot;endpoints&quot;&gt;10.3.5.1:5701, 10.3.5.2:5701&lt;/property&gt;
              &lt;property name=&quot;group.password&quot;&gt;london-pass&lt;/property&gt;
          &lt;/properties&gt;
      &lt;/wan-publisher&gt;
  &lt;/wan-replication&gt;
...
&lt;/hazelcast&gt;
</code></pre>
<p>Following are the definitions of configuration elements:</p>
<ul>
<li><code>name</code>: Name of your WAN Replication. This name is referenced in IMap or ICache configuration when you add WAN Replication for these data structures (using the element <wan-replication-ref> in the configuration of IMap or ICache).</li>
<li><code>group-name</code>: Configures target cluster&#39;s group name.</li>
<li><code>class-name</code>: Name of the class implementation for the WAN replication.</li>
<li><code>queue-full-behavior</code>: Policy to be applied when WAN Replication event queues are full.</li>
<li><code>queue-capacity</code>: Size of the queue of events. Its default value is 10000.</li>
<li><code>batch.size</code>: Maximum size of events that are sent to the target cluster in a single batch.</li>
<li><code>batch.max.delay.millis</code>: Maximum amount of time to be waited before sending a batch of events in case <code>batch.size</code> is not reached. </li>
<li><code>snapshot.enabled</code>: When set to <code>true</code>, only the latest events (based on key) are selected and sent in a batch.</li>
<li><code>response.timeout.millis</code>: Time in milliseconds to be waited for the acknowledgment of a sent WAN event to target cluster. </li>
<li><code>ack.type</code>: Acknowledgment type for each target cluster.</li>
<li><code>endpoints</code>: IP addresses of the cluster members for which the WAN replication is implemented.</li>
<li><code>group.password</code>: Configures target cluster&#39;s group password.</li>
</ul>
<p>And the following is the equivalent programmatic configuration snippet:</p>
<pre><code class="lang-java">Config config = new Config();

WanReplicationConfig wrConfig = new WanReplicationConfig();
WanPublisherConfig  publisherConfig = wrConfig.getWanPublisherConfig();

wrConfig.setName(&quot;my-wan-cluster-batch&quot;);
publisherConfig.setGroupName(&quot;london&quot;);
publisherConfig.setClassName(&quot;com.hazelcast.enterprise.wan.replication.WanBatchReplication&quot;);

Map&lt;String, Comparable&gt; props = publisherConfig.getProperties();
props.put(&quot;group.password&quot;, &quot;london-pass&quot;);
props.put(&quot;snapshot.enabled&quot;, false);
props.put(&quot;endpoints&quot;, &quot;10.3.5.1:5701,10.3.5.2:5701&quot;); 
config.addWanReplicationConfig(wrConfig);
</code></pre>
<p>Using this configuration, the cluster running in New York will replicate to Tokyo and London. The Tokyo and London clusters should
have similar configurations if you want to run in Active-Active mode.</p>
<p>If the New York and London cluster configurations contain the <code>wan-replication</code> element and the Tokyo cluster does not, it means
New York and London are active endpoints and Tokyo is a passive endpoint.</p>
<a name="wanbatchreplication-implementation"></a><h3 id="wanbatchreplication-implementation">WanBatchReplication Implementation</h3>
<p>Hazelcast offers <code>WanBatchReplication</code> implementation for the WAN replication.</p>
<p>As you see in the above configuration examples, this implementation is configured using the <code>class-name</code> element (in the declarative configuration) or the method <code>setClassName</code> (in the programmatic configuration).</p>
<p>The implementation <code>WanBatchReplication</code> waits until:</p>
<ul>
<li>a pre-defined number of replication events are generated, (please refer to the <a href="#batch-size">Batch Size section</a>).</li>
<li>or a pre-defined amount of time is passed (please refer to the <a href="#batch-maximum-delay">Batch Maximum Delay section</a>).</li>
</ul>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em><code>WanNoDelayReplication</code> implementation has been removed. You can still achieve this behavior by setting the batch size to <code>1</code> while configuring your WAN replication.</em>
<br></br></p>

<a name="configuring-wan-replication-for-imap-and-icache"></a><h3 id="configuring-wan-replication-for-imap-and-icache">Configuring WAN Replication for IMap and ICache</h3>
<p>Yon can configure the WAN replication for Hazelcast&#39;s IMap and ICache data structures. To enable WAN replication for an IMap or ICache instance, you can use the <code>wan-replication-ref</code> element. Each IMap and ICache instance can have different WAN replication configurations.</p>
<p><strong>Enabling WAN Replication for IMap:</strong></p>
<p>Imagine you have different distributed maps, however only one of those maps should be replicated to a target cluster. To achieve this, configure the map that you want
replicated by adding the <code>wan-replication-ref</code> element in the map configuration as shown below.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
    ...
  &lt;/wan-replication&gt;
  &lt;map name=&quot;my-shared-map&quot;&gt;
    &lt;wan-replication-ref name=&quot;my-wan-cluster&quot;&gt;
       &lt;merge-policy&gt;com.hazelcast.map.merge.PassThroughMergePolicy&lt;/merge-policy&gt;
       &lt;republishing-enabled&gt;false&lt;/republishing-enabled&gt;
    &lt;/wan-replication-ref&gt;
  &lt;/map&gt;
  ...
&lt;/hazelcast&gt;
</code></pre>
<p>The following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">Config config = new Config();

WanReplicationConfig wrConfig = new WanReplicationConfig();
WanTargetClusterConfig  wtcConfig = wrConfig.getWanTargetClusterConfig();

wrConfig.setName(&quot;my-wan-cluster&quot;);
...
config.addWanReplicationConfig(wrConfig);

WanReplicationRef wanRef = new WanReplicationRef();
wanRef.setName(&quot;my-wan-cluster&quot;);
wanRef.setMergePolicy(PassThroughMergePolicy.class.getName());
wanRef.setRepublishingEnabled(false);
config.getMapConfig(&quot;my-shared-map&quot;).setWanReplicationRef(wanRef);
</code></pre>
<p>You see that we have <code>my-shared-map</code> configured to replicate itself to the cluster targets defined in the earlier
<code>wan-replication</code> element.</p>
<p><code>wan-replication-ref</code> has the following elements;</p>
<ul>
<li><code>name</code>: Name of <code>wan-replication</code> configuration. IMap or ICache instance uses this <code>wan-replication</code> configuration. </li>
<li><code>merge-policy</code>: Resolve conflicts that are occurred when target cluster already has the replicated entry key.</li>
<li><code>republishing-enabled</code>: When enabled, an incoming event to a member is forwarded to target cluster of that member.</li>
</ul>
<p>When using Active-Active Replication, multiple clusters can simultaneously update the same entry in a distributed data structure.
You can configure a merge policy to resolve these potential conflicts, as shown in the above example configuration (using the <code>merge-policy</code> sub-element under the <code>wan-replication-ref</code> element).</p>
<p>Hazelcast provides the following merge policies for IMap:</p>
<ul>
<li><code>com.hazelcast.map.merge.PutIfAbsentMapMergePolicy</code>: Incoming entry merges from the source map to the target map if it does not exist in the target map.</li>
<li><code>com.hazelcast.map.merge.HigherHitsMapMergePolicy</code>: Incoming entry merges from the source map to the target map if the source entry has more hits than the target one.</li>
<li><code>com.hazelcast.map.merge.PassThroughMergePolicy</code>: Incoming entry merges from the source map to the target map unless the incoming entry is not null.</li>
<li><code>com.hazelcast.map.merge.LatestUpdateMapMergePolicy</code>: Incoming entry merges from the source map to the target map if the source entry has been updated more recently than the target entry. Please note that this merge policy can only be used when the clusters&#39; clocks are in sync.</li>
</ul>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>When using WAN replication, please note that only key based events are replicated to the target cluster. Operations like <code>clear</code>, <code>destroy</code> and <code>evictAll</code> are NOT replicated.</em></p>
<p><br></br></p>
<p><strong>Enabling WAN Replication for ICache:</strong></p>
<p>The following is a declarative configuration example for enabling WAN Replication for ICache:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
   ...
&lt;/wan-replication&gt;
&lt;cache name=&quot;my-shared-cache&quot;&gt;
   &lt;wan-replication-ref name=&quot;my-wan-cluster&quot;&gt;
      &lt;merge-policy&gt;com.hazelcast.cache.merge.PassThroughCacheMergePolicy&lt;/merge-policy&gt;
      &lt;republishing-enabled&gt;true&lt;/republishing-enabled&gt;
   &lt;/wan-replication-ref&gt;
&lt;/cache&gt;
</code></pre>
<p>The following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">Config config = new Config();

WanReplicationConfig wrConfig = new WanReplicationConfig();
WanTargetClusterConfig  wtcConfig = wrConfig.getWanTargetClusterConfig();

wrConfig.setName(&quot;my-wan-cluster&quot;);
...
config.addWanReplicationConfig(wrConfig);

WanReplicationRef cacheWanRef = new WanReplicationRef();
cacheWanRef.setName(&quot;my-wan-cluster&quot;);
cacheWanRef.setMergePolicy(&quot;com.hazelcast.cache.merge.PassThroughCacheMergePolicy&quot;);
cacheWanRef.setRepublishingEnabled(true);
config.getCacheConfig(&quot;my-shared-cache&quot;).setWanReplicationRef(cacheWanRef);
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Caches that are created dynamically do not support WAN replication functionality. Cache configurations should be defined either declaratively (by XML) or programmatically on both source and target clusters.</em></p>
<p>Hazelcast provides the following merge policies for ICache:</p>
<ul>
<li><code>com.hazelcast.cache.merge.HigherHitsCacheMergePolicy</code>: Incoming entry merges from the source cache to the target cache if the source entry has more hits than the target one.</li>
<li><code>com.hazelcast.cache.merge.PassThroughCacheMergePolicy</code>: Incoming entry merges from the source cache to the target cache unless the incoming entry is not null.</li>
</ul>

<a name="batch-size"></a><h3 id="batch-size">Batch Size</h3>
<p>The maximum size of events that are sent in a single batch can be changed 
depending on your needs. Default value for batch size is <code>500</code>.</p>
<p>Batch size can be set for each target cluster by modifying related <code>WanPublisherConfig</code>.</p>
<p>Below is the declarative configuration for changing the value of the property:</p>
<pre><code class="lang-xml">...
 &lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
    &lt;wan-publisher group-name=&quot;london&quot;&gt;
        ...
        &lt;properties&gt;
            ...
            &lt;property name=&quot;batch.size&quot;&gt;1000&lt;/property&gt;
            ...
        &lt;/properties&gt;
        ...
    &lt;/wan-publisher&gt;
 &lt;/wan-replication&gt;
...
</code></pre>
<p>And, following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig(&quot;my-wan-cluster&quot;);
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 Map&lt;String, Comparable&gt; props = publisherConfig.getProperties();
 props.put(&quot;batch.size&quot;, 1000);
 wanConfig.addWanPublisherConfig(publisherConfig)
...
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"><strong><em>NOTE:</em></strong> <em><code>WanNoDelayReplication</code> implementation has been removed. You can still achieve this behavior by setting the batch size to <code>1</code> while configuring your WAN replication.</em>
<br></br></p>

<a name="batch-maximum-delay"></a><h3 id="batch-maximum-delay">Batch Maximum Delay</h3>
<p>When using <code>WanBatchReplication</code> if the number of WAN replication events generated does not reach <a href="#batch-size">Batch Size</a>,
they are sent to the target cluster after a certain amount of time is passed. You can set this duration in milliseconds using this batch maximum delay configuration. Default value of for this duration is 1 second (1000 milliseconds).</p>
<p>Maximum delay can be set for each target cluster by modifying related <code>WanPublisherConfig</code>.</p>
<p>You can change this property using the declarative configuration as shown below.</p>
<pre><code class="lang-xml">...
 &lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
    &lt;wan-publisher group-name=&quot;london&quot;&gt;
        ...
        &lt;properties&gt;
            ...
            &lt;property name=&quot;batch.max.delay.millis&quot;&gt;2000&lt;/property&gt;
            ... 
        &lt;/properties&gt;
        ...
    &lt;/wan-publisher&gt;
 &lt;/wan-replication&gt;
...
</code></pre>
<p>And, the following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig(&quot;my-wan-cluster&quot;);
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 Map&lt;String, Comparable&gt; props = publisherConfig.getProperties();
 props.put(&quot;batch.max.delay.millis&quot;, 2000);
 wanConfig.addWanPublisherConfig(publisherConfig)
...
</code></pre>

<a name="response-timeout"></a><h3 id="response-timeout">Response Timeout</h3>
<p>After a replication event is sent to the target cluster, the source member waits for an acknowledgement of the delivery of the event to the target.
If the confirmation is not received inside a timeout duration window, the event is resent to the target cluster. Default value of this duration is <code>60000</code> milliseconds.</p>
<p>You can change this duration depending on your network latency for each target cluster by modifying related <code>WanTargetClusterConfig</code>.</p>
<p>Below is an example of declarative configuration:</p>
<pre><code class="lang-xml">...
 &lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
    &lt;target-cluster group-name=&quot;london&quot; group-password=&quot;london-pass&quot;&gt;
        ...
        &lt;response-timeout-millis&gt;70000&lt;/response-timeout-millis&gt;
        ...
    &lt;/target-cluster&gt;
 &lt;/wan-replication&gt;
...
</code></pre>
<p>And, the following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig(&quot;my-wan-cluster&quot;);
 WanTargetClusterConfig targetClusterConfig = new WanTargetClusterConfig();
 ...
 targetClusterConfig.setResponseTimeoutMillis(70000);
 wanConfig.addTargetClusterConfig(targetClusterConfig)
...
</code></pre>

<a name="queue-capacity"></a><h3 id="queue-capacity">Queue Capacity</h3>
<p>For huge clusters or high data mutation rates, you might need to increase the replication queue size. The default queue
size for replication queues is <code>10000</code>. This means, if you have heavy put/update/remove rates, you might exceed the queue size
so that the oldest, not yet replicated, updates might get lost. Note that a separate queue is used for each WAN Replication configured for IMap and ICache.</p>
<p>Queue capacity can be set for each target cluster by modifying related <code>WanPublisherConfig</code>.</p>
<p>You can change this property using the declarative configuration as shown below.</p>
<pre><code class="lang-xml">...
 &lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
    &lt;wan-publisher group-name=&quot;london&quot;&gt;
        ...
        &lt;queue-capacity&gt;15000&lt;/queue-capacity&gt;
        ...
    &lt;/target-cluster&gt;
 &lt;/wan-replication&gt;
...
</code></pre>
<p>And, the following is the equivalent programmatic configuration:</p>
<pre><code class="lang-java">...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig(&quot;my-wan-cluster&quot;);
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 publisherConfig.setQueueCapacity(15000);
 wanConfig.addWanPublisherConfig(publisherConfig)
...
</code></pre>

<a name="queue-full-behavior"></a><h3 id="queue-full-behavior">Queue Full Behavior</h3>
<p>In the previous Hazelcast releases, WAN replication was dropping the new events if WAN replication event queues are full.
This behavior is configurable starting with the release 3.6. </p>
<p>There are two different supported behaviors:</p>
<ul>
<li><code>DISCARD_AFTER_MUTATION</code>: If you select this option, the new WAN events generated by the member are dropped and not replicated to the target cluster
when the WAN event queues are full.   </li>
<li><code>THROW_EXCEPTION</code>: If you select this option, the WAN queue size is checked before each supported mutating operation (like <code>IMap#put</code>, <code>ICache#put</code>).
If one the queues of target cluster is full, <code>WANReplicationQueueFullException</code> is thrown and the operation is not allowed.</li>
</ul>
<p>The following is an example configuration:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
  &lt;wan-publisher group-name=&quot;test-cluster-1&quot;&gt;
    ...
    &lt;queue-full-behavior&gt;DISCARD_AFTER_MUTATION&lt;/queue-full-behavior&gt;
  &lt;/wan-publisher&gt;
&lt;/wan-replication&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>queue-full-behavior</code> configuration is optional. Its default value is <code>DISCARD_AFTER_MUTATION</code></em>.</p>

<a name="event-filtering-api"></a><h3 id="event-filtering-api">Event Filtering API</h3>
<p>Starting with 3.6, Enterprise WAN replication allows you to intercept WAN replication events before they are placed to
WAN event replication queues by providing a filtering API. Using this API, you can monitor WAN replication events of each data structure
separately.</p>
<p>You can attach filters to your data structures using  <code>filter</code> property of <code>wan-replication-ref</code> configuration inside <code>hazelcast.xml</code> as shown in the following example configuration. You can also configure it using programmatic configuration.</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;map name=&quot;testMap&quot;&gt;
    &lt;wan-replication-ref name=&quot;test&quot;&gt;
      ...
      &lt;filters&gt;
        &lt;filter-impl&gt;com.example.SampleFilter&lt;/filter-impl&gt;
        &lt;filter-impl&gt;com.example.SampleFilter2&lt;/filter-impl&gt;
      &lt;/filters&gt;
    &lt;/wan-replication-ref&gt;
  &lt;/map&gt;&quot;
&lt;/hazelcast&gt;
</code></pre>
<p>As shown in the above configuration, you can define more than one filter. Filters are called in the order that they are introduced.
A WAN replication event is only eligible to publish if it passes all the filters.</p>
<p>Map and Cache have different filter interfaces. These interfaces are shown below.</p>
<p><strong>For map</strong>:</p>
<pre><code class="lang-java">package com.hazelcast.map.wan.filter;
...

/**
 * Wan event filtering interface for {@link com.hazelcast.core.IMap}
 * based wan replication events
 *
 * @param &lt;K&gt; the type of the key
 * @param &lt;V&gt; the type of the value
 */
public interface MapWanEventFilter&lt;K, V&gt; {

    /**
     * This method decides whether this entry view is suitable to replicate
     * over WAN
     *
     * @param mapName
     * @param entryView
     * @return &lt;tt&gt;true&lt;/tt&gt; if WAN event is not eligible for replication
     */
    boolean filter(String mapName, EntryView&lt;K, V&gt; entryView, WanFilterEventType eventType);
}
</code></pre>
<p><strong>For cache</strong>:</p>
<pre><code class="lang-java">package com.hazelcast.cache.wan.filter;
...

/**
 * Wan event filtering interface for cache based wan replication events
 *
 * @param &lt;K&gt; the type of the key
 * @param &lt;V&gt; the type of the value
 */
public interface CacheWanEventFilter&lt;K, V&gt; {

    /**
     * This method decides whether this entry view is suitable to replicate
     * over WAN.
     *
     * @param entryView
     * @return &lt;tt&gt;true&lt;/tt&gt; if WAN event is not eligible for replication.
     */
    boolean filter(String cacheName, CacheEntryView&lt;K, V&gt; entryView, WanFilterEventType eventType);
}
</code></pre>
<p>The method <code>filter</code> takes three parameters:</p>
<ul>
<li><code>mapName</code>/<code>cacheName</code>: Name of the related data structure.</li>
<li><code>entryView</code>: <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/core/EntryView.java">EntryView</a> 
or <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/cache/CacheEntryView.java">CacheEntryView</a> depending on the data structure.</li>
<li><code>eventType</code>: Enum type - <code>UPDATED(1)</code> or <code>REMOVED(2)</code> - depending on the event.</li>
</ul>

<a name="acknowledgment-types"></a><h3 id="acknowledgment-types">Acknowledgment Types</h3>
<p>Starting with 3.6, WAN replication supports different acknowledgment (ACK) types for each target cluster group.
You can choose from 2 different ACK type depending on your consistency requirements. The following ACK types are supported:</p>
<ul>
<li><code>ACK_ON_RECEIPT</code>: Events that are received by target cluster that are considered as successful. This option does not guarantee that the received event is actually applied but it is faster.</li>
<li><code>ACK_ON_OPERATION_COMPLETE</code>: This option guarantees that the event is received by the target cluster and it is applied. It is more time consuming. But it is the best way if you have strong consistency requirements.</li>
</ul>
<p>Following is an example configuration:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
  &lt;wan-publisher group-name=&quot;test-cluster-1&quot;&gt;
    ...
    &lt;properties&gt;
        &lt;property name=&quot;ack.type&quot;&gt;ACK_ON_OPERATION_COMPLETE&lt;/property&gt;
    &lt;/properties&gt;
  &lt;/wan-publisher&gt;
&lt;/wan-replication&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em><code>ack.type</code> configuration is optional. Its default value is <code>ACK_ON_RECEIPT</code></em>.</p>

<a name="synchronizing-wan-target-cluster"></a><h3 id="synchronizing-wan-target-cluster">Synchronizing WAN Target Cluster</h3>
<p>Starting with 3.7 you can initiate a synchronization operation on an IMap for a specific target cluster. 
Synchronization operation sends all the data of an IMap to a target cluster to align the state of target IMap with source IMap.
Synchronization is useful if two remote clusters lost their synchronization due to WAN queue overflow or in restart scenarios.</p>
<p>Synchronization can be initiated through Hazelcast&#39;s REST API. Below is the URL for the REST call;</p>
<pre><code>http://member_ip:port/hazelcast/rest/wan/sync/map
</code></pre><p>You need to add parameters to the request in the following order separated by &quot;&amp;&quot;;</p>
<ul>
<li>Name of the WAN replication configuration</li>
<li>Target group name</li>
<li>Map name to be synchronized</li>
</ul>
<p>Assume that you have configured an IMap with a WAN replication configuration as follows:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;my-wan-cluster&quot;&gt;
      &lt;wan-publisher group-name=&quot;tokyo&quot;&gt;
          &lt;class-name&gt;com.hazelcast.enterprise.wan.replication.WanBatchReplication&lt;/class-name&gt;
            ...
      &lt;/wan-publisher&gt;
&lt;wan-replication&gt;
...
&lt;map name=&quot;my-map&quot;&gt;
    &lt;wan-replication-ref name=&quot;my-wan-cluster&quot;&gt;
       &lt;merge-policy&gt;com.hazelcast.map.merge.PassThroughMergePolicy&lt;/merge-policy&gt;
    &lt;/wan-replication-ref&gt;
&lt;/map&gt;
</code></pre>
<p>Then, a sample CURL command to initiate the synchronization for &quot;my-map&quot; would be as follows:</p>
<pre><code>curl -H &quot;Content-type: text/plain&quot; -X POST -d &quot;my-wan-cluster&amp;tokyo&amp;my-map&quot; --URL http://127.0.0.1:5701/hazelcast/rest/wan/sync/map
</code></pre><p><img src="images/NoteSmall.jpg" alt="Note"> <strong><em>NOTE:</em></strong> <em>WAN synchronization works with Hazelcast Enterprise edition, but for this release it does not work with Hazelcast Enterprise HD, i.e., when you use WAN with maps or caches having High-Density memory store.</em></p>

<a name="wan-replication-additional-information"></a><h3 id="wan-replication-additional-information">WAN Replication Additional Information</h3>
<p>Each cluster in WAN topology has to have a unique <code>group-name</code> property for a proper handling of forwarded events.</p>
<p>Starting with 3.6, WAN replication backs up its event queues to other members to prevent event loss in case of member failures.
WAN replication&#39;s backup mechanism depends on the related data structures&#39; backup operations. Note that, WAN replication is supported for IMap and ICache.
That means, as far as you set a backup count for your IMap or ICache instances, WAN replication events generated by these instances are also replicated.</p>
<p>There is no additional configuration to enable/disable WAN replication event backups.</p>

<a name="solace-integration"></a><h2 id="solace-integration">Solace Integration</h2>
<p><img src="images/Plugin_New.png" alt="Azure Plugin" height="22" width="84">
<br></br></p>
<p>This section explains how you can integrate Hazelcast&#39;s WAN replication with <a href="http://www.solacesystems.com/">Solace</a> messaging platform. With this integration, you can publish and consume WAN replication events to/from Solace appliances. </p>
<a name="installing-solace-jars"></a><h3 id="installing-solace-jars">Installing Solace JARs</h3>
<p>Since Solace libraries are not hosted in any Maven repository, you need to install them manually.
Change directory to &quot;solace-jars&quot; and run the following commands to install Solace libraries to your local Maven repo:</p>
<p><code>mvn install:install-file   -Dfile=sol-common-7.1.2.230.jar   -DgroupId=com.solacesystems   -DartifactId=sol-common   -Dversion=7.1.2.230   -Dpackaging=jar</code>
<code>mvn install:install-file   -Dfile=sol-jcsmp-7.1.2.230.jar   -DgroupId=com.solacesystems   -DartifactId=sol-jcsmp   -Dversion=7.1.2.230   -Dpackaging=jar</code></p>
<a name="enabling-integration"></a><h3 id="enabling-integration">Enabling Integration</h3>
<p>To publish and consume WAN replication events on Solace appliances, Hazelcast WAN replication has the following classes:</p>
<ul>
<li><code>SolaceWanPublisher</code></li>
<li><code>SolaceWanConsumer</code></li>
</ul>
<p>You can register these classes using the configuration elements <code>&lt;wan-publisher&gt;</code> and <code>&lt;wan-consumer&gt;</code> while configuring your WAN replication. </p>
<p>Please see the following sections for configuration details.</p>
<a name="configuring-publisher"></a><h4 id="configuring-publisher">Configuring Publisher</h4>
<p>Following is an example declarative configuration for the publisher side:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;AtoB&quot;&gt;
    &lt;wan-publisher group-name=&quot;clusterB&quot;&gt;
        &lt;class-name&gt;com.hazelcast.enterprise.wan.solace.SolaceWanPublisher&lt;/class-name&gt;
        &lt;properties&gt;
            &lt;property name=&quot;host&quot;&gt;192.168.2.66&lt;/property&gt;
            &lt;property name=&quot;vpn.name&quot;&gt;YOUR_VPN_NAME&lt;/property&gt;
            &lt;property name=&quot;username&quot;&gt;admin&lt;/property&gt;
            &lt;property name=&quot;password&quot;&gt;YOUR_PASSWORD&lt;/property&gt;
            &lt;property name=&quot;topic.base.name&quot;&gt;BaseTopic&lt;/property&gt;
            &lt;property name=&quot;queue.name&quot;&gt;Q/hz/clusterA&lt;/property&gt;
            &lt;property name=&quot;initial.queue.mapping.enabled&quot;&gt;true&lt;/property&gt;
        &lt;/properties&gt;
    &lt;/wan-publisher&gt;
&lt;/wan-replication&gt;
</code></pre>
<p>Descriptions of the properties are as follows:</p>
<ul>
<li><code>class-name</code>: Full class name of Solace WAN publisher, i.e., <code>com.hazelcast.enterprise.wan.solace.SolaceWanPublisher</code></li>
<li><code>host</code>: IP address of the Solace host machine. It can be in the format &quot;IP address:Port number&quot;. It is a mandatory property.</li>
<li><code>vpn.name</code>: Name of the Solace VPN. It is an optional property.</li>
<li><code>username</code>: Username for the Solace host. It is a mandatory property.</li>
<li><code>password</code>: Password for the Solace host. It is an optional property.</li>
<li><code>topic.base.name</code>: Base topic name to use while publishing events to Solace appliance. If not defined, the members will publish to topics using the format &quot;T/hz/[cluster-group-name]/partitionId&quot;. If defined, the format &quot;[topic.base.name]/partitionId&quot; will be used. It is an optional property.</li>
<li><code>queue.name</code>: Name of the queue to be used in topic-to-queue mapping. This property is only valid if the property <code>initial.queue.mapping.enabled</code> is set to &quot;true&quot;. Default queue name is &quot;Q/hz/[cluster-group-name].</li>
<li><code>initial.queue.mapping.enabled</code>: Decides if a default topic-to-queue mapping should be performed on the publisher side. When enabled, it tries to provide a queue, whose name is defined by the property <code>queue.name</code>, and all topics that are generated by this instance is mapped to this queue. When disabled, you should perform this mapping manually. It is enabled by default.</li>
</ul>
<a name="configuring-consumer"></a><h4 id="configuring-consumer">Configuring Consumer</h4>
<p>Following is an example declarative configuration for the consumer side:</p>
<pre><code class="lang-xml">&lt;wan-replication name=&quot;AtoB&quot;&gt;
    &lt;wan-consumer&gt;
        &lt;class-name&gt;com.hazelcast.enterprise.wan.solace.SolaceWanConsumer&lt;/class-name&gt;
        &lt;properties&gt;
            &lt;property name=&quot;host&quot;&gt;192.168.2.66&lt;/property&gt;
            &lt;property name=&quot;vpn.name&quot;&gt;YOUR_VPN_NAME&lt;/property&gt;
            &lt;property name=&quot;username&quot;&gt;admin&lt;/property&gt;
            &lt;property name=&quot;password&quot;&gt;YOUR_PASSWORD&lt;/property&gt;
            &lt;property name=&quot;queue.name&quot;&gt;Q/hz/clusterA&lt;/property&gt;
        &lt;/properties&gt;
    &lt;/wan-consumer&gt;
&lt;/wan-replication&gt;
</code></pre>
<p>Descriptions of the properties are as follows:</p>
<ul>
<li><code>class-name</code>: Full class name of Solace WAN consumer, i.e., <code>com.hazelcast.enterprise.wan.solace.SolaceWanConsumer</code></li>
<li><code>host</code>: IP address of the Solace host machine. It can be in the format &quot;IP address:Port number&quot;. It is a mandatory property.</li>
<li><code>vpn.name</code>: Name of the Solace VPN. It is an optional property.</li>
<li><code>username</code>: Username for the Solace host. It is a mandatory property.</li>
<li><code>password</code>: Password for the Solace host. It is an optional property.</li>
<li><code>queue.name</code>: Name of the queue to be polled by the consumer. It is a mandatory property.</li>
</ul>

<a name="osgi"></a><h1 id="osgi">OSGI</h1>
<p>This chapter explains how Hazelcast is supported on OSGI (Open Service Gateway Initiatives) environments.</p>
<a name="osgi-support"></a><h2 id="osgi-support">OSGI Support</h2>
<p>Hazelcast bundles provide OSGI services so that Hazelcast users can manage (create, access, shutdown) Hazelcast instances through these services on OSGI environments. When you enable the property <code>hazelcast.osgi.start</code> (default is disabled), when an Hazelcast OSGI service is activated, a default Hazelcast instance is created automatically.</p>
<p>Created Hazelcast instances can be served as an OSGI service that the other Hazelcast bundles can access. Registering created Hazelcast instances behavior is enabled by default; you can disable it using the property <code>hazelcast.osgi.register.disabled</code>.</p>
<p>Each Hazelcast bundle provides a different OSGI service. Their instances can be grouped (clustered) together to prevent possible compatibility issues between different Hazelcast versions/bundles. This grouping behavior is enabled by default and you disable it using the property <code>hazelcast.osgi.grouping.disabled</code>.</p>
<p>Hazelcast OSGI service&#39;s lifecycle (and the owned/created instances&#39;s lifecycles) is the same with the owner Hazelcast bundles. When the bundle is stopped (deactivated), the owned service and Hazelcast instances are also deactivated/shutdown and deregistered automatically. When the bundle is re-activated, its service is registered again.</p>
<p>The Hazelcast Enterprise JAR package is also an OSGI bundle like the Hazelcast Open Source JAR package.</p>
<a name="api"></a><h2 id="api">API</h2>
<p><strong><code>HazelcastOSGiService</code>:</strong> Contract point for Hazelcast services on top of OSGI. Registered to <code>org.osgi.framework.BundleContext</code> as the OSGI service so the other bundles can access and use Hazelcast on the OSGI environment through this service.</p>
<p><strong><code>HazelcastOSGiInstance</code>:</strong> Contract point for <code>HazelcastInstance</code> implementations based on OSGI service. <code>HazelcastOSGiService</code> provides proxy Hazelcast instances typed <code>HazelcastOSGiInstance</code> which is a subtype of <code>HazelcastInstance</code> and these instances delegate all calls to the underlying <code>HazelcastInstance</code>.</p>
<a name="configuring-hazelcast-osgi-support"></a><h2 id="configuring-hazelcast-osgi-support">Configuring Hazelcast OSGI Support</h2>
<p><code>HazelcastOSGiService</code> uses three configurations:</p>
<ul>
<li><strong><code>hazelcast.osgi.start</code>:</strong> If this property is enabled (it is disabled by default), when an <code>HazelcastOSGiService</code> is activated, a default Hazelcast instance is created automatically.
<br></br></li>
<li><strong><code>hazelcast.osgi.register.disabled</code>:</strong> If this property is disabled (it is disabled by default), when a Hazelcast instance is created by <code>HazelcastOSGiService</code>, the created <code>HazelcastOSGiInstance</code> is registered automatically as OSGI service with type of <code>HazelcastOSGiInstance</code> and it is deregistered automatically when the created <code>HazelcastOSGiInstance</code> is shutdown.
<br></br></li>
<li><strong><code>hazelcast.osgi.grouping.disabled</code>:</strong> If this property is disabled (it is disabled by default), every created <code>HazelcastOSGiInstance</code> is grouped as their owner <code>HazelcastOSGiService</code> and do not join each other unless no group name is specified in the <code>GroupConfig</code> of <code>Config</code>.</li>
</ul>
<a name="design"></a><h2 id="design">Design</h2>
<p><code>HazelcastOSGiService</code> is specific to each Hazelcast bundle. This means that every Hazelcast bundle has its own <code>HazelcastOSGiService</code> instance.</p>
<p>Every Hazelcast bundle registers its <code>HazelcastOSGiService</code> instances via Hazelcast Bundle Activator (<code>com.hazelcast.osgi.impl.Activator</code>) while they are being started, and it deregisters its <code>HazelcastOSGiService</code> instances while they are being stopped.</p>
<p>Each <code>HazelcastOSGiService</code> instance has a different service ID as the combination of Hazelcast version and artifact type (<code>OSS</code> or <code>EE</code>). Examples are <code>3.6#OSS</code>, <code>3.6#EE</code>, <code>3.7#OSS</code>, <code>3.7#EE</code>, etc.</p>
<p><code>HazelcastOSGiService</code> instance lifecycle is the same with the owner Hazelcast bundle. This means that when the owner bundle is deactivated, the owned <code>HazelcastOSGiService</code> instance is deactivated, and all active Hazelcast instances that are created and served by that <code>HazelcastOSGiService</code> instance are also shutdown and deregistered. When the Hazelcast bundle is re-activated, its <code>HazelcastOSGiService</code> instance is registered again as the OSGI service.</p>
<p><img src="images/Design.png" alt=""></p>
<a name="using-hazelcast-osgi-service"></a><h2 id="using-hazelcast-osgi-service">Using Hazelcast OSGI Service</h2>
<a name="getting-hazelcast-osgi-service-instances"></a><h3 id="getting-hazelcast-osgi-service-instances">Getting Hazelcast OSGI Service Instances</h3>
<p>You can access all <code>HazelcastOSGiService</code> instances through <code>org.osgi.framework.BundleContext</code> for each Hazelcast bundle as follows:</p>
<pre><code class="lang-java">for (ServiceReference serviceRef : context.getServiceReferences(HazelcastOSGiService.class.getName(), null)) {
    HazelcastOSGiService service = (HazelcastOSGiService) context.getService(serviceRef);
    String serviceId = service.getId();
    ...
}
</code></pre>
<a name="managing-and-using-hazelcast-instances"></a><h3 id="managing-and-using-hazelcast-instances">Managing and Using Hazelcast instances</h3>
<p>You can use <code>HazelcastOSGiService</code> instance to create and shutdown Hazelcast instances on OSGI environments. The created Hazelcast instances are <code>HazelcastOSGiInstance</code> typed (which is sub-type of <code>HazelcastInstance</code>) and are just proxies to the underlying Hazelcast instance. There are several methods in <code>HazelcastOSGiService</code> to use Hazelcast instances on OSGI environments as shown below.</p>
<pre><code class="lang-java">// Get the default Hazelcast instance owned by `hazelcastOsgiService`
// Returns null if `HAZELCAST_OSGI_START` is not enabled
HazelcastOSGiInstance defaultInstance = hazelcastOsgiService.getDefaultHazelcastInstance();


// Creates a new Hazelcast instance with default configurations as owned by `hazelcastOsgiService`
HazelcastOSGiInstance newInstance1 = hazelcastOsgiService.newHazelcastInstance();


// Creates a new Hazelcast instance with specified configuration as owned by `hazelcastOsgiService`
Config config = new Config();
config.setInstanceName(&quot;OSGI-Instance&quot;);
...
HazelcastOSGiInstance newInstance2 = hazelcastOsgiService.newHazelcastInstance(config);

// Gets the Hazelcast instance with the name `OSGI-Instance`, which is `newInstance2` created above
HazelcastOSGiInstance instance = hazelcastOsgiService.getHazelcastInstanceByName(&quot;OSGI-Instance&quot;);

// Shuts down the Hazelcast instance with name `OSGI-Instance`, which is `newInstance2`
hazelcastOsgiService.shutdownHazelcastInstance(instance);

// Print all active Hazelcast instances owned by `hazelcastOsgiService`
for (HazelcastOSGiInstance instance : hazelcastOsgiService.getAllHazelcastInstances()) {
    System.out.println(instance);
}

// Shuts down all Hazelcast instances owned by `hazelcastOsgiService`
hazelcastOsgiService.shutdownAll();
</code></pre>

<a name="extending-hazelcast"></a><h1 id="extending-hazelcast">Extending Hazelcast</h1>
<p>This chapter describes the different possibilities to extend Hazelcast with additional services or features.</p>

<a name="user-defined-services"></a><h2 id="user-defined-services">User Defined Services</h2>
<p>In the case of special/custom needs, you can use Hazelcast&#39;s SPI (Service Provider Interface) module to develop your own distributed data structures and services on top of Hazelcast. Hazelcast SPI is an internal, low-level API which is expected to change in each release except for the patch releases. Your structures and services evolve as the SPI changes. </p>
<p>Throughout this section, we create an example distributed counter that will be the guide to reveal the Hazelcast Services SPI usage.</p>
<p>Here is our counter.</p>
<pre><code class="lang-java">public interface Counter{
   int inc(int amount);
}
</code></pre>
<p>This counter will have the following features:</p>
<ul>
<li>It will be stored in Hazelcast. </li>
<li>Different cluster members can call it. </li>
<li>It will be scalable, meaning that the capacity for the number of counters scales with the number of cluster members.</li>
<li>It will be highly available, meaning that if a member hosting this counter goes down, a backup will be available on a different member.</li>
</ul>
<p>All these features are done with the steps below. Each step adds a new functionality to this counter.</p>
<ol>
<li>Create the class.</li>
<li>Enable the class.</li>
<li>Add properties.</li>
<li>Place a remote call.</li>
<li>Create the containers.</li>
<li>Enable partition migration.</li>
<li>Create the backups.</li>
</ol>

<a name="creating-the-service-class"></a><h3 id="creating-the-service-class">Creating the Service Class</h3>
<p>To have the counter as a functioning distributed object, we need a class. This class (named CounterService in the following example code) is the gateway between Hazelcast internals and the counter, allowing us to add features to the counter. The following example code creates the class <code>CounterService</code>. Its lifecycle is managed by Hazelcast. </p>
<p><code>CounterService</code> should implement the interface <code>com.hazelcast.spi.ManagedService</code> as shown below. The <code>com.hazelcast.spi.ManagedService</code> <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/ManagedService.java">source code is here</a>.</p>
<p><code>CounterService</code> implements the following methods. </p>
<ul>
<li><code>init</code>: This is called when <code>CounterService</code> is initialized. <code>NodeEngine</code> enables access to Hazelcast internals such as <code>HazelcastInstance</code> and <code>PartitionService</code>. Also, the object <code>Properties</code> will provide us with the ability to create our own properties.</li>
<li><code>shutdown</code>: This is called when <code>CounterService</code> is shutdown. It cleans up the resources.</li>
<li><code>reset</code>: This is called when cluster members face the Split-Brain issue. This occurs when disconnected members that have created their own cluster are merged back into the main cluster. Services can also implement the <code>SplitBrainHandleService</code> to indicate that they can take part in the merge process. For <code>CounterService</code> we are going to implement <code>reset</code> as a no-op.</li>
</ul>
<pre><code class="lang-java">import com.hazelcast.spi.ManagedService;
import com.hazelcast.spi.NodeEngine;

import java.util.Properties;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

public class CounterService implements ManagedService {
    private NodeEngine nodeEngine;

    @Override
    public void init( NodeEngine nodeEngine, Properties properties ) {
        System.out.println( &quot;CounterService.init&quot; );
        this.nodeEngine = nodeEngine;
    }

    @Override
    public void shutdown( boolean terminate ) {
        System.out.println( &quot;CounterService.shutdown&quot; );
    }

    @Override
    public void reset() {
    }

}
</code></pre>

<a name="enabling-the-service-class"></a><h3 id="enabling-the-service-class">Enabling the Service Class</h3>
<p>Now, we need to enable the class <code>CounterService</code>. The declarative way of doing this is shown below.</p>
<pre><code class="lang-xml">&lt;network&gt;
   &lt;join&gt;&lt;multicast enabled=&quot;true&quot;/&gt; &lt;/join&gt;
&lt;/network&gt;
&lt;services&gt;
   &lt;service enabled=&quot;true&quot;&gt;
      &lt;name&gt;CounterService&lt;/name&gt;
      &lt;class-name&gt;CounterService&lt;/class-name&gt;
   &lt;/service&gt;
&lt;/services&gt;
</code></pre>
<p>The <code>CounterService</code> is declared within the <code>services</code> configuration element. </p>
<ul>
<li>Set the <code>enabled</code> attribute to <code>true</code> to enable the service.</li>
<li>Set the <code>name</code> attribute to the name of the service. It should be a unique name (<code>CounterService</code> in our case) since it will be looked up when a remote call is made. Note that the value of this attribute will be sent at each request, and that a longer <code>name</code> value means more data (de)serialization. A good practice is to give an understandable name with the shortest possible length.</li>
<li>Set the <code>class-name</code> attribute to the class name of the service (<code>CounterService</code> in our case). The class should have a <em>no-arg</em> constructor. Otherwise, the object cannot be initialized.</li>
</ul>
<p>Note that multicast is enabled as the join mechanism. In the later sections for the <code>CounterService</code> example, we will see why.</p>

<a name="adding-properties-to-the-service"></a><h3 id="adding-properties-to-the-service">Adding Properties to the Service</h3>
<p>The <code>init</code> method for <code>CounterService</code> takes the <code>Properties</code> object as an argument. This means we can add properties to the service that are passed to the <code>init</code> method; see <a href="#creating-the-service-class">Creating the Service Class</a>. You can add properties declaratively as shown below. (You likely want to name your properties something other than someproperty.)</p>
<pre><code class="lang-xml">&lt;service enabled=&quot;true&quot;&gt;
   &lt;name&gt;CounterService&lt;/name&gt;
   &lt;class-name&gt;CounterService&lt;/class-name&gt;
   &lt;properties&gt; 
      &lt;someproperty&gt;10&lt;/someproperty&gt;
   &lt;/properties&gt;
&lt;/service&gt;
</code></pre>
<p>If you want to parse a more complex XML, you can use the interface <code>com.hazelcast.spi.ServiceConfigurationParser</code>. It gives you access to the XML DOM tree.</p>

<a name="starting-the-service"></a><h3 id="starting-the-service">Starting the Service</h3>
<p>Now, let&#39;s start a <code>HazelcastInstance</code> as shown below, which will start the <code>CounterService</code>.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;

public class Member {
    public static void main(String[] args) {
        Hazelcast.newHazelcastInstance();
    }
}
</code></pre>
<p>Once it starts, the <code>CounterService init</code> method prints the following output.</p>
<p><code>CounterService.init</code></p>
<p>Once the HazelcastInstance is shutdown (for example, with Ctrl+C), the <code>CounterService shutdown</code> method prints the following output.</p>
<p><code>CounterService.shutdown</code></p>

<a name="placing-a-remote-call-via-proxy"></a><h3 id="placing-a-remote-call-via-proxy">Placing a Remote Call via Proxy</h3>
<p>In the previous sections for the <code>CounterService</code> example, we started <code>CounterService</code> as part of a HazelcastInstance startup.</p>
<p>Now, let&#39;s connect the <code>Counter</code> interface to <code>CounterService</code> and perform a remote call to the cluster member hosting the counter data. Then, we will return a dummy result. </p>
<p>Remote calls are performed via a proxy in Hazelcast. Proxies expose the methods at the client side. Once a method is called, proxy creates an operation object, sends this object to the cluster member responsible from executing that operation, and then sends the result. </p>
<a name="making-counter-a-distributed-object"></a><h4 id="making-counter-a-distributed-object">Making Counter a Distributed Object</h4>
<p>First, we need to make the <code>Counter</code> interface a distributed object by extending the <code>DistributedObject</code> interface, as shown below.</p>
<pre><code class="lang-java">import com.hazelcast.core.DistributedObject;

public interface Counter extends DistributedObject {
    int inc(int amount);
}
</code></pre>
<a name="implementing-managedservice-and-remoteservice"></a><h4 id="implementing-managedservice-and-remoteservice">Implementing ManagedService and RemoteService</h4>
<p>Now, we need to make the <code>CounterService</code> class implement not only the <code>ManagedService</code> interface, but also the interface <code>com.hazelcast.spi.RemoteService</code>. This way, a client will be able to get a handle of a counter proxy. You can read the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/RemoteService.java">source code for RemoteService here</a>.</p>
<pre><code class="lang-java">import com.hazelcast.core.DistributedObject;
import com.hazelcast.spi.ManagedService;
import com.hazelcast.spi.NodeEngine;
import com.hazelcast.spi.RemoteService;

import java.util.Properties;

public class CounterService implements ManagedService, RemoteService {
    public static final String NAME = &quot;CounterService&quot;;

    private NodeEngine nodeEngine;

    @Override
    public DistributedObject createDistributedObject(String objectName) {
        return new CounterProxy(objectName, nodeEngine, this);
    }

    @Override
    public void destroyDistributedObject(String objectName) {
        // for the time being a no-op, but in the later examples this will be implemented
    }

    @Override
    public void init(NodeEngine nodeEngine, Properties properties) {
        this.nodeEngine = nodeEngine;
    }

    @Override
    public void shutdown(boolean terminate) {
    }

    @Override
    public void reset() {
    }
}
</code></pre>
<p>The <code>CounterProxy</code> returned by the method <code>createDistributedObject</code> is a local representation to (potentially) remote managed data and logic.
<br></br></p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Note that caching and removing the proxy instance are done outside of this service.</em>
<br></br></p>
<a name="implementing-counterproxy"></a><h4 id="implementing-counterproxy">Implementing CounterProxy</h4>
<p>Now, it is time to implement the <code>CounterProxy</code> as shown below. <code>CounterProxy</code> extends <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/AbstractDistributedObject.java">AbstractDistributedObject, source code here</a>. </p>
<pre><code class="lang-java">import com.hazelcast.spi.AbstractDistributedObject;
import com.hazelcast.spi.InvocationBuilder;
import com.hazelcast.spi.NodeEngine;
import com.hazelcast.util.ExceptionUtil;

import java.util.concurrent.Future;

public class CounterProxy extends AbstractDistributedObject&lt;CounterService&gt; implements Counter {
    private final String name;

    public CounterProxy(String name, NodeEngine nodeEngine, CounterService counterService) {
        super(nodeEngine, counterService);
        this.name = name;
    }

    @Override
    public String getServiceName() {
        return CounterService.NAME;
    }

    @Override
    public String getName() {
        return name;
    }

    @Override
    public int inc(int amount) {
        NodeEngine nodeEngine = getNodeEngine();
        IncOperation operation = new IncOperation(name, amount);
        int partitionId = nodeEngine.getPartitionService().getPartitionId(name);
        InvocationBuilder builder = nodeEngine.getOperationService()
                .createInvocationBuilder(CounterService.NAME, operation, partitionId);
        try {
            final Future&lt;Integer&gt; future = builder.invoke();
            return future.get();
        } catch (Exception e) {
            throw ExceptionUtil.rethrow(e);
        }
    }
}
</code></pre>
<p><code>CounterProxy</code> is a local representation of remote data/functionality. It does not include the counter state. Therefore, the method <code>inc</code> should be invoked on the cluster member hosting the real counter. You can invoke it using Hazelcast SPI; then it will send the operations to the correct member and return the results.</p>
<p>Let&#39;s dig deeper into the method <code>inc</code>.</p>
<ul>
<li>First, we create <code>IncOperation</code> with a given <code>name</code> and <code>amount</code>.</li>
<li>Then, we get the partition ID based on the <code>name</code>; by this way, all operations for a given name will result in the same partition ID.</li>
<li>Then, we create an <code>InvocationBuilder</code> where the connection between operation and partition is made.</li>
<li>Finally, we invoke the <code>InvocationBuilder</code> and wait for its result. This waiting is performed with a <code>future.get()</code>. In our case, timeout is not important. However, it is a good practice to use a timeout for a real system since operations should complete in a certain amount of time. </li>
</ul>
<a name="dealing-with-exceptions"></a><h4 id="dealing-with-exceptions">Dealing with Exceptions</h4>
<p>Hazelcast&#39;s <code>ExceptionUtil</code> is a good solution when it comes to dealing with execution exceptions. When the execution of the operation fails with an exception, an <code>ExecutionException</code> is thrown and handled with the method <code>ExceptionUtil.rethrow(Throwable)</code>. </p>
<p>If it is an <code>InterruptedException</code>, we have two options: either propagate the exception or just use the <code>ExceptionUtil.rethrow</code> for all exceptions. Please see the example code below.</p>
<pre><code class="lang-java">  try {
     final Future&lt;Integer&gt; future = invocation.invoke();
     return future.get();
  } catch(InterruptedException e){
     throw e;
  } catch(Exception e){
     throw ExceptionUtil.rethrow(e);
  }
</code></pre>
<a name="implementing-the-partitionawareoperation-interface"></a><h4 id="implementing-the-partitionawareoperation-interface">Implementing the PartitionAwareOperation Interface</h4>
<p>Now, let&#39;s write the <code>IncOperation</code>. It implements the <code>PartitionAwareOperation</code> interface, meaning that it will be executed on the partition that hosts the counter. See the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/PartitionAwareOperation.java">PartitionAwareOperation source code here</a>.</p>
<p>The method <code>run</code> does the actual execution. Since <code>IncOperation</code> will return a response, the method <code>returnsResponse</code> returns <code>true</code>. If your method is asynchronous and does not need to return a response, it is better to return <code>false</code> since it will be faster. The actual response is stored in the field <code>returnValue</code>; retrieve it with the method <code>getResponse</code>.</p>
<p>There are two more methods in this code: <code>writeInternal</code> and <code>readInternal</code>. Since <code>IncOperation</code> needs to be serialized, these two methods are overridden, and hence, <code>objectId</code> and <code>amount</code> are serialized and available when those operations are executed. </p>
<p>For the deserialization, note that the operation must have a <em>no-arg</em> constructor.</p>
<pre><code class="lang-java">import com.hazelcast.nio.ObjectDataInput;
import com.hazelcast.nio.ObjectDataOutput;
import com.hazelcast.spi.AbstractOperation;
import com.hazelcast.spi.PartitionAwareOperation;

import java.io.IOException;

class IncOperation extends AbstractOperation implements PartitionAwareOperation {
    private String objectId;
    private int amount, returnValue;

    // Important to have a no-arg constructor for deserialization
    public IncOperation() {
    }

    public IncOperation(String objectId, int amount) {
        this.amount = amount;
        this.objectId = objectId;
    }

    @Override
    public void run() throws Exception {
        System.out.println(&quot;Executing &quot; + objectId + &quot;.inc() on: &quot; + getNodeEngine().getThisAddress());
        returnValue = 0;
    }

    @Override
    public boolean returnsResponse() {
        return true;
    }

    @Override
    public Object getResponse() {
        return returnValue;
    }

    @Override
    protected void writeInternal(ObjectDataOutput out) throws IOException {
        super.writeInternal(out);
        out.writeUTF(objectId);
        out.writeInt(amount);
    }

    @Override
    protected void readInternal(ObjectDataInput in) throws IOException {
        super.readInternal(in);
        objectId = in.readUTF();
        amount = in.readInt();
    }
}
</code></pre>
<a name="running-the-code"></a><h4 id="running-the-code">Running the Code</h4>
<p>Now, let&#39;s run our code.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;

import java.util.UUID;

public class Member {
    public static void main(String[] args) {
        HazelcastInstance[] instances = new HazelcastInstance[2];
        for (int k = 0; k &lt; instances.length; k++)
            instances[k] = Hazelcast.newHazelcastInstance();

        Counter[] counters = new Counter[4];
        for (int k = 0; k &lt; counters.length; k++)
            counters[k] = instances[0].getDistributedObject(CounterService.NAME, k+&quot;counter&quot;);

        for (Counter counter : counters)
            System.out.println(counter.inc(1));

        System.out.println(&quot;Finished&quot;);
        System.exit(0);
    }
}
</code></pre>
<p>Once run, you will see the output as below.</p>
<p><code>Executing 0counter.inc() on: Address[192.168.1.103]:5702</code></p>
<p><code>0</code></p>
<p><code>Executing 1counter.inc() on: Address[192.168.1.103]:5702</code></p>
<p><code>0</code></p>
<p><code>Executing 2counter.inc() on: Address[192.168.1.103]:5701</code></p>
<p><code>0</code></p>
<p><code>Executing 3counter.inc() on: Address[192.168.1.103]:5701</code></p>
<p><code>0</code></p>
<p><code>Finished</code></p>
<p>Note that counters are stored in different cluster members. Also note that increment is not active for now since the value remains as <strong>0</strong>. </p>
<p>Until now, we have performed the basics to get this up and running. In the next section, we will make a real counter, cache the proxy instances and deal with proxy instance destruction.</p>

<a name="creating-containers"></a><h3 id="creating-containers">Creating Containers</h3>
<p>Let&#39;s create a Container for every partition in the system. This container will contain all counters and proxies.</p>
<pre><code class="lang-java">import java.util.HashMap;
import java.util.Map;

class Container {
    private final Map&lt;String, Integer&gt; values = new HashMap();

    int inc(String id, int amount) {
        Integer counter = values.get(id);
        if (counter == null) {
            counter = 0;
        }
        counter += amount;
        values.put(id, counter);
        return counter;
    }

    public void init(String objectName) {
        values.put(objectName,0);
    }

    public void destroy(String objectName) {
        values.remove(objectName);
    }

    ...
}
</code></pre>
<p>Hazelcast guarantees that a single thread will be active in a single partition. Therefore, when accessing a container, concurrency control will not be an issue. </p>
<p>The code in our example uses a <code>Container</code> instance per partition approach. With this approach, there will not be any mutable shared state between partitions. This approach also makes operations on partitions simpler since you do not need to filter out data that does not belong to a certain partition. </p>
<p>The code performs the tasks below.</p>
<ul>
<li>It creates a container for every partition with the method <code>init</code>.</li>
<li>It creates the proxy with the method <code>createDistributedObject</code>.</li>
<li>It removes the value of the object with the method <code>destroyDistributedObject</code>, otherwise we may get an OutOfMemory exception.</li>
</ul>
<a name="integrating-the-container-in-the-counterservice"></a><h4 id="integrating-the-container-in-the-counterservice">Integrating the Container in the CounterService</h4>
<p>Let&#39;s integrate the <code>Container</code> in the <code>CounterService</code>, as shown below.</p>
<pre><code class="lang-java">import com.hazelcast.spi.ManagedService;
import com.hazelcast.spi.NodeEngine;
import com.hazelcast.spi.RemoteService;

import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

public class CounterService implements ManagedService, RemoteService {
    public final static String NAME = &quot;CounterService&quot;;
    Container[] containers;
    private NodeEngine nodeEngine;

    @Override
    public void init(NodeEngine nodeEngine, Properties properties) {
        this.nodeEngine = nodeEngine;
        containers = new Container[nodeEngine.getPartitionService().getPartitionCount()];
        for (int k = 0; k &lt; containers.length; k++)
            containers[k] = new Container();
    }

    @Override
    public void shutdown(boolean terminate) {
    }

    @Override
    public CounterProxy createDistributedObject(String objectName) {
        int partitionId = nodeEngine.getPartitionService().getPartitionId(objectName);
        Container container = containers[partitionId];
        container.init(objectName);
        return new CounterProxy(objectName, nodeEngine, this);
    }

    @Override
    public void destroyDistributedObject(String objectName) {
        int partitionId = nodeEngine.getPartitionService().getPartitionId(objectName);
        Container container = containers[partitionId];
        container.destroy(objectName);
    }

    @Override
    public void reset() {
    }

    public static class Container {
        final Map&lt;String, Integer&gt; values = new HashMap&lt;String, Integer&gt;();

        private void init(String objectName) {
            values.put(objectName, 0);
        }

        private void destroy(String objectName){
            values.remove(objectName);
        }
    }
}
</code></pre>
<a name="connecting-the-incoperationrun-method-to-the-container"></a><h4 id="connecting-the-incoperation-run-method-to-the-container">Connecting the IncOperation.run Method to the Container</h4>
<p>As the last step in creating a Container, we connect the method <code>IncOperation.run</code> to the Container, as shown below.</p>
<p><code>partitionId</code> has a range between <strong>0</strong> and <strong>partitionCount</strong> and can be used as an index for the container array. Therefore, you can use <code>partitionId</code> to retrieve the container, and once the container has been retrieved, you can access the value. </p>
<pre><code class="lang-java">import com.hazelcast.nio.ObjectDataInput;
import com.hazelcast.nio.ObjectDataOutput;
import com.hazelcast.spi.AbstractOperation;
import com.hazelcast.spi.PartitionAwareOperation;

import java.io.IOException;
import java.util.Map;

class IncOperation extends AbstractOperation implements PartitionAwareOperation {
    private String objectId;
    private int amount, returnValue;

    public IncOperation() {
    }

    public IncOperation(String objectId, int amount) {
        this.amount = amount;
        this.objectId = objectId;
    }

    @Override
    public void run() throws Exception {
        System.out.println(&quot;Executing &quot; + objectId + &quot;.inc() on: &quot; + getNodeEngine().getThisAddress());
        CounterService service = getService();
        CounterService.Container container = service.containers[getPartitionId()];
        Map&lt;String, Integer&gt; valuesMap = container.values;

        Integer counter = valuesMap.get(objectId);
        counter += amount;
        valuesMap.put(objectId, counter);
        returnValue = counter;
    }

    @Override
    public boolean returnsResponse() {
        return true;
    }

    @Override
    public Object getResponse() {
        return returnValue;
    }

    @Override
    protected void writeInternal(ObjectDataOutput out) throws IOException {
        super.writeInternal(out);
        out.writeUTF(objectId);
        out.writeInt(amount);
    }

    @Override
    protected void readInternal(ObjectDataInput in) throws IOException {
        super.readInternal(in);
        objectId = in.readUTF();
        amount = in.readInt();
    }
}
</code></pre>
<a name="running-the-sample-code"></a><h4 id="running-the-sample-code">Running the Sample Code</h4>
<p>Let&#39;s run the following sample code.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;

public class Member {
    public static void main(String[] args) {
        HazelcastInstance[] instances = new HazelcastInstance[2];
        for (int k = 0; k &lt; instances.length; k++)
            instances[k] = Hazelcast.newHazelcastInstance();

        Counter[] counters = new Counter[4];
        for (int k = 0; k &lt; counters.length; k++)
            counters[k] = instances[0].getDistributedObject(CounterService.NAME, k+&quot;counter&quot;);

        System.out.println(&quot;Round 1&quot;);
        for (Counter counter: counters)
            System.out.println(counter.inc(1));

        System.out.println(&quot;Round 2&quot;);
        for (Counter counter: counters)
            System.out.println(counter.inc(1));

        System.out.println(&quot;Finished&quot;);
        System.exit(0);
    }
}
</code></pre>
<p>The output will be as follows. It indicates that we have now a basic distributed counter up and running.</p>
<pre><code>Round 1
Executing 0counter.inc() on: Address[192.168.1.103]:5702
1
Executing 1counter.inc() on: Address[192.168.1.103]:5702
1
Executing 2counter.inc() on: Address[192.168.1.103]:5701
1
Executing 3counter.inc() on: Address[192.168.1.103]:5701
1
Round 2
Executing 0counter.inc() on: Address[192.168.1.103]:5702
2
Executing 1counter.inc() on: Address[192.168.1.103]:5702
2
Executing 2counter.inc() on: Address[192.168.1.103]:5701
2
Executing 3counter.inc() on: Address[192.168.1.103]:5701
2
Finished
</code></pre>
<a name="partition-migration"></a><h3 id="partition-migration">Partition Migration</h3>
<p>In the previous section, we created a real distributed counter. Now, we need to make sure that the content of the partition containers is migrated to different cluster members when a member joins or leaves the cluster. To make this happen, first we need to add three new methods (<code>applyMigrationData</code>, <code>toMigrationData</code> and <code>clear</code>) to the <code>Container</code>.</p>
<ul>
<li><code>toMigrationData</code>: This method is called when Hazelcast wants to start the partition migration from the member owning the partition. The result of the <code>toMigrationData</code> method is the partition data in a form that can be serialized to another member.</li>
<li><code>applyMigrationData</code>: This method is called when <code>migrationData</code> (created by the method <code>toMigrationData</code>) will be applied to the member that will be the new partition owner.</li>
<li><code>clear</code>: This method is called when the partition migration is successfully completed and the old partition owner gets rid of all data in the partition. This method is also called when the partition migration operation fails and the to-be-the-new partition owner needs to roll back its changes.</li>
</ul>
<pre><code class="lang-java">import java.util.HashMap;
import java.util.Map;

class Container {
    private final Map&lt;String, Integer&gt; values = new HashMap();

    int inc(String id, int amount) {
        Integer counter = values.get(id);
        if (counter == null) {
            counter = 0;
        }
        counter += amount;
        values.put(id, counter);
        return counter;
    }

    void clear() {
        values.clear();
    }

    void applyMigrationData(Map&lt;String, Integer&gt; migrationData) {
        values.putAll(migrationData);
    }

    Map&lt;String, Integer&gt; toMigrationData() {
        return new HashMap(values);
    }

    public void init(String objectName) {
        values.put(objectName,0);
    }

    public void destroy(String objectName) {
        values.remove(objectName);
    }
}
</code></pre>
<a name="transferring-migrationdata"></a><h4 id="transferring-migrationdata">Transferring migrationData</h4>
<p>After you add these three methods to the <code>Container</code>, you need to create a <code>CounterMigrationOperation</code> class that transfers <code>migrationData</code> from one member to another and calls the method <code>applyMigrationData</code> on the correct partition of the new partition owner. </p>
<p>An example is shown below.</p>
<pre><code class="lang-java">import com.hazelcast.nio.ObjectDataInput;
import com.hazelcast.nio.ObjectDataOutput;
import com.hazelcast.spi.AbstractOperation;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

public class CounterMigrationOperation extends AbstractOperation {

    Map&lt;String, Integer&gt; migrationData;

    public CounterMigrationOperation() {
    }

    public CounterMigrationOperation(Map&lt;String, Integer&gt; migrationData) {
        this.migrationData = migrationData;
    }

    @Override
    public void run() throws Exception {
        CounterService service = getService();
        Container container = service.containers[getPartitionId()];
        container.applyMigrationData(migrationData);
    }

    @Override
    protected void writeInternal(ObjectDataOutput out) throws IOException {
        out.writeInt(migrationData.size());
        for (Map.Entry&lt;String, Integer&gt; entry : migrationData.entrySet()) {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue());
        }
    }

    @Override
    protected void readInternal(ObjectDataInput in) throws IOException {
        int size = in.readInt();
        migrationData = new HashMap&lt;String, Integer&gt;();
        for (int i = 0; i &lt; size; i++)
            migrationData.put(in.readUTF(), in.readInt());
    }
}
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>During a partition migration, no other operations are executed on the related partition.</em>
<br></br></p>
<a name="letting-hazelcast-know-counterservice-can-do-partition-migrations"></a><h4 id="letting-hazelcast-know-counterservice-can-do-partition-migrations">Letting Hazelcast Know CounterService Can Do Partition Migrations</h4>
<p>We need to make our <code>CounterService</code> class implement the <code>MigrationAwareService</code> interface. This will let Hazelcast know that the <code>CounterService</code> can perform partition migration.</p>
<p>With the <code>MigrationAwareService</code> interface, some additional methods are exposed. For example, the method <code>prepareMigrationOperation</code> returns all the data of the partition that is going to be moved. You can read the <a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/MigrationAwareService.java">MigrationAwareService source code here</a>.</p>
<p>The method <code>commitMigration</code> commits the data, meaning that in this case, it clears the partition container of the old owner. </p>
<pre><code class="lang-java">import com.hazelcast.core.DistributedObject;
import com.hazelcast.partition.MigrationEndpoint;
import com.hazelcast.spi.*;

import java.util.Map;
import java.util.Properties;

public class CounterService implements ManagedService, RemoteService, MigrationAwareService {
    public final static String NAME = &quot;CounterService&quot;;
    Container[] containers;
    private NodeEngine nodeEngine;

    @Override
    public void init(NodeEngine nodeEngine, Properties properties) {
        this.nodeEngine = nodeEngine;
        containers = new Container[nodeEngine.getPartitionService().getPartitionCount()];
        for (int k = 0; k &lt; containers.length; k++)
            containers[k] = new Container();
    }

    @Override
    public void shutdown(boolean terminate) {
    }

    @Override
    public DistributedObject createDistributedObject(String objectName) {
        int partitionId = nodeEngine.getPartitionService().getPartitionId(objectName);
        Container container = containers[partitionId];
        container.init(objectName);
        return new CounterProxy(objectName, nodeEngine,this);
    }

    @Override
    public void destroyDistributedObject(String objectName) {
        int partitionId = nodeEngine.getPartitionService().getPartitionId(objectName);
        Container container = containers[partitionId];
        container.destroy(objectName);
    }

    @Override
    public void beforeMigration(PartitionMigrationEvent e) {
        //no-op
    }

    @Override
    public void clearPartitionReplica(int partitionId) {
        Container container = containers[partitionId];
        container.clear();
    }

    @Override
    public Operation prepareReplicationOperation(PartitionReplicationEvent e) {
        if (e.getReplicaIndex() &gt; 1) {
            return null;
        }
        Container container = containers[e.getPartitionId()];
        Map&lt;String, Integer&gt; data = container.toMigrationData();
        return data.isEmpty() ? null : new CounterMigrationOperation(data);
    }

    @Override
    public void commitMigration(PartitionMigrationEvent e) {
        if (e.getMigrationEndpoint() == MigrationEndpoint.SOURCE) {
            Container c = containers[e.getPartitionId()];
            c.clear();
        }

        //todo
    }

    @Override
    public void rollbackMigration(PartitionMigrationEvent e) {
        if (e.getMigrationEndpoint() == MigrationEndpoint.DESTINATION) {
            Container c = containers[e.getPartitionId()];
            c.clear();
        }
    }

    @Override
    public void reset() {
    }
}
</code></pre>
<a name="running-the-sample-code"></a><h4 id="running-the-sample-code">Running the Sample Code</h4>
<p>We can run the following code.</p>
<pre><code class="lang-java">import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;

public class Member {
    public static void main(String[] args) throws Exception {
        HazelcastInstance[] instances = new HazelcastInstance[3];
        for (int k = 0; k &lt; instances.length; k++)
            instances[k] = Hazelcast.newHazelcastInstance();

        Counter[] counters = new Counter[4];
        for (int k = 0; k &lt; counters.length; k++)
            counters[k] = instances[0].getDistributedObject(CounterService.NAME, k + &quot;counter&quot;);

        for (Counter counter : counters)
            System.out.println(counter.inc(1));

        Thread.sleep(10000);

        System.out.println(&quot;Creating new members&quot;);

        for (int k = 0; k &lt; 3; k++) {
            Hazelcast.newHazelcastInstance();
        }

        Thread.sleep(10000);

        for (Counter counter : counters)
            System.out.println(counter.inc(1));

        System.out.println(&quot;Finished&quot;);
        System.exit(0);
    }
}
</code></pre>
<p>And we get the following output.</p>
<pre><code>Executing 0counter.inc() on: Address[192.168.1.103]:5702
Executing backup 0counter.inc() on: Address[192.168.1.103]:5703
1
Executing 1counter.inc() on: Address[192.168.1.103]:5703
Executing backup 1counter.inc() on: Address[192.168.1.103]:5701
1
Executing 2counter.inc() on: Address[192.168.1.103]:5701
Executing backup 2counter.inc() on: Address[192.168.1.103]:5703
1
Executing 3counter.inc() on: Address[192.168.1.103]:5701
Executing backup 3counter.inc() on: Address[192.168.1.103]:5703
1
Creating new members
Executing 0counter.inc() on: Address[192.168.1.103]:5705
Executing backup 0counter.inc() on: Address[192.168.1.103]:5703
2
Executing 1counter.inc() on: Address[192.168.1.103]:5703
Executing backup 1counter.inc() on: Address[192.168.1.103]:5704
2
Executing 2counter.inc() on: Address[192.168.1.103]:5705
Executing backup 2counter.inc() on: Address[192.168.1.103]:5704
2
Executing 3counter.inc() on: Address[192.168.1.103]:5704
Executing backup 3counter.inc() on: Address[192.168.1.103]:5705
2
Finished
</code></pre><p>You can see that the counters have moved. <code>0counter</code> moved from <em>192.168.1.103:5702</em> to <em>192.168.1.103:5705</em> and it is incremented correctly. Our counters can now move around in the cluster. You will see the counters will be redistributed once you add or remove a cluster member.</p>

<a name="creating-backups"></a><h3 id="creating-backups">Creating Backups</h3>
<p>Finally, we make sure that the counter data is available on another member when a member goes down. To do this, have the <code>IncOperation</code> class implement the <code>BackupAwareOperation</code> interface contained in the SPI package. See the following code.</p>
<pre><code class="lang-java">class IncOperation extends AbstractOperation 
    implements PartitionAwareOperation, BackupAwareOperation {
   ...   

   @Override
   public int getAsyncBackupCount() {
      return 0;
   }

   @Override
   public int getSyncBackupCount() {
      return 1;
   }

   @Override
   public boolean shouldBackup() {
      return true;
   }

   @Override
   public Operation getBackupOperation() {
      return new IncBackupOperation(objectId, amount);
   }
}
</code></pre>
<p>The methods <code>getAsyncBackupCount</code> and <code>getSyncBackupCount</code> specify the count for asynchronous and synchronous backups. Our sample has one synchronous backup and no asynchronous backups. In the above code, counts of the backups are hard-coded, but they can also be passed to <code>IncOperation</code> as parameters. </p>
<p>The method <code>shouldBackup</code> specifies whether our Operation needs a backup or not. For our sample, it returns <code>true</code>, meaning the Operation will always have a backup even if there are no changes. Of course, in real systems, we want to have backups if there is a change. For <code>IncOperation</code> for example, having a backup when <code>amount</code> is null would be a good practice.</p>
<p>The method <code>getBackupOperation</code> returns the operation (<code>IncBackupOperation</code>) that actually performs the backup creation; the backup itself is an operation and will run on the same infrastructure. </p>
<p>If a backup should be made and <code>getSyncBackupCount</code> returns <strong>3</strong>, then three <code>IncBackupOperation</code> instances are created and sent to the three machines containing the backup partition. If fewer machines are available, then backups need to be created. Hazelcast will just send a smaller number of operations. </p>
<a name="performing-the-backup-with-incbackupoperation"></a><h4 id="performing-the-backup-with-incbackupoperation">Performing the Backup with IncBackupOperation</h4>
<p>Now, let&#39;s have a look at the <code>IncBackupOperation</code>. It implements <code>BackupOperation</code>, you can see the
<a href="https://github.com/hazelcast/hazelcast/blob/master/hazelcast/src/main/java/com/hazelcast/spi/BackupOperation.java">source code for BackupOperation here</a>.</p>
<pre><code class="lang-java">public class IncBackupOperation 
    extends AbstractOperation implements BackupOperation {
   private String objectId;
   private int amount;

   public IncBackupOperation() {
   }

   public IncBackupOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;
   }

   @Override
   protected void writeInternal(ObjectDataOutput out) throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }

   @Override
   protected void readInternal(ObjectDataInput in) throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }

   @Override
   public void run() throws Exception {
      CounterService service = getService();
      System.out.println(&quot;Executing backup &quot; + objectId + &quot;.inc() on: &quot; 
        + getNodeEngine().getThisAddress());
      Container c = service.containers[getPartitionId()];
      c.inc(objectId, amount);
   }
}
</code></pre>
<p><br></br>
<img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Hazelcast will also make sure that a new IncOperation for that particular key will not be executed before the (synchronous) backup operation has completed.</em>
<br></br></p>
<a name="running-the-sample-code"></a><h4 id="running-the-sample-code">Running the Sample Code</h4>
<p>Let&#39;s see the backup functionality in action with the following code.</p>
<pre><code class="lang-java">public class Member {
   public static void main(String[] args) throws Exception {
      HazelcastInstance[] instances = new HazelcastInstance[2];
      for (int k = 0; k &lt; instances.length; k++) 
         instances[k] = Hazelcast.newHazelcastInstance();

      Counter counter = instances[0].getDistributedObject(CounterService.NAME, &quot;counter&quot;);
      counter.inc(1);
      System.out.println(&quot;Finished&quot;);
      System.exit(0);
    }
}
</code></pre>
<p>Once it is run, the following output will be seen.</p>
<pre><code>Executing counter0.inc() on: Address[192.168.1.103]:5702
Executing backup counter0.inc() on: Address[192.168.1.103]:5701
Finished
</code></pre><p>As it can be seen, both <code>IncOperation</code> and <code>IncBackupOperation</code> are executed. Notice that these operations have been executed on different cluster members to guarantee high availability.</p>

<a name="waitnotifyservice"></a><h2 id="waitnotifyservice">WaitNotifyService</h2>
<p><code>WaitNotifyService</code> is an interface offered by SPI for the objects (e.g. Lock, Semaphore) to be used when a thread needs to wait for a lock to be released. You can see the <a href="https://github.com/hazelcast/hazelcast/tree/master/hazelcast/src/main/java/com/hazelcast/spi/impl/waitnotifyservice">WaitNotifyService source code here</a>.</p>
<p><code>WaitNotifyService</code> keeps a list of waiters. For each notify operation:</p>
<ul>
<li>it looks for a waiter,</li>
<li>it asks the waiter whether it wants to keep waiting,</li>
<li>if the waiter responds <em>no</em>, the service executes its registered operation (operation itself knows where to send a response),</li>
<li>it rinses and repeats until a waiter wants to keep waiting.</li>
</ul>
<p>Each waiter can sit on a wait-notify queue for, at most, its operation&#39;s call timeout. For example, by default, each waiter can wait here for at most 1 minute. A continuous task scans expired/timed-out waiters and invalidates them with <code>CallTimeoutException</code>. Each waiter on the remote side should retry and keep waiting if it still wants to wait. This is a liveness check for remote waiters. </p>
<p>This way, it is possible to distinguish an unresponsive member and a long (~infinite) wait. On the caller side, if the waiting thread does not get a response for either a call timeout or for more than <em>2 times the call-timeout</em>, it will exit with <code>OperationTimeoutException</code>.  </p>
<p>Note that this behavior breaks the fairness. Hazelcast does not support fairness for any of the data structures with blocking operations (i.e. lock and semaphore).</p>

<a name="discovery-spi"></a><h2 id="discovery-spi">Discovery SPI</h2>
<p>By default, Hazelcast is bundled with multiple ways to define and find other members in the same network. Commonly used, especially with development, is the Multicast discovery. This sends out a multicast request to a network segment and awaits other members to answer with their IP addresses. In addition, Hazelcast supports fixed IP addresses: <a href="https://jclouds.apache.org/reference/providers/">JClouds</a> or <a href="https://aws.amazon.com/de/ec2/">AWS (Amazon EC2)</a> based discoveries.</p>
<p>Since there is an ever growing number of public and private cloud environments, as well as numerous Service Discovery systems in the wild, Hazelcast provides cloud or service discovery vendors with the option to implement their own discovery strategy.</p>
<p>Over the course of this section, we will build a simple discovery strategy based on the <code>/etc/hosts</code> file.</p>

<a name="discovery-spi-interfaces-and-classes"></a><h3 id="discovery-spi-interfaces-and-classes">Discovery SPI Interfaces and Classes</h3>
<p>The Hazelcast Discovery SPI (Member Discovery Extensions) consists of multiple interfaces and abstract classes. In the following sub-sections, we will have a quick look at all of them and shortly introduce the idea and usage behind them. The example will follow in the next section, <a href="#discovery-strategy">Discovery Strategy</a>.</p>
<a name="discoverystrategy-implement"></a><h4 id="discoverystrategy-implement">DiscoveryStrategy: Implement</h4>
<p>The <code>com.hazelcast.spi.discovery.DiscoveryStrategy</code> interface is the main entry point for vendors to implement their corresponding member discovery strategies. Its main purpose is to return discovered members on request. The <code>com.hazelcast.spi.discovery.DiscoveryStrategy</code> interface also offers light lifecycle capabilities for setup and teardown logic (for example, opening or closing sockets or REST API clients).</p>
<p><code>DiscoveryStrategy</code>s can also do automatic registration / de-registration on service discovery systems if necessary. You can use the provided <code>DiscoveryNode</code> that is passed to the factory method to retrieve local addresses and ports, as well as metadata.</p>
<a name="abstractdiscoverystrategy-abstract-class"></a><h4 id="abstractdiscoverystrategy-abstract-class">AbstractDiscoveryStrategy: Abstract Class</h4>
<p>The <code>com.hazelcast.spi.discovery.AbstractDiscoveryStrategy</code> is a convenience abstract class meant to ease the implementation of strategies. It basically provides additional support for reading / resolving configuration properties and empty implementations of lifecycle methods if unnecessary. </p>
<a name="discoverystrategyfactory-factory-contract"></a><h4 id="discoverystrategyfactory-factory-contract">DiscoveryStrategyFactory: Factory Contract</h4>
<p>The <code>com.hazelcast.spi.discovery.DiscoveryStrategyFactory</code> interface describes the factory contract that creates a certain <code>DiscoveryStrategy</code>. <code>DiscoveryStrategyFactory</code>s are registered automatically at startup of a Hazelcast member or client whenever they are found in the classpath. For automatic discovery, factories need to announce themselves as SPI services using a resource file according to the <a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html">Java Service Provider Interface</a>. The service registration file must be part of the JAR file, located under <code>META-INF/services/com.hazelcast.spi.discovery.DiscoveryStrategyFactory</code>, and consist of a line with the full canonical class name of the <code>DiscoveryStrategy</code> per provided strategy implementation.</p>
<a name="discoverynode-describe-a-member"></a><h4 id="discoverynode-describe-a-member">DiscoveryNode: Describe a Member</h4>
<p>The <code>com.hazelcast.spi.discovery.DiscoveryNode</code> abstract class describes a member in the Discovery SPI. It is used for multiple purposes, since it will be returned from strategies for discovered members. It is also passed to <code>DiscoveryStrategyFactory</code>s factory method to define the local member itself if created on a Hazelcast member; on Hazelcast clients, null will be passed.</p>
<a name="simplediscoverynode-default-discoverynode"></a><h4 id="simplediscoverynode-default-discoverynode">SimpleDiscoveryNode: Default DiscoveryNode</h4>
<p><code>com.hazelcast.spi.discovery.SimpleDiscoveryNode</code> is a default implementation of the <code>DiscoveryNode</code>. It is meant for convenience use of the Discovery SPI and can be returned from vendor implementations if no special needs are required. </p>
<a name="nodefilter-filter-members"></a><h4 id="nodefilter-filter-members">NodeFilter: Filter Members</h4>
<p>You can configure <code>com.hazelcast.spi.discovery.NodeFilter</code> before startup and you can implement logic to do additional filtering of members. This might be necessary if query languages for discovery strategies are not expressive enough to describe members or to overcome inefficiencies of strategy implementations.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>The <code>DiscoveryStrategy</code> vendor does not need to take possibly configured filters into account as their use is transparent to the strategies.</em></p>
<a name="discoveryservice-support-in-integrator-systems"></a><h4 id="discoveryservice-support-in-integrator-systems">DiscoveryService: Support In Integrator Systems</h4>
<p>A <code>com.hazelcast.spi.discovery.integration.DiscoveryService</code> is part of the integration domain. <code>DiscoveryStrategy</code> vendors do not need to implement <code>DiscoveryService</code> because it is meant to support the Discovery SPI in situations where vendors integrate Hazelcast into their own systems or frameworks. Certain needs might be necessary as part of the classloading or <a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html">Java Service Provider Interface</a> lookup.</p>
<a name="discoveryserviceprovider-provide-a-discoveryservice"></a><h4 id="discoveryserviceprovider-provide-a-discoveryservice">DiscoveryServiceProvider: Provide a DiscoveryService</h4>
<p>Use the <code>com.hazelcast.spi.discovery.integration.DiscoveryServiceProvider</code> to provide a <code>DiscoveryService</code> to the Hazelcast discovery subsystem. Configure the provider with the Hazelcast configuration API.</p>
<a name="discoveryservicesettings-configure-discoveryservice"></a><h4 id="discoveryservicesettings-configure-discoveryservice">DiscoveryServiceSettings: Configure DiscoveryService</h4>
<p>A <code>com.hazelcast.spi.discovery.integration.DiscoveryServiceSettings</code> instance is passed to the <code>DiscoveryServiceProvider</code> at creation time to configure the <code>DiscoveryService</code>.</p>
<a name="discoverymode-member-or-client"></a><h4 id="discoverymode-member-or-client">DiscoveryMode: Member or Client</h4>
<p>The <code>com.hazelcast.spi.discovery.integration.DiscoveryMode</code> enum tells if a created <code>DiscoveryService</code> is running on a Hazelcast member or client, and to change behavior accordingly.   </p>

<a name="discovery-strategy"></a><h3 id="discovery-strategy">Discovery Strategy</h3>
<p>This sub-section will walk through the implementation of a simple <code>DiscoveryStrategy</code> and their necessary setup.</p>
<a name="discovery-strategy-example"></a><h4 id="discovery-strategy-example">Discovery Strategy Example</h4>
<p>The example strategy will use the local <code>/etc/hosts</code> (and on Windows it will use the equivalent to the *nix hosts file named <code>%SystemRoot%\system32\drivers\etc\hosts</code>) to lookup IP addresses of different hosts. The strategy implementation expects hosts to be configured with hostname sub-groups under the same domain. So far to theory, let&#39;s get into it.</p>
<p>The full example&#39;s source code can be found in the <a href="https://github.com/hazelcast/hazelcast-code-samples">Hazelcast examples repository</a>. </p>
<a name="configuring-site-domain"></a><h4 id="configuring-site-domain">Configuring Site Domain</h4>
<p>As a first step we do some basic configuration setup. We want the user to be able to configure the site domain for the discovery inside the hosts file, therefore we define a configuration property called <code>site-domain</code>. The configuration is not optional: it must be configured before the creation of the <code>HazelcastInstance</code>, either via XML or the Hazelcast Config API.</p>
<p>It is recommended that you keep all defined properties in a separate configuration class as public constants (public final static) with sufficient documentation. This allows users to easily look up possible configuration values.</p>
<pre><code class="lang-java">package com.hazelcast.examples.spi.discovery;

import com.hazelcast...;

public class HostsDiscoveryConfiguration {
  /**
   * &#39;site-domain&#39; configures the basic site domain for the lookup, to
   * find other sub-domains of the cluster members and retrieve their assigned
   * IP addresses.
   */
  public static final PropertyDefinition DOMAIN = new SimplePropertyDefinition(
    &quot;site-domain&quot;, PropertyTypeConverter.STRING
  );

  // Prevent instantiation
  private HostsDiscoveryConfiguration() {}
}
</code></pre>
<p>An additional <code>ValueValidator</code> could be passed to the definition to make sure the configured value looks like a domain or has a special format.</p>
<a name="creating-discovery"></a><h4 id="creating-discovery">Creating Discovery</h4>
<p>As the second step we create the very simple <code>DiscoveryStrategyFactory</code> implementation class. To keep things clear we are going to name the discovery strategy after its purpose: looking into the hosts file.</p>
<pre><code class="lang-java">package com.hazelcast.examples.spi.discovery;

import com.hazelcast...;

public class HostsDiscoveryStrategyFactory
    implements DiscoveryStrategyFactory {

  private static final Collection&lt;PropertyDefinition&gt; PROPERTIES =
      Collections.singletonList( HostsDiscoveryConfiguration.SITE_DOMAIN );

  public Class&lt;? extends DiscoveryStrategy&gt; getDiscoveryStrategyType() {
    // Returns the actual class type of the DiscoveryStrategy
    // implementation, to match it against the configuration
    return HostsDiscoveryStrategy.class;
  }

  public Collection&lt;PropertyDefinition&gt; getConfigurationProperties() {
    return PROPERTIES;
  }

  public DiscoveryStrategy newDiscoveryStrategy( DiscoveryNode discoveryNode,
                                          ILogger logger,
                                          Map&lt;String, Comparable&gt; properties ) {

    return new HostsDiscoveryStrategy( logger, properties );                                      
  }   
}
</code></pre>
<p>This factory now defines properties known to the discovery strategy implementation and provides a clean way to instantiate it. While creating the <code>HostsDiscoveryStrategy</code> we ignore the passed <code>DiscoveryNode</code> since this strategy will not support automatic registration of new nodes. In cases where the strategy does not support registration, the environment has to handle this in some provided way.</p>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Remember that, when created on a Hazelcast client, the provided <code>DiscoveryNode</code> will be null, as there is no local member in existence.</em></p>
<p>Next, we register the <code>DiscoveryStrategyFactory</code> to make Hazelcast pick it up automatically at startup. As described earlier, this is done according to the <a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html">Java Service Provider Interface</a> specification. The filename is the name of the interface itself. Therefore we create a new resource file called <code>com.hazelcast.spi.discovery.DiscoveryStrategyFactory</code> and place it under <code>META-INF/services</code>. The content is the full canonical class name of our factory implementation. </p>
<pre><code class="lang-plain">com.hazelcast.examples.spi.discovery.HostsDiscoveryStrategyFactory
</code></pre>
<p>If our JAR file will contain multiple factories, each consecutive line can define another full canonical <code>DiscoveryStrategyFactory</code> implementation class name.</p>
<a name="implementing-discovery-strategy"></a><h4 id="implementing-discovery-strategy">Implementing Discovery Strategy</h4>
<p>Now comes the interesting part. We are going to implement the discovery itself. The previous parts we did are normally pretty similar for all strategies aside from the configuration properties itself. However, implementing the discovery heavily depends on the way the strategy has to come up with IP addresses of other Hazelcast members.</p>
<a name="extending-the-abstractdiscoverystrategy"></a><h4 id="extending-the-abstractdiscoverystrategy-">Extending The <code>AbstractDiscoveryStrategy</code></h4>
<p>For ease of implementation, we will back our implementation by extending the <code>AbstractDiscoveryStrategy</code> and only implementing the absolute minimum ourselves.</p>
<pre><code class="lang-java">package com.hazelcast.examples.spi.discovery;

import com.hazelcast...;

public class HostsDiscoveryStrategy
    extends AbstractDiscoveryStrategy {

  private final String siteDomain;  

  public HostsDiscoveryStrategy( ILogger logger,
                                 Map&lt;String, Comparable&gt; properties ) {

    super( logger, properties );

    // Make it possible to override the value from the configuration on
    // the system&#39;s environment or JVM properties
    // -Ddiscovery.hosts.site-domain=some.domain
    this.siteDomain = getOrNull( &quot;discovery.hosts&quot;,
                                 HostsDiscoveryConfiguration.DOMAIN );
  }                              

  public Iterable&lt;DiscoveryNode&gt; discoverNodes() {
    List&lt;String&gt; assignments = filterHosts();
    return mapToDiscoveryNodes( assignments );
  }

  // ...
}
</code></pre>
<a name="overriding-discovery-configuration"></a><h4 id="overriding-discovery-configuration">Overriding Discovery Configuration</h4>
<p>So far our implementation will retrieve the configuration property for the <code>site-domain</code>. Our implementation offers the option to override the value from the configuration (XML or Config API) right from the system environment or JVM properties. That can be useful when the <code>hazelcast.xml</code> defines a setup for an developer system (like <code>cluster.local</code>) and operations wants to override it for the real deployment. By providing a prefix (in this case <code>discovery.hosts</code>) we created an external property named <code>discovery.hosts.site-domain</code> which can be set as an environment variable or passed as a JVM property from the startup script.</p>
<p>The lookup priority is explained in the following list, priority is from top to bottom:</p>
<ul>
<li>JVM properties (or <code>hazelcast.xml</code> <properties/> section)</li>
<li>System environment</li>
<li>Configuration properties</li>
</ul>
<a name="implementing-lookup"></a><h4 id="implementing-lookup">Implementing Lookup</h4>
<p>Since we now have the value for our property we can implement the actual lookup and mapping as already prepared in the <code>discoverNodes</code> method. The following part is very specific to this special discovery strategy, for completeness we&#39;re showing it anyways.</p>
<pre><code class="lang-java">private static final String HOSTS_NIX = &quot;/etc/hosts&quot;;
private static final String HOSTS_WINDOWS =
                   &quot;%SystemRoot%\\system32\\drivers\\etc\\hosts&quot;;

private List&lt;String&gt; filterHosts() {
  String os = System.getProperty( &quot;os.name&quot; );

  String hostsPath;
  if ( os.contains( &quot;Windows&quot; ) ) {
    hostsPath = HOSTS_WINDOWS;
  } else {
    hostsPath = HOSTS_NIX;
  }

  File hosts = new File( hostsPath );

  // Read all lines
  List&lt;String&gt; lines = readLines( hosts );

  List&lt;String&gt; assignments = new ArrayList&lt;String&gt;();
  for ( String line : lines ) {
    // Example:
    // 192.168.0.1   host1.cluster.local
    if ( matchesDomain( line ) ) {
      assignments.add( line );
    }
  }
  return assignments;
}
</code></pre>
<a name="mapping-to-discoverynodes"></a><h4 id="mapping-to-discoverynode-s">Mapping to <code>DiscoveryNode</code>s</h4>
<p>After we now collected the address assignments configured in the hosts file we can go to the final step and map those to the <code>DiscoveryNode</code>s to return them from our strategy.</p>
<pre><code class="lang-java">private Iterable&lt;DiscoveryNode&gt; mapToDiscoveryNodes( List&lt;String&gt; assignments ) {
  Collection&lt;DiscoveryNode&gt; discoveredNodes = new ArrayList&lt;DiscoveryNode&gt;();

  for ( String assignment : assignments ) {
    String address = sliceAddress( assignment );
    String hostname = sliceHostname( assignment );

    Map&lt;String, Object&gt; attributes = 
        Collections.singletonMap( &quot;hostname&quot;, hostname );

    InetAddress inetAddress = mapToInetAddress( address );
    Address addr = new Address( inetAddress, NetworkConfig.DEFAULT_PORT );

    discoveredNodes.add( new SimpleDiscoveryNode( addr, attributes ) );
  }
  return discoveredNodes;
}
</code></pre>
<p>With that mapping we now have a full discovery, executed whenever Hazelcast asks for IPs. So why don&#39;t we read them in once and cache them? The answer is simple, it might happen that members go down or come up over time. Since we expect the hosts file to be injected into the running container it also might change over time. We want to get the latest available members, therefore we read the file on request.</p>
<a name="configuring-discoverystrategy"></a><h4 id="configuring-discoverystrategy-">Configuring <code>DiscoveryStrategy</code></h4>
<p>To actually use the new <code>DiscoveryStrategy</code> implementation we need to configure it like in the following example:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  &lt;!-- activate Discovery SPI --&gt;
  &lt;properties&gt;
    &lt;property name=&quot;hazelcast.discovery.enabled&quot;&gt;true&lt;/property&gt;
  &lt;/properties&gt;

  &lt;network&gt;
    &lt;join&gt;
      &lt;!-- deactivating other discoveries --&gt;
      &lt;multicast enabled=&quot;false&quot;/&gt;
      &lt;tcp-ip enabled=&quot;false&quot; /&gt;
      &lt;aws enabled=&quot;false&quot;/&gt;

      &lt;!-- activate our discovery strategy --&gt;
      &lt;discovery-strategies&gt;

        &lt;!-- class equals to the DiscoveryStrategy not the factory! --&gt;
        &lt;discovery-strategy enabled=&quot;true&quot;
            class=&quot;com.hazelcast.examples.spi.discovery.HostsDiscoveryStrategy&quot;&gt;

          &lt;properties&gt;
            &lt;property name=&quot;site-domain&quot;&gt;cluster.local&lt;/property&gt;
          &lt;/properties&gt;
        &lt;/discovery-strategy&gt;
      &lt;/discovery-strategies&gt;
    &lt;/join&gt;
  &lt;/network&gt;
&lt;/hazelcast&gt;
</code></pre>
<p>To find out further details, please have a look at the Discovery SPI Javadoc.</p>

<a name="discoveryservice-framework-integration"></a><h3 id="discoveryservice-framework-integration-">DiscoveryService (Framework integration)</h3>
<p>Since the <code>DiscoveryStrategy</code> is meant for cloud vendors or implementors of service discovery systems, the <code>DiscoveryService</code> is meant for integrators. In this case, integrators means people integrating Hazelcast into their own systems or frameworks. In those situations, there are sometimes special requirements on how to lookup framework services like the discovery strategies or similar services. Integrators can extend or implement their own <code>DiscoveryService</code> and <code>DiscoveryServiceProvider</code> and inject it using the Hazelcast Config API (<code>com.hazelcast.config.DiscoveryConfig</code>) prior to instantiating the <code>HazelcastInstance</code>. In any case, integrators might have to remember that a <code>DiscoveryService</code> might have to change behavior based on the runtime environment (Hazelcast member or client), and then the <code>DiscoveryServiceSettings</code> should provide information about the started <code>HazelcastInstance</code>.</p>
<p>Since the implementation heavily depends on one&#39;s needs, there is no reason to provide an example of how to implement your own <code>DiscoveryService</code>. However, Hazelcast provides a default implementation which can be a good example to get started. This default implementation is <code>com.hazelcast.spi.discovery.impl.DefaultDiscoveryService</code>.</p>

<a name="config-properties-spi"></a><h2 id="config-properties-spi">Config Properties SPI</h2>
<p>The Config Properties SPI is an easy way that you can configure SPI plugins using a prebuilt system of automatic conversion and validation.</p>
<a name="config-properties-spi-classes"></a><h3 id="config-properties-spi-classes">Config Properties SPI Classes</h3>
<p>The Config Properties SPI consists of a small set of classes and provided implementations.</p>
<a name="propertydefinition-define-a-single-property"></a><h4 id="propertydefinition-define-a-single-property">PropertyDefinition: Define a Single Property</h4>
<p>The <code>com.hazelcast.config.properties.PropertyDefinition</code> interface defines a single property inside a given configuration. It consists of a key string and type (in form of a <code>com.hazelcast.core.TypeConverter</code>).</p>
<p>You can mark properties as optional and you can have an additional validation step to make sure the provided value matches certain rules (like port numbers must be between 0-65535 or similar).</p>
<a name="simplepropertydefinition-basic-propertydefinition"></a><h4 id="simplepropertydefinition-basic-propertydefinition">SimplePropertyDefinition: Basic PropertyDefinition</h4>
<p>For convenience, the <code>com.hazelcast.config.properties.SimplePropertyDefinition</code> class is provided. This class is a basic implementation of the <code>PropertyDefinition</code> interface and should be enough for most situations. In case of additional needs, you are free to provide your own implementation of the <code>PropertyDefinition</code> interface.</p>
<a name="propertytypeconverter-set-of-typeconverters"></a><h4 id="propertytypeconverter-set-of-typeconverters">PropertyTypeConverter: Set of TypeConverters</h4>
<p>The <code>com.hazelcast.config.properties.PropertyTypeConverter</code> enum provides a preset of <code>TypeConverter</code>s. Provided are the most common basic types:</p>
<ul>
<li>String</li>
<li>Short</li>
<li>Integer</li>
<li>Long</li>
<li>Float</li>
<li>Double</li>
<li>Boolean</li>
</ul>
<a name="valuevalidator-and-validationexception"></a><h4 id="valuevalidator-and-validationexception">ValueValidator and ValidationException</h4>
<p>The <code>com.hazelcast.config.properties.ValueValidator</code> interface implements additional value validation. The configured value will be validated before it is returned to the requester. If validation fails, a <code>com.hazelcast.config.properties.ValidationException</code> is thrown and the requester has to handle it or throw the exception further.</p>
<a name="config-properties-spi-example"></a><h3 id="config-properties-spi-example">Config Properties SPI Example</h3>
<p>This sub-section will show a quick example of how to setup, configure and use the Config Properties SPI.</p>
<a name="defining-a-config-propertydefinition"></a><h4 id="defining-a-config-propertydefinition">Defining a Config PropertyDefinition</h4>
<p>Defining a property is as easy as giving it a name and a type.</p>
<pre><code class="lang-java">PropertyDefinition property = new SimplePropertyDefinition(
    &quot;my-key&quot;, PropertyTypeConverter.STRING
);
</code></pre>
<p>We defined a property named <code>my-key</code> with a type of a string. If none of the predefined <code>TypeConverter</code>s matches the need, users are free to provide their own implementation.</p>
<a name="providing-a-value-in-xml"></a><h4 id="providing-a-value-in-xml">Providing a value in XML</h4>
<p>The above property is now configurable in two ways:</p>
<pre><code class="lang-xml">&lt;!-- option 1 --&gt;
&lt;my-key&gt;value&lt;/my-key&gt;

&lt;!-- option 2 --&gt;
&lt;property name=&quot;my-key&quot;&gt;value&lt;/property&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>In any case, both options are useable interchangeably, however the later version is recommended by Hazelcast for schema applicability.</em> </p>
<a name="retrieving-a-propertydefinition-value"></a><h4 id="retrieving-a-propertydefinition-value">Retrieving a PropertyDefinition Value</h4>
<p>To eventually retrieve a value, use the <code>PropertyDefinition</code> to get and convert the value automatically.</p>
<pre><code class="lang-java">public &lt;T&gt; T getConfig( PropertyDefinition property, 
                        Map&lt;String, Comparable&gt; properties ) {

  Map&lt;String, Comparable&gt; properties = ...;
  TypeConverter typeConverter = property.typeConverter();

  Comparable value = properties.get( property.key() );
  return typeConverter.convert( value );
}
</code></pre>

<a name="network-partitioning-split-brain-syndrome"></a><h1 id="network-partitioning-split-brain-syndrome">Network Partitioning - Split Brain Syndrome</h1>
<p>Imagine that you have a cluster with ten members and that the network is divided into two in a way that four members cannot see the other six members. As a result, you end up having two separate clusters: one with four members and one with six members. Members in each sub-cluster think that the other members are dead even though they are not. This situation is called Network Partitioning (a.k.a. <em>Split-Brain Syndrome</em>).</p>
<p>However, these two clusters have a combination of the 271 (using default) primary and backup partitions. It is very likely that not all of the 271 partitions, including both primaries and backups, exist in both mini-clusters.
Therefore, from each mini-clusters perspective, data has been lost as some partitions no longer exist (they exist on the other segment).</p>
<a name="understanding-partition-recreation"></a><h2 id="understanding-partition-recreation">Understanding Partition Recreation</h2>
<p>If a MapStore was in use, those lost partitions would be reloaded from some database, making each mini-cluster complete.
Each mini-cluster will then recreate the missing primary partitions and continue to store data in them, including backups on the other members.</p>
<a name="understanding-backup-partition-creation"></a><h2 id="understanding-backup-partition-creation">Understanding Backup Partition Creation</h2>
<p>When primary partitions exist without a backup, a backup version problem will be detected and a backup partition will be created.
When backups exist without a primary, the backups will be promoted to primary partitions and new backups will be created with proper versioning.
At this time, both mini-clusters have repaired themselves with all 271 partitions with backups, and continue to handle traffic without any knowledge of each other.
Given that they have enough remaining memory (assumption), they are just smaller and can handle less throughput.</p>
<a name="understanding-the-update-overwrite-scenario"></a><h2 id="understanding-the-update-overwrite-scenario">Understanding The Update Overwrite Scenario</h2>
<p>If a MapStore is in use and the network to the database is available, one or both of the mini-clusters will write updates to the same database.
There is a potential for the mini-clusters to overwrite the same cache entry records if modified in both mini-clusters.
This overwrite scenario represents a potential data loss, and thus the database design should consider an insert and aggregate on read or version strategy rather than update records in place.</p>
<p>If the network to the database is not available, then based on the configured or coded consistency level or transaction, entry updates are held in cache or updates are rejected (fully synchronous and consistent).
When held in cache, the updates will be considered dirty and will be written to the database when it becomes available. You can view the dirty entry counts per cluster member in the Management Center web console (please see the <a href="#map-monitoring">Map Monitoring section</a>).</p>
<a name="what-happens-when-the-network-failure-is-fixed"></a><h2 id="what-happens-when-the-network-failure-is-fixed">What Happens When The Network Failure Is Fixed</h2>
<p>Since it is a network failure, there is no way to programmatically avoid your application running as two separate independent clusters.
But what will happen after the network failure is fixed and connectivity is restored between these two clusters?
Will these two clusters merge into one again? If they do, how are the data conflicts resolved, because you might end up having two different values for the same key in the same map?</p>
<p>When the network is restored, all 271 partitions should exist in both mini-clusters and they should all undergo the merge. Once all primaries are merged,
all backups are rewritten so their versions are correct. You may want to write a merge policy using the <code>MapMergePolicy</code> interface that rebuilds the entry from the database rather than from memory.</p>
<p>The only metadata available for merge decisions are from the <code>EntryView</code> interface that includes object size (cost), hits count, last updated/stored dates, and a version number that starts at zero and is incremented for each entry update.
You could also create your own versioning scheme or capture a time series of deltas to reconstruct an entry.</p>
<a name="how-hazelcast-split-brain-merge-happens"></a><h2 id="how-hazelcast-split-brain-merge-happens">How Hazelcast Split Brain Merge Happens</h2>
<p>Here is, step by step, how Hazelcast split brain merge happens:</p>
<ol>
<li>The oldest member of the cluster checks if there is another cluster with the same <em>group-name</em> and <em>group-password</em> in the network.</li>
<li>If the oldest member finds such a cluster, then it figures out which cluster should merge to the other.</li>
<li><p>Each member of the merging cluster will do the following.</p>
</li>
<li><p>Pause.</p>
</li>
<li>Take locally owned map entries.</li>
<li>Close all of its network connections (detach from its cluster).</li>
<li>Join to the new cluster.</li>
<li>Send merge request for each of its locally owned map entry.</li>
<li>Resume.</li>
</ol>
<p>Each member of the merging cluster rejoins the new cluster and sends a merge request for each of its locally owned map entries. Two important points:</p>
<ul>
<li>The smaller cluster will merge into the bigger one. If they have equal number of members then a hashing algorithm determines the merging cluster.</li>
<li>Each cluster may have different versions of the same key in the same map. The destination cluster will decide how to handle merging entry based on the <code>MergePolicy</code> set for that map. There are built-in merge policies such as <code>PassThroughMergePolicy</code>, <code>PutIfAbsentMapMergePolicy</code>, <code>HigherHitsMapMergePolicy</code> and <code>LatestUpdateMapMergePolicy</code>. You can develop your own merge policy by implementing <code>com.hazelcast.map.merge.MapMergePolicy</code>. You should set the full class name of your implementation to the <code>merge-policy</code> configuration.</li>
</ul>
<pre><code class="lang-java">public interface MergePolicy {
  /**
  * Returns the value of the entry after the merge
  * of entries with the same key. Returning value can be
  * You should consider the case where existingEntry is null.
  *
  * @param mapName       name of the map
  * @param mergingEntry  entry merging into the destination cluster
  * @param existingEntry existing entry in the destination cluster
  * @return final value of the entry. If returns null then entry will be removed.
  */
  Object merge( String mapName, EntryView mergingEntry, EntryView existingEntry );
}
</code></pre>
<a name="specifying-merge-policies"></a><h2 id="specifying-merge-policies">Specifying Merge Policies</h2>
<p>Here is how merge policies are specified per map:</p>
<pre><code class="lang-xml">&lt;hazelcast&gt;
  ...
  &lt;map name=&quot;default&quot;&gt;
    &lt;backup-count&gt;1&lt;/backup-count&gt;
    &lt;eviction-policy&gt;NONE&lt;/eviction-policy&gt;
    &lt;max-size&gt;0&lt;/max-size&gt;
    &lt;eviction-percentage&gt;25&lt;/eviction-percentage&gt;
    &lt;!--
      While recovering from split-brain (network partitioning),
      map entries in the small cluster will merge into the bigger cluster
      based on the policy set here. When an entry merge into the
      cluster, there might an existing entry with the same key already.
      Values of these entries might be different for that same key.
      Which value should be set for the key? Conflict is resolved by
      the policy set here. Default policy is hz.ADD_NEW_ENTRY

      There are built-in merge policies such as
      There are built-in merge policies such as
      com.hazelcast.map.merge.PassThroughMergePolicy; entry will be added if
          there is no existing entry for the key.
      com.hazelcast.map.merge.PutIfAbsentMapMergePolicy ; entry will be
          added if the merging entry doesn&#39;t exist in the cluster.
      com.hazelcast.map.merge.HigherHitsMapMergePolicy ; entry with the
          higher hits wins.
      com.hazelcast.map.merge.LatestUpdateMapMergePolicy ; entry with the
          latest update wins.
    --&gt;
    &lt;merge-policy&gt;MY_MERGE_POLICY_CLASS&lt;/merge-policy&gt;
  &lt;/map&gt;

  ...
&lt;/hazelcast&gt;
</code></pre>
<p><img src="images/NoteSmall.jpg" alt="image"> <strong><em>NOTE:</em></strong> <em>Map is the only Hazelcast distributed data structure that merges after a split brain syndrome. For the other data structures (e.g. Queue, Topic, IdGenerator, etc. ), one instance of that data structure is chosen after split brain syndrome. </em></p>

<a name="system-properties"></a><h1 id="system-properties">System Properties</h1>
<p>The table below lists the system properties with their descriptions in alphabetical order.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default Value</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>hazelcast.application.validation.token</code></td>
<td style="text-align:left"></td>
<td style="text-align:left">string</td>
<td style="text-align:left">This property can be used to verify that Hazelcast members only join when their application level configuration is the same.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.backpressure.backoff.timeout.millis</code></td>
<td style="text-align:left">60000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Controls the maximum timeout in milliseconds to wait for an invocation space to be available. The value needs to be equal to or larger than 0.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.backpressure.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable back pressure.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.backpressure.max.concurrent.invocations.per.partition</code></td>
<td style="text-align:left">100</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The maximum number of concurrent invocations per partition.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.backpressure.syncwindow</code></td>
<td style="text-align:left">1000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Used when back pressure is enabled. The larger the sync window value, the less frequent a asynchronous backup is converted to a sync backup.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.cache.invalidation.batch.enabled</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Specifies whether the cache invalidation event batch sending is enabled or not.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.cache.invalidation.batch.size</code></td>
<td style="text-align:left">100</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Defines the maximum number of cache invalidation events to be drained and sent to the event listeners in a batch.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.cache.invalidation.batchfrequency.seconds</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Defines cache invalidation event batch sending frequency in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.clientengine.thread.count</code></td>
<td style="text-align:left"></td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum number of threads to process non-partition-aware client requests, like <code>map.size()</code>, query, executor tasks, etc. Default count is 20 times number of cores.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.event.queue.capacity</code></td>
<td style="text-align:left">1000000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Default value of the capacity of executor that handles incoming event packets.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.event.thread.count</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Thread count for handling incoming event packets.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.heartbeat.interval</code></td>
<td style="text-align:left">10000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">The frequency of heartbeat messages sent by the clients to members.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.heartbeat.timeout</code></td>
<td style="text-align:left">300000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Timeout for the heartbeat messages sent by the client to members. If no messages pass between client and member within the given time via this property in milliseconds, the connection will be closed.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.invocation.timeout.seconds</code></td>
<td style="text-align:left">120</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Time to give up the invocation when a member in the member list is not reachable.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.max.no.heartbeat.seconds</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Time after which the member assumes the client is dead and closes its connections to the client.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.client.shuffle.member.list</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">string</td>
<td style="text-align:left">The client shuffles the given member list to prevent all clients to connect to the same member when this property is <code>false</code>. When it is set to <code>true</code>, the client tries to connect to the members in the given order.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.connect.all.wait.seconds</code></td>
<td style="text-align:left">120</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Timeout to connect all other cluster members when a member is joining to a cluster.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.connection.monitor.interval</code></td>
<td style="text-align:left">100</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Minimum interval in milliseconds to consider a connection error as critical.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.connection.monitor.max.faults</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum IO error count before disconnecting from a member.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.discovery.public.ip.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable use of public IP address in member discovery with Discovery SPI.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.enterprise.license.key</code></td>
<td style="text-align:left">null</td>
<td style="text-align:left">string</td>
<td style="text-align:left"><a href="http://www.hazelcast.com/products.jsp" target="_blank">Hazelcast Enterprise</a> license key.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.event.queue.capacity</code></td>
<td style="text-align:left">1000000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Capacity of internal event queue.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.event.queue.timeout.millis</code></td>
<td style="text-align:left">250</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Timeout to enqueue events to event queue.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.event.thread.count</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of event handler threads.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.graceful.shutdown.max.wait</code></td>
<td style="text-align:left">600</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum wait in seconds during graceful shutdown.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.health.monitoring.delay.seconds</code></td>
<td style="text-align:left">30</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Health monitoring logging interval in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.health.monitoring.level</code></td>
<td style="text-align:left">SILENT</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Health monitoring log level. When <em>SILENT</em>, logs are printed only when values exceed some predefined threshold. When <em>NOISY</em>, logs are always printed periodically. Set <em>OFF</em> to turn off completely.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.heartbeat.interval.seconds</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Heartbeat send interval in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.icmp.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable ICMP ping.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.icmp.timeout</code></td>
<td style="text-align:left">1000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">ICMP timeout in milliseconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.icmp.ttl</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">ICMP TTL (maximum numbers of hops to try).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.initial.min.cluster.size</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Initial expected cluster size to wait before member to start completely.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.initial.wait.seconds</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Initial time in seconds to wait before member to start completely.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.internal.map.expiration.cleanup.operation.count</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">This is a property which is used internally and subject to change in the future releases.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.internal.map.expiration.cleanup.percentage</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">This is a property which is used internally and subject to change in the future releases.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.internal.map.expiration.task.period.seconds</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">This is a property which is used internally and subject to change in the future releases.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.io.balancer.interval.seconds</code></td>
<td style="text-align:left">20</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval in seconds between IOBalancer executions.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.io.input.thread.count</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of socket input threads.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.io.output.thread.count</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of socket output threads.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.io.thread.count</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of threads performing socket input and socket output. If, for example, the default value (3) is used, it means there are 3 threads performing input and 3 threads performing output (6 threads in total).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.jcache.provider.type</code></td>
<td style="text-align:left"></td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of the JCache provider. Values can be <code>client</code> or <code>server</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.jmx</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable <a href="#monitoring-with-jmx">JMX</a> agent.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.lock.max.lease.time.seconds</code></td>
<td style="text-align:left">Long.MAX_VALUE</td>
<td style="text-align:left">long</td>
<td style="text-align:left">All locks which are acquired without an explicit lease time use this value (in seconds) as the lease time. When you want to set an explicit lease time for your locks, you cannot set it to a longer time than this value.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.logging.type</code></td>
<td style="text-align:left">jdk</td>
<td style="text-align:left">enum</td>
<td style="text-align:left">Name of <a href="#logging-configuration">logging</a> framework type to send logging events.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.mancenter.home</code></td>
<td style="text-align:left">mancenter</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Folder where Management Center data files are stored (license information, time travel information, etc.).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.map.entry.filtering.natural.event.types</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Notify <a href="#listening-to-map-entries-with-predicates">entry listeners with predicates</a> on map entry updates with events that match entry, update or exit from predicate value space.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.map.expiry.delay.seconds</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Useful to deal with some possible edge cases. For example, when using EntryProcessor, without this delay, you may see an EntryProcessor running on owner partition found a key but EntryBackupProcessor did not find it on backup. As a result of this, when backup promotes to owner, you will end up an unprocessed key.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.map.load.chunk.size</code></td>
<td style="text-align:left">1000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Chunk size for <a href="#loading-and-storing-persistent-data">MapLoader</a>&#39;s map initialization process (MapLoader.loadAllKeys()).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.map.replica.wait.seconds.for.scheduled.tasks</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Scheduler delay for map tasks those will be executed on backup members.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.map.write.behind.queue.capacity</code></td>
<td style="text-align:left">50000</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Maximum write-behind queue capacity per member. It is the total of all write-behind queue sizes in a member including backups. Its maximum value is <code>Integer.MAX_VALUE</code>. The value of this property is taken into account only if the <code>write-coalescing</code> element of the Map Store configuration is <code>false</code>. Please refer to the <a href="#map-store">Map Store section</a> for the description of the <code>write-coalescing</code> element.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.master.confirmation.interval.seconds</code></td>
<td style="text-align:left">30</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval at which members send master confirmation.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.max.join.merge.target.seconds</code></td>
<td style="text-align:left">20</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Split-brain merge timeout for a specific target.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.max.join.seconds</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Join timeout, maximum time to try to join before giving.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.max.no.heartbeat.seconds</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum timeout of heartbeat in seconds for a member to assume it is dead.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.max.no.master.confirmation.seconds</code></td>
<td style="text-align:left">450</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Max timeout of master confirmation from other members.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.max.wait.seconds.before.join</code></td>
<td style="text-align:left">20</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum wait time before join operation.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.mc.max.visible.instance.count</code></td>
<td style="text-align:left">100</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Management Center maximum visible instance count.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.mc.max.visible.slow.operations.count</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Management Center maximum visible slow operations count.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.mc.url.change.enabled</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Management Center changing server url is enabled.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.member.list.publish.interval.seconds</code></td>
<td style="text-align:left">600</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval at which master member publishes a member list.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.memcache.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable <a href="#memcache-client">Memcache</a> client request listener service.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.merge.first.run.delay.seconds</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Initial run delay of <a href="#network-partitioning-split-brain-syndrome">split brain/merge process</a> in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.merge.next.run.delay.seconds</code></td>
<td style="text-align:left">120</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Run interval of <a href="#network-partitioning-split-brain-syndrome">split brain/merge process</a> in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.migration.min.delay.on.member.removed.seconds</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Minimum delay (in seconds) between detection of a member that has left and start of the rebalancing process.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.operation.backup.timeout.millis</code></td>
<td style="text-align:left">5000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum time a caller to wait for backup responses of an operation. After this timeout, operation response will be returned to the caller even no backup response is received.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.operation.call.timeout.millis</code></td>
<td style="text-align:left">60000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Timeout to wait for a response when a remote call is sent, in milliseconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.operation.generic.thread.count</code></td>
<td style="text-align:left">-1</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of generic operation handler threads. <code>-1</code> means CPU core count x 2.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.operation.thread.count</code></td>
<td style="text-align:left">-1</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of partition based operation handler threads. <code>-1</code> means CPU core count x 2.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.backup.sync.interval</code></td>
<td style="text-align:left">30</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval for syncing backup replicas.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.count</code></td>
<td style="text-align:left">271</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Total partition count.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.max.parallel.replications</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum number of parallel partition backup replication operations per member. When a partition backup ownership changes or a backup inconsistency is detected, the members start to sync their backup partitions. This parameter limits the maximum running replication operations in parallel.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.migration.interval</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval to run partition migration tasks in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.migration.stale.read.disabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Hazelcast allows read operations to be performed while a partition is being migrated. This can lead to stale reads for some scenarios. You can disable stale read operations by setting this system property&#39;s value to &quot;true&quot;. Its default value is &quot;false&quot;, meaning that stale reads are allowed.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.migration.timeout</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Timeout for partition migration tasks in seconds.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partition.table.send.interval</code></td>
<td style="text-align:left">15</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Interval for publishing partition table periodically to all cluster members.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.partitioning.strategy.class</code></td>
<td style="text-align:left">null</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Class name implementing <code>com.hazelcast.core.PartitioningStrategy</code>, which defines key to partition mapping.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.performance.monitor.max.rolled.file.count</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The PerformanceMonitor uses a rolling file approach to prevent eating too much disk space. This property sets the maximum number of rolling files to keep on disk.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.performance.monitor.max.rolled.file.size.mb</code></td>
<td style="text-align:left">10</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The performance monitor uses a rolling file approach to prevent eating too much disk space. This property sets the maximum size in MB for a single file. Every HazelcastInstance gets its own history of log files.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.performance.monitoring.enabled</code></td>
<td style="text-align:left"></td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable the performance monitor, a tool which allows you to see internal performance metrics. These metrics are written to a dedicated log file.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.performance.monitor.delay.seconds</code></td>
<td style="text-align:left"></td>
<td style="text-align:left">int</td>
<td style="text-align:left">The period between successive entries in the performance monitor&#39;s log file.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.prefer.ipv4.stack</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Prefer IPv4 network interface when picking a local address.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.query.max.local.partition.limit.for.precheck</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Maximum value of local partitions to trigger local pre-check for TruePredicate query operations on maps.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.query.optimizer.type</code></td>
<td style="text-align:left">RULES</td>
<td style="text-align:left">String</td>
<td style="text-align:left">Type of the query optimizer. For optimizations based on static rules, set the value to <code>RULES</code>. To disable the optimization, set the value to <code>NONE</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.query.predicate.parallel.evaluation</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Each Hazelcast member evaluates query predicates using a single thread by default. In most cases, the overhead of inter-thread communications overweight can benefit from parallel execution. When you have a large dataset and/or slow predicate, you may benefit from parallel predicate evaluations. Set to <code>true</code> if you are using slow predicates or have &gt; 100,000s entries per member.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.query.result.size.limit</code></td>
<td style="text-align:left">-1</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Result size limit for query operations on maps. This value defines the maximum number of returned elements for a single query result. If a query exceeds this number of elements, a QueryResultSizeExceededException is thrown. Its default value is -1, meaning it is disabled.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.rest.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable <a href="#rest-client">REST</a> client request listener service.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.shutdownhook.enabled</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable Hazelcast shutdownhook thread. When this is enabled, this thread terminates the Hazelcast instance without waiting to shutdown gracefully.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.slow.operation.detector.enabled</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enables/disables the <a href="#slowoperationdetector">SlowOperationDetector</a>.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.slow.operation.detector.log.purge.interval.seconds</code></td>
<td style="text-align:left">300</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Purge interval for slow operation logs.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.slow.operation.detector.log.retention.seconds</code></td>
<td style="text-align:left">3600</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Defines the retention time of invocations in slow operation logs. If an invocation is older than this value, it will be purged from the log to prevent unlimited memory usage. When all invocations are purged from a log, the log itself will be deleted.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.slow.operation.detector.stacktrace.logging.enabled</code></td>
<td style="text-align:left">false</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Defines if the stacktraces of slow operations are logged in the log file. Stack traces are always reported to the Management Center, but by default, they are not printed to keep the log size small.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.slow.operation.detector.threshold.millis</code></td>
<td style="text-align:left">10000</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Defines a threshold above which a running operation in <code>OperationService</code> is considered to be slow. These operations log a warning and are shown in the Management Center with detailed information, e.g. stacktrace.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.bind.any</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Bind both server-socket and client-sockets to any local interface.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.client.bind</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Bind client socket to an interface when connecting to a remote server socket. When set to <code>false</code>, client socket is not bound to any interface.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.client.bind.any</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Bind client-sockets to any local interface. If not set, <code>hazelcast.socket.bind.any</code> will be used as default.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.client.receive.buffer.size</code></td>
<td style="text-align:left">-1</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Hazelcast creates all connections with receive buffer size set according to the <code>hazelcast.socket.receive.buffer.size</code>. When it detects a connection opened by a client, then it adjusts the receive buffer size according to this property. It is in kilobytes and the default value is -1.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.client.send.buffer.size</code></td>
<td style="text-align:left">-1</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Hazelcast creates all connections with send buffer size set according to the <code>hazelcast.socket.send.buffer.size</code>. When it detects a connection opened by a client, then it adjusts the send buffer size according to this property. It is in kilobytes and the default value is -1.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.connect.timeout.seconds</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Socket connection timeout in seconds. <code>Socket.connect()</code> will be blocked until either connection is established or connection is refused or this timeout passes. Default is 0, means infinite.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.keep.alive</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Socket set keep alive (<code>SO_KEEPALIVE</code>).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.linger.seconds</code></td>
<td style="text-align:left">0</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Set socket <code>SO_LINGER</code> option.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.no.delay</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Socket set TCP no delay.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.receive.buffer.size</code></td>
<td style="text-align:left">32</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Socket receive buffer (<code>SO_RCVBUF</code>) size in KB. If you have a very fast network (e.g. 10gbit) and/or you have large entries, then you may benefit from increasing sender/receiver buffer sizes. Use this property and the next one below tune the size. For example, a send/receive buffer size of 1024 kB is a safe starting point for a 10gbit network.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.send.buffer.size</code></td>
<td style="text-align:left">32</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Socket send buffer (<code>SO_SNDBUF</code>) size in KB.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.socket.server.bind.any</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Bind server-socket to any local interface. If not set, <code>hazelcast.socket.bind.any</code> will be used as default.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.tcp.join.port.try.count</code></td>
<td style="text-align:left">3</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The number of incremental ports, starting with the port number defined in the network configuration, that will be used to connect to a host (which is defined without a port in TCP/IP member list while a member is searching for a cluster).</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.unsafe.mode</code></td>
<td style="text-align:left">auto</td>
<td style="text-align:left">string</td>
<td style="text-align:left">&quot;auto&quot; (the default value) automatically detects whether the usage of <code>Unsafe</code> is suitable for a given platform. &quot;disabled&quot; explicitly disables the <code>Unsafe</code> usage in your platform. &quot;enforced&quot; enforces the usage of <code>Unsafe</code> even if your platform does not support it. This property can only be set by passing a JVM-wide system property.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.phone.home.enabled</code></td>
<td style="text-align:left">true</td>
<td style="text-align:left">bool</td>
<td style="text-align:left">Enable or disable the sending of phone home data to Hazelcast&#39;s phone home server.</td>
</tr>
<tr>
<td style="text-align:left"><code>hazelcast.wait.seconds.before.join</code></td>
<td style="text-align:left">5</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Wait time before join operation.</td>
</tr>
</tbody>
</table>

<a name="common-exception-types"></a><h1 id="common-exception-types">Common Exception Types</h1>
<p>You may see the following exceptions in any Hazelcast operation when the following situations occur:</p>
<ul>
<li><p><code>HazelcastInstanceNotActiveException</code>: Thrown when <code>HazelcastInstance</code> is not active (already shutdown or being shutdown) during an invocation. </p>
</li>
<li><p><code>HazelcastOverloadException</code>: Thrown when the system will not handle any more load due to an overload. This exception is thrown when back pressure is enabled.</p>
</li>
<li><p><code>DistributedObjectDestroyedException</code>: Thrown when an already destroyed <code>DistributedObject</code> (IMap, IQueue, etc.) is accessed or when a method is called over a destroyed <code>DistributedObject</code>.</p>
</li>
<li><p><code>MemberLeftException</code>: Thrown when a member leaves during an invocation or execution.</p>
</li>
</ul>
<p>Hazelcast also throws the following exceptions in the cases of overall system problems such as networking issues and long pauses:</p>
<ul>
<li><p><code>PartitionMigratingException</code>: Thrown when an operation is executed on a partition, but that partition is currently being moved.</p>
</li>
<li><p><code>TargetNotMemberException</code>: Thrown when an operation is sent to a machine that is not a member of the cluster.</p>
</li>
<li><p><code>CallerNotMemberException</code>: Thrown when an operation was sent by a machine which is not a member in the cluster when the operation is executed.</p>
</li>
<li><p><code>WrongTargetException</code>: Thrown when an operation is executed on the wrong machine, usually because the partition that operation belongs to has been moved to some other member.</p>
</li>
</ul>

<a name="license-questions"></a><h1 id="license-questions">License Questions</h1>
<p>Hazelcast is distributed using the <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License 2</a>, therefore permissions are granted
to use, reproduce and distribute it along with any kind of open source and closed source applications.</p>
<p>Hazelcast Enterprise is a commercial product of Hazelcast, Inc. and is distributed under a commercial license that must be acquired
before using it in any type of released software. Feel free to contact <a href="http://hazelcast.com/contact/" target="_blank">Hazelcast sales department</a>
for more information on commercial offers.</p>
<p>Depending on the used feature-set, Hazelcast has certain runtime dependencies which might have different licenses. Following are dependencies and their respective licenses.</p>
<a name="embedded-dependencies"></a><h2 id="embedded-dependencies">Embedded Dependencies</h2>
<p>Embedded dependencies are merged (shaded) with the Hazelcast codebase at compile-time. These dependencies become an integral part
of the Hazelcast distribution.</p>
<p>For license files of embedded dependencies, please see the <code>license</code> directory of the Hazelcast distribution, available at our
<a href="http://hazelcast.org/download/" target="_blank">download page</a>.</p>
<p><strong>minimal-json</strong>:</p>
<p>minimal-json is a JSON parsing and generation library which is a part of the Hazelcast distribution. It is used for communication
between the Hazelcast cluster and the Management Center.</p>
<p>minimal-json is distributed under the <a href="http://opensource.org/licenses/MIT" target="_blank">MIT license</a> and offers the same rights to add, use,
modify, and distribute the source code as the Apache License 2.0 that Hazelcast uses. However, some other restrictions might apply.</p>
<a name="runtime-dependencies"></a><h2 id="runtime-dependencies">Runtime Dependencies</h2>
<p>Depending on the used features, additional dependencies might be added to the dependency set. Those runtime dependencies might have
other licenses. See the following list of additional runtime dependencies.</p>
<p><strong>Spring Framework</strong>:</p>
<p>Hazelcast offers a tight integration into the Spring Framework. Hazelcast can be configured and controlled using Spring.</p>
<p>The Spring Framework is distributed under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License 2</a> and therefore it is
fully compatible with Hazelcast.</p>
<p><strong>Hibernate</strong>:</p>
<p>Hazelcast integrates itself into Hibernate as a second-level cache provider.</p>
<p>Hibernate is distributed under the terms of the <a href="https://www.gnu.org/licenses/lgpl-2.1.html" target="_blank">Lesser General Public License 2.1</a>, 
also known as LGPL. Please read carefully the terms of the LGPL since restrictions might apply.</p>
<p><strong>Apache Tomcat</strong>:</p>
<p>Hazelcast Enterprise offers native integration into Apache Tomcat for web session clustering.</p>
<p>Apache Tomcat is distributed under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License 2</a> and therefore
fully compatible with Hazelcast.</p>
<p><strong>Eclipse Jetty</strong>:</p>
<p>Hazelcast Enterprise offers native integration into Jetty for web session clustering.</p>
<p>Jetty is distributed with a dual licensing strategy. It is licensed under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License 2</a>
and under the <a href="https://www.eclipse.org/legal/epl-v10.html" target="_blank">Eclipse Public License v1.0</a>, also known as EPL. Due to the Apache License,
it is fully compatible with Hazelcast.</p>
<p><strong>JCache API (JSR 107)</strong>:</p>
<p>Hazelcast offers a native implementation for JCache (JSR 107), which has a runtime dependency to the JCache API.</p>
<p>The JCache API is distributed under the terms of the so called <a href="https://jcp.org/aboutJava/communityprocess/licenses/jsr107/Spec-License-JSR-107-10_22_12.pdf" target="_blank">Specification License</a>).
Please read carefully the terms of this license since restrictions might apply.</p>
<p><strong>Boost C++ Libraries</strong>:</p>
<p>Hazelcast Enterprise offers a native C++ client, which has a link-time dependency to the Boost C++ Libraries.</p>
<p>The Boost Libraries are distributed under the terms of the <a href="http://www.boost.org/LICENSE_1_0.txt" target="_blank">Boost Software License</a>), which is
very similar to the MIT or BSD license. Please read carefully the terms of this license since restrictions might apply.</p>

<a name="frequently-asked-questions"></a><h1 id="frequently-asked-questions">Frequently Asked Questions</h1>
<a name="why-271-as-the-default-partition-count"></a><h2 id="why-271-as-the-default-partition-count-">Why 271 as the default partition count?</h2>
<p>The partition count of 271, being a prime number, is a good choice because it will be distributed to the members almost evenly. For a small to medium sized cluster, the count of 271 gives an almost even partition distribution and optimal-sized partitions.  As your cluster becomes bigger, you should make this count bigger to have evenly distributed partitions.</p>
<p><br></br></p>
<a name="is-hazelcast-thread-safe"></a><h2 id="is-hazelcast-thread-safe-">Is Hazelcast thread safe?</h2>
<p>Yes. All Hazelcast data structures are thread safe.</p>
<p><br></br></p>
<a name="how-do-members-discover-each-other"></a><h2 id="how-do-members-discover-each-other-">How do members discover each other?</h2>
<p>When a member is started in a cluster, it will dynamically and automatically be discovered. There are three types of discovery.</p>
<ul>
<li>Multicast discovery: members in a cluster discover each other by multicast, by default. </li>
<li>Discovery by TCP/IP: the first member created in the cluster (leader) will form a list of IP addresses of other joining members and will send this list to these members so the members will know each other.</li>
<li>If your application is placed on Amazon EC2, Hazelcast has an automatic discovery mechanism. You will give your Amazon credentials and the joining member will be discovered automatically.</li>
</ul>
<p>Once members are discovered, all the communication between them will be via TCP/IP.
<br></br>
<strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to the <a href="#discovering-cluster-members">Discovering Cluster Members section</a> for detailed information.</em></p>
<p><br></br></p>
<a name="what-happens-when-a-member-goes-down"></a><h2 id="what-happens-when-a-member-goes-down-">What happens when a member goes down?</h2>
<p>Once a member is gone (crashes), the following happens since data in each member has a backup in other members.</p>
<ul>
<li>First, the backups in other members are restored.</li>
<li>Then, data from these restored backups are recovered.</li>
<li>And finally, backups for these recovered data are formed.</li>
</ul>
<p>So eventually, no data is lost.</p>
<p><br></br></p>
<a name="how-do-i-test-the-connectivity"></a><h2 id="how-do-i-test-the-connectivity-">How do I test the connectivity?</h2>
<p>If you notice that there is a problem with a member joining a cluster, you may want to perform a connectivity test between the member to be joined and a member from the cluster. You can use the <code>iperf</code> tool for this purpose. For example, you can execute the below command on one member (i.e. listening on port 5701).</p>
<p><code>iperf -s -p 5701</code></p>
<p>And you can execute the below command on the other member.</p>
<p><code>iperf -c</code> <em><code>&lt;IP address&gt;</code></em> <code>-d -p 5701</code></p>
<p>The output should include connection information, such as the IP addresses, transfer speed, and bandwidth. Otherwise, if the output says <code>No route to host</code>, it means a network connection problem exists.</p>
<p><br></br></p>
<a name="how-do-i-choose-keys-properly"></a><h2 id="how-do-i-choose-keys-properly-">How do I choose keys properly?</h2>
<p>When you store a key and value in a distributed Map, Hazelcast serializes the key and value, and stores the byte array version of them in local ConcurrentHashMaps. These ConcurrentHashMaps use <code>equals</code> and <code>hashCode</code> methods of byte array version of your key. It does not take into account the actual <code>equals</code> and <code>hashCode</code> implementations of your objects. So it is important that you choose your keys in a proper way. </p>
<p>Implementing <code>equals</code> and <code>hashCode</code> is not enough, it is also important that the object is always serialized into the same byte array. All primitive types like String, Long, Integer, etc. are good candidates for keys to be used in Hazelcast. An unsorted Set is an example of a very bad candidate because Java Serialization may serialize the same unsorted set in two different byte arrays.</p>
<p><br></br></p>
<a name="how-do-i-reflect-value-modifications"></a><h2 id="how-do-i-reflect-value-modifications-">How do I reflect value modifications?</h2>
<p>Hazelcast always return a clone copy of a value. Modifying the returned value does not change the actual value in the map (or multimap, list, set). You should put the modified value back to make changes visible to all members.</p>
<pre><code class="lang-java">V value = map.get( key );
value.updateSomeProperty();
map.put( key, value );
</code></pre>
<p>Collections which return values of methods (such as <code>IMap.keySet</code>, <code>IMap.values</code>, <code>IMap.entrySet</code>, <code>MultiMap.get</code>, <code>MultiMap.remove</code>, <code>IMap.keySet</code>, <code>IMap.values</code>) contain cloned values. These collections are NOT backed up by related Hazelcast objects. Therefore, changes to them are <strong>NOT</strong> reflected in the originals, and vice-versa.</p>
<p><br></br></p>
<a name="how-do-i-test-my-hazelcast-cluster"></a><h2 id="how-do-i-test-my-hazelcast-cluster-">How do I test my Hazelcast cluster?</h2>
<p>Hazelcast allows you to create more than one instance on the same JVM. Each member is called <code>HazelcastInstance</code> and each will have its own configuration, socket and threads, so you can treat them as totally separate instances. </p>
<p>This enables you to write and to run cluster unit tests on a single JVM. Because you can use this feature for creating separate members different applications running on the same JVM (imagine running multiple web applications on the same JVM), you can also use this feature for testing your Hazelcast cluster.</p>
<p>Let&#39;s say you want to test if two members have the same size of a map.</p>
<pre><code class="lang-java">@Test
public void testTwoMemberMapSizes() {
  // start the first member
  HazelcastInstance h1 = Hazelcast.newHazelcastInstance();
  // get the map and put 1000 entries
  Map map1 = h1.getMap( &quot;testmap&quot; );
  for ( int i = 0; i &lt; 1000; i++ ) {
    map1.put( i, &quot;value&quot; + i );
  }
  // check the map size
  assertEquals( 1000, map1.size() );
  // start the second member
  HazelcastInstance h2 = Hazelcast.newHazelcastInstance();
  // get the same map from the second member
  Map map2 = h2.getMap( &quot;testmap&quot; );
  // check the size of map2
  assertEquals( 1000, map2.size() );
  // check the size of map1 again
  assertEquals( 1000, map1.size() );
}
</code></pre>
<p>In the test above, everything happens in the same thread. When developing a multi-threaded test, you need to carefully handle coordination of the thread executions. it is highly recommended that you use <code>CountDownLatch</code> for thread coordination (you can certainly use other ways). Here is an example where we need to listen for messages and make sure that we got these messages.</p>
<pre><code class="lang-java">@Test
public void testTopic() {
  // start two member cluster
  HazelcastInstance h1 = Hazelcast.newHazelcastInstance();
  HazelcastInstance h2 = Hazelcast.newHazelcastInstance();
  String topicName = &quot;TestMessages&quot;;
  // get a topic from the first member and add a messageListener
  ITopic&lt;String&gt; topic1 = h1.getTopic( topicName );
  final CountDownLatch latch1 = new CountDownLatch( 1 );
  topic1.addMessageListener( new MessageListener() {
    public void onMessage( Object msg ) {
      assertEquals( &quot;Test1&quot;, msg );
      latch1.countDown();
    }
  });
  // get a topic from the second member and add a messageListener
  ITopic&lt;String&gt; topic2 = h2.getTopic(topicName);
  final CountDownLatch latch2 = new CountDownLatch( 2 );
  topic2.addMessageListener( new MessageListener() {
    public void onMessage( Object msg ) {
      assertEquals( &quot;Test1&quot;, msg );
      latch2.countDown();
    }
  } );
  // publish the first message, both should receive this
  topic1.publish( &quot;Test1&quot; );
  // shutdown the first member
  h1.shutdown();
  // publish the second message, second member&#39;s topic should receive this
  topic2.publish( &quot;Test1&quot; );
  try {
    // assert that the first member&#39;s topic got the message
    assertTrue( latch1.await( 5, TimeUnit.SECONDS ) );
    // assert that the second members&#39; topic got two messages
    assertTrue( latch2.await( 5, TimeUnit.SECONDS ) );
  } catch ( InterruptedException ignored ) {
  }
}
</code></pre>
<p>You can start Hazelcast members with different configurations. Remember to call <code>Hazelcast.shutdownAll()</code> after each test case to make sure that there is no other running member left from the previous tests.</p>
<pre><code class="lang-java">@After
public void cleanup() throws Exception {
  Hazelcast.shutdownAll();
}
</code></pre>
<p>For more information please <a href="https://github.com/hazelcast/hazelcast/tree/master/hazelcast/src/test/java/com/hazelcast/cluster" target="_blank">check our existing tests</a>.</p>
<p><br></br></p>
<a name="does-hazelcast-support-hundreds-of-members"></a><h2 id="does-hazelcast-support-hundreds-of-members-">Does Hazelcast support hundreds of members?</h2>
<p>Yes. Hazelcast performed a successful test on Amazon EC2 with 200 members.</p>
<p><br></br></p>
<a name="does-hazelcast-support-thousands-of-clients"></a><h2 id="does-hazelcast-support-thousands-of-clients-">Does Hazelcast support thousands of clients?</h2>
<p>Yes. However, there are some points you should consider. The environment should be LAN with a high stability and the network speed should be 10 Gbps or higher. If the number of members is high, the client type should be selected as Dummy, not Smart Client. In the case of Smart Clients, since each client will open a connection to the members, these members should be powerful enough (for example, more cores) to handle hundreds or thousands of connections and client requests. Also, you should consider using near caches in clients to lower the network traffic. And you should use the Hazelcast releases with the NIO implementation (which starts with Hazelcast 3.2).</p>
<p>Also, you should configure the clients attentively. Please refer to the <a href="#hazelcast-java-client">Java Client section</a> section for configuration notes.</p>
<p><br></br></p>
<a name="difference-between-lite-member-and-smart-client"></a><h2 id="difference-between-lite-member-and-smart-client-">Difference between Lite Member and Smart Client?</h2>
<p>Lite member supports task execution (distributed executor service), smart client does not. Also, Lite Member is highly coupled with cluster, smart client is not.</p>
<p><br></br></p>
<a name="how-do-you-give-support"></a><h2 id="how-do-you-give-support-">How do you give support?</h2>
<p>We have two support services: community and commercial support. Community support is provided through our <a href="https://groups.google.com/forum/#!forum/hazelcast" target="_blank">Mail Group</a> and <a href="http://stackoverflow.com/" target="_blank">StackOverflow</a> web site. For information on support subscriptions, please see <a href="https://hazelcast.com/pricing/" target="_blank">Hazelcast.com</a>.</p>
<p><br></br></p>
<a name="does-hazelcast-persist"></a><h2 id="does-hazelcast-persist-">Does Hazelcast persist?</h2>
<p>No. However, Hazelcast provides <code>MapStore</code> and <code>MapLoader</code> interfaces. For example, when you implement the <code>MapStore</code> interface, Hazelcast calls your store and load methods whenever needed.</p>
<p><br></br></p>
<a name="can-i-use-hazelcast-in-a-single-server"></a><h2 id="can-i-use-hazelcast-in-a-single-server-">Can I use Hazelcast in a single server?</h2>
<p>Yes. But please note that Hazelcast&#39;s main design focus is multi-member clusters to be used as a distribution platform. </p>
<p><br></br></p>
<a name="how-can-i-monitor-hazelcast"></a><h2 id="how-can-i-monitor-hazelcast-">How can I monitor Hazelcast?</h2>
<p><a href="#management-center">Hazelcast Management Center</a> is what you use to monitor and manage the members running Hazelcast. In addition to monitoring the overall state of a cluster, you can analyze and browse data structures in detail, you can update map configurations, and you can take thread dumps from members. </p>
<p>Moreover, JMX monitoring is also provided. Please see the <a href="#monitoring-with-jmx">Monitoring with JMX section</a> for details.</p>
<p><br></br></p>
<a name="how-can-i-see-debug-level-logs"></a><h2 id="how-can-i-see-debug-level-logs-">How can I see debug level logs?</h2>
<p>By changing the log level to &quot;Debug&quot;. Below are sample lines for <strong>log4j</strong> logging framework. Please see the <a href="#logging-configuration">Logging Configuration section</a> to learn how to set logging types.</p>
<p>First, set the logging type as follows.</p>
<pre><code class="lang-java">String location = &quot;log4j.configuration&quot;;
String logging = &quot;hazelcast.logging.type&quot;;
System.setProperty( logging, &quot;log4j&quot; );
/**if you want to give a new location. **/
System.setProperty( location, &quot;file:/path/mylog4j.properties&quot; );
</code></pre>
<p>Then set the log level to &quot;Debug&quot; in the properties file. Below is example content.</p>
<p><code># direct log messages to stdout #</code></p>
<p><code>log4j.appender.stdout=org.apache.log4j.ConsoleAppender</code></p>
<p><code>log4j.appender.stdout.Target=System.out</code></p>
<p><code>log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</code></p>
<p><code>log4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %5p [%c{1}] - %m%n</code></p>
<p><br> </br></p>
<p><code>log4j.logger.com.hazelcast=debug</code></p>
<p><code>#log4j.logger.com.hazelcast.cluster=debug</code></p>
<p><code>#log4j.logger.com.hazelcast.partition=debug</code></p>
<p><code>#log4j.logger.com.hazelcast.partition.InternalPartitionService=debug</code></p>
<p><code>#log4j.logger.com.hazelcast.nio=debug</code></p>
<p><code>#log4j.logger.com.hazelcast.hibernate=debug</code></p>
<p>The line <code>log4j.logger.com.hazelcast=debug</code> is used to see debug logs for all Hazelcast operations. Below this line, you can select to see specific logs (cluster, partition, hibernate, etc.).</p>
<p><br></br></p>
<a name="client-server-vs-embedded-topologies"></a><h2 id="client-server-vs-embedded-topologies-">Client-server vs. embedded topologies?</h2>
<p>In the embedded topology, members include both the data and application. This type of topology is the most useful if your application focuses on high performance computing and many task executions. Since application is close to data, this topology supports data locality. </p>
<p>In the client-server topology, you create a cluster of members and scale the cluster independently. Your applications are hosted on the clients, and the clients communicate with the members in the cluster to reach data. </p>
<p>Client-server topology fits better if there are multiple applications sharing the same data or if application deployment is significantly greater than the cluster size (for example, 500 application servers vs. 10 member cluster).</p>
<p><br></br></p>
<a name="how-do-i-know-it-is-safe-to-kill-the-second-member"></a><h2 id="how-do-i-know-it-is-safe-to-kill-the-second-member-">How do I know it is safe to kill the second member?</h2>
<p>Below code snippet shuts down the cluster members if the cluster is safe for a member shutdown.</p>
<pre><code class="lang-java">PartitionService partitionService = hazelcastInstance.getPartitionService();
if (partitionService.isClusterSafe()) {
  hazelcastInstance.shutdown(); // or terminate
}
</code></pre>
<p>Below code snippet shuts down the local member if the member is safe to be shutdown.</p>
<pre><code class="lang-java">PartitionService partitionService = hazelcastInstance.getPartitionService();
if (partitionService.isLocalMemberSafe()) {
  hazelcastInstance.shutdown(); // or terminate
}
</code></pre>
<p><strong><em>RELATED INFORMATION</em></strong></p>
<p><em>Please refer to <a href="#safety-checking-cluster-members">Safety Checking Cluster Members</a> for more information.</em></p>
<p><br></br></p>
<a name="when-do-i-need-native-memory-solutions"></a><h2 id="when-do-i-need-native-memory-solutions-">When do I need Native Memory solutions?</h2>
<p>Native Memory solutions can be preferred:</p>
<ul>
<li>when the amount of data per member is large enough to create significant garbage collection pauses.</li>
<li>when your application requires predictable latency.</li>
</ul>
<p><br></br></p>
<a name="is-there-any-disadvantage-of-using-near-cache"></a><h2 id="is-there-any-disadvantage-of-using-near-cache-">Is there any disadvantage of using near-cache?</h2>
<p>The only disadvantage when using Near Cache is that it may cause stale reads.</p>
<p><br></br></p>
<a name="is-hazelcast-secure"></a><h2 id="is-hazelcast-secure-">Is Hazelcast secure?</h2>
<p>Hazelcast supports symmetric encryption, secure sockets layer (SSL), and Java Authentication and Authorization Service (JAAS). Please see the <a href="#security">Security chapter</a> for more information.</p>
<p><br></br></p>
<a name="how-can-i-set-socket-options"></a><h2 id="how-can-i-set-socket-options-">How can I set socket options?</h2>
<p>Hazelcast allows you to set some socket options such as <code>SO_KEEPALIVE</code>, <code>SO_SNDBUF</code>, and <code>SO_RCVBUF</code> using Hazelcast configuration properties. Please see <code>hazelcast.socket.*</code> properties explained in the <a href="#system-properties">System Properties section</a>.</p>
<p><br></br></p>
<a name="client-disconnections-during-idle-time"></a><h2 id="client-disconnections-during-idle-time-">Client disconnections during idle time?</h2>
<p>In Hazelcast, socket connections are created with the <code>SO_KEEPALIVE</code> option enabled by default. In most operating systems, default keep-alive time is 2 hours. If you have a firewall between clients and servers which is configured to reset idle connections/sessions, make sure that the firewall&#39;s idle timeout is greater than the TCP keep-alive defined in the OS.</p>
<p>For additional information please see:</p>
<ul>
<li><a href="http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html" target="_blank">Using TCP keepalive under Linux</a></li>
<li><p><a href="http://technet.microsoft.com/en-us/library/cc957549.aspx" target="_blank">Microsoft TechNet</a></p>
<p><br></br></p>
</li>
</ul>
<a name="oome-unable-to-create-new-native-thread"></a><h2 id="oome-unable-to-create-new-native-thread-">OOME: Unable to create new native thread?</h2>
<p>If you encounter an error of <code>java.lang.OutOfMemoryError: unable to create new native thread</code>, it may be caused by exceeding the available file descriptors on your operating system, especially if it is Linux. This exception is usually thrown on a running member, after a period of time when the thread count exhausts the file descriptor availability.</p>
<p>The JVM on Linux consumes a file descriptor for each thread created.  The default number of file descriptors available in Linux is usually 1024. If you have many JVMs running on a single machine, it is possible to exceed this default number.</p>
<p>You can view the limit using the following command.</p>
<p><code># ulimit -a</code></p>
<p>At the operating system level, Linux users can control the amount of resources (and in particular, file descriptors) used via one of the following options.</p>
<p>1 - Editing the <code>limits.conf</code> file:</p>
<p><code># vi /etc/security/limits.conf</code> </p>
<pre><code>testuser soft nofile 4096&lt;br&gt;
testuser hard nofile 10240&lt;br&gt;
</code></pre><p>2 - Or using the <code>ulimit</code> command:</p>
<p><code># ulimit -Hn</code></p>
<pre><code>10240
</code></pre><p>The default number of process per users is 1024. Adding the following to your <code>$HOME/.profile</code> could solve the issue:</p>
<p><code># ulimit -u 4096</code></p>
<p><br></br></p>
<a name="does-repartitioning-wait-for-entry-processor"></a><h2 id="does-repartitioning-wait-for-entry-processor-">Does repartitioning wait for Entry Processor?</h2>
<p>Repartitioning is the process of redistributing the partition ownerships. Hazelcast performs the repartitioning in the cases where a member leaves the cluster or joins the cluster. If a repartitioning will happen while an entry processor is active in a member processing on an entry object, the repartitioning waits for the entry processor to complete its job.</p>
<a name="instances-on-different-machines-cannot-see-each-other"></a><h2 id="instances-on-different-machines-cannot-see-each-other-">Instances on different machines cannot see each other?</h2>
<p>Assume you have two instances on two different machines and you develop a configuration as shown below.</p>
<pre><code class="lang-java">Config config = new Config();
NetworkConfig network = config.getNetworkConfig();

JoinConfig join = network.getJoin();
join.getMulticastConfig().setEnabled(false);
join.getTcpIpConfig().addMember(&quot;IP1&quot;)
    .addMember(&quot;IP2&quot;).setEnabled(true);
network.getInterfaces().setEnabled(true)
    .addInterface(&quot;IP1&quot;).addInterface(&quot;IP2&quot;);
</code></pre>
<p>When you create the Hazelcast instance, you have to pass the configuration to the instance. If you create the instances without passing the configuration, each instance starts but cannot see each other. Therefore, a correct way to create the instance is the following:</p>
<pre><code>HazelcastInstance instance = Hazelcast.newHazelcastInstance(config);
</code></pre><p>The following is an incorrect way:</p>
<pre><code>HazelcastInstance instance = Hazelcast.newHazelcastInstance();
</code></pre><a name="what-does-replica-1-has-no-owner-mean"></a><h2 id="what-does-replica-1-has-no-owner-mean-">What Does &quot;Replica: 1 has no owner&quot; Mean?</h2>
<p>When you start more members after the first one is started, you will see <code>replica: 1 has no owner</code> entry in the newly started member&#39;s log. There is no need to worry about it since it refers to a transitory state. It only means the replica partition is not ready/assigned yet and eventually it will be.</p>

<a name="glossary"></a><h1 id="glossary">Glossary</h1>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>2-phase Commit</strong></td>
<td style="text-align:left">2-phase commit protocol is an atomic commitment protocol for distributed systems. It consists of two phases: commit-request and commit. In commit-request phase, transaction manager coordinates all of the transaction resources to commit or abort. In commit-phase, transaction manager decides to finalize operation by committing or aborting according to the votes of the each transaction resource.</td>
</tr>
<tr>
<td style="text-align:left"><strong>ACID</strong></td>
<td style="text-align:left">A set of properties (Atomicity, Consistency, Isolation, Durability) guaranteeing that transactions are processed reliably. Atomicity requires that each transaction be all or nothing (i.e. if one part of the transaction fails, the entire transaction will fail). Consistency ensures that only valid data following all rules and constraints is written. Isolation ensures that transactions are securely and independently processed at the same time without interference (and without transaction ordering). Durability means that once a transaction has been committed, it will remain so, no matter if there is a power loss, crash, or error.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Cache</strong></td>
<td style="text-align:left">A high-speed access area that can be either a reserved section of main memory or storage device. </td>
</tr>
<tr>
<td style="text-align:left"><strong>Garbage Collection</strong></td>
<td style="text-align:left">Garbage collection is the recovery of storage that is being used by an application when that application no longer needs the storage. This frees the storage for use by other applications (or processes within an application). It also ensures that an application using increasing amounts of storage does not reach its quota. Programming languages that use garbage collection are often interpreted within virtual machines like the JVM. The environment that runs the code is also responsible for garbage collection.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Hazelcast Cluster</strong></td>
<td style="text-align:left">A virtual environment formed by Hazelcast members communicating with each other in the cluster.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Hazelcast Partitions</strong></td>
<td style="text-align:left">Memory segments containing the data. Hazelcast is built-on the partition concept, it uses partitions to store and process data. Each partition can have hundreds or thousands of data entries depending on your memory capacity. You can think of a partition as a block of data. In general and optimally, a partition should have a maximum size of 50-100 Megabytes.</td>
</tr>
<tr>
<td style="text-align:left"><strong>IMDG</strong></td>
<td style="text-align:left">An in-memory data grid (IMDG) is a data structure that resides entirely in memory, and is distributed among many members in a single location or across multiple locations. IMDGs can support thousands of in-memory data updates per second, and they can be clustered and scaled in ways that support large quantities of data.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Invalidation</strong></td>
<td style="text-align:left">The process of marking an object as being invalid across the distributed cache.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Java heap</strong></td>
<td style="text-align:left">Java heap is the space that Java can reserve and use in memory for dynamic memory allocation. All runtime objects created by a Java application are stored in heap. By default, the heap size is 128 MB, but this limit is reached easily for business applications. Once the heap is full, new objects cannot be created and the Java application shows errors.</td>
</tr>
<tr>
<td style="text-align:left"><strong>LRU, LFU</strong></td>
<td style="text-align:left">LRU and LFU are two of eviction algorithms. LRU is the abbreviation for Least Recently Used. It refers to entries eligible for eviction due to lack of interest by applications. LFU is the abbreviation for Least Frequently Used. It refers to the entries eligible for eviction due to having the lowest usage frequency.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Member</strong></td>
<td style="text-align:left">A Hazelcast instance. Depending on your Hazelcast usage, it can refer to a server or a Java virtual machine (JVM). Members belong to a Hazelcast cluster. Members are also referred as member nodes, cluster members, or Hazelcast members.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Multicast</strong></td>
<td style="text-align:left">A type of communication where data is addressed to a group of destination members simultaneously.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Near Cache</strong></td>
<td style="text-align:left">A caching model. When near cache is enabled, an object retrieved from a remote member is put into the local cache and the future requests made to this object will be handled by this local member. For example, if you have a map with data that is mostly read, then using near cache is a good idea.</td>
</tr>
<tr>
<td style="text-align:left"><strong>NoSQL</strong></td>
<td style="text-align:left">&quot;Not Only SQL&quot;. A database model that provides a mechanism for storage and retrieval of data that is tailored in means other than the tabular relations used in relational databases. It is a type of database which does not adhering to the traditional relational database management system (RDMS) structure. It is not built on tables and does not employ SQL to manipulate data. It also may not provide full ACID guarantees, but still has a distributed and fault tolerant architecture.</td>
</tr>
<tr>
<td style="text-align:left"><strong>OSGI</strong></td>
<td style="text-align:left">Formerly known as the Open Services Gateway initiative, it describes a modular system and a service platform for the Java programming language that implements a complete and dynamic component model.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Race Condition</strong></td>
<td style="text-align:left">This condition occurs when two or more threads can access shared data and they try to change it at the same time.</td>
</tr>
<tr>
<td style="text-align:left"><strong>RSA</strong></td>
<td style="text-align:left">An algorithm developed by Rivest, Shamir and Adleman to generate, encrypt and decrypt keys for secure data transmissions. </td>
</tr>
<tr>
<td style="text-align:left"><strong>Serialization</strong></td>
<td style="text-align:left">Process of converting an object into a stream of bytes in order to store the object or transmit it to memory, a database, or a file. Its main purpose is to save the state of an object in order to be able to recreate it when needed. The reverse process is called deserialization.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Split Brain</strong></td>
<td style="text-align:left">Split brain syndrome, in a clustering context, is a state in which a cluster of members gets divided (or partitioned) into smaller clusters of members, each of which believes it is the only active cluster.</td>
</tr>
<tr>
<td style="text-align:left"><strong>Transaction</strong></td>
<td style="text-align:left">Means a sequence of information exchange and related work (such as data store updating) that is treated as a unit for the purposes of satisfying a request and for ensuring data store integrity.</td>
</tr>
</tbody>
</table>





	        </div>
	        </div>			
    </div>
</body>
</html>
